{"docstore/metadata": {"50eaa830-2996-4f4e-8e1e-a687335b8c2a": {"doc_hash": "ea801c9a6d029864527d4789d9756d9d84370a0da130c77323226b323c845553"}, "4dfa9ad1-5cef-461d-b13d-e9bbf9eb6448": {"doc_hash": "f5296b600c248e0aa4990d6ff294bfa2896989187cf0963bbe2cd5c963a3faec"}, "eb1d4855-f305-44b9-b417-5e9ebc4ba70b": {"doc_hash": "e9b15002047af8b7cd039583f06fefda5c39cb20e78f176b6780447750a10338"}, "bee08983-b1a0-4337-89e8-d6c7c60eec90": {"doc_hash": "188f7a95b934d6afdc860da7574cf437a9d249d749ca97ff91467dde982eacde"}, "30b7950d-f176-4da1-92b1-47db9ff103d6": {"doc_hash": "a831c5ace5102796b828157108cb8d1a63144312b568ddc62a2b91abd41b5f94"}, "666f45c1-93d4-4081-8d75-7275a24aceff": {"doc_hash": "3cbd5ad410ef116c653624eedaa3264968cd967e26106a0c4b17a388eb9afeab"}, "23f40994-b97b-4321-b26c-c39a6c2361a5": {"doc_hash": "a30c4f27ce57234b8ad61e0df4f23050825f582c33113f4327d62e011c285c66"}, "aaf853f8-6515-49be-b87b-0bbea59462d2": {"doc_hash": "eb4170c7ead9db208b4e40539311eb88c8ebb998aa8c2e123641eb81a9c0d8b7"}, "be401141-ec1c-4ee1-9dc4-52fe6940a365": {"doc_hash": "5719623d31bc1c206d661c920320c0e0c387a04456c996e99a3607caf7435d85"}, "8d82b3d0-f4d8-4d67-a1fe-aff488212800": {"doc_hash": "0dd369615181f0597dc335f2b8a6fe89d59d5b890941869681c5b0a9e3411a1e"}, "dbcf148c-f2cc-418a-96ed-ad469eb99b95": {"doc_hash": "ea801c9a6d029864527d4789d9756d9d84370a0da130c77323226b323c845553"}, "e5a55779-a21a-4bd4-b054-f87d55317877": {"doc_hash": "f5296b600c248e0aa4990d6ff294bfa2896989187cf0963bbe2cd5c963a3faec"}, "236f1170-1385-476f-b976-06c9d21bf00d": {"doc_hash": "e9b15002047af8b7cd039583f06fefda5c39cb20e78f176b6780447750a10338"}, "f11a5cfa-9966-4efa-a233-6837ee306c6a": {"doc_hash": "188f7a95b934d6afdc860da7574cf437a9d249d749ca97ff91467dde982eacde"}, "fdc96a9a-0207-455a-94d4-1de9e3268cdf": {"doc_hash": "a831c5ace5102796b828157108cb8d1a63144312b568ddc62a2b91abd41b5f94"}, "d8408380-1cb2-4bc0-a567-26245027660a": {"doc_hash": "3cbd5ad410ef116c653624eedaa3264968cd967e26106a0c4b17a388eb9afeab"}, "5b273a66-5a72-4f2c-aa64-3a00694facbf": {"doc_hash": "a30c4f27ce57234b8ad61e0df4f23050825f582c33113f4327d62e011c285c66"}, "f9fdccac-4c2f-4eb1-b8a5-a5f777ccdc0f": {"doc_hash": "eb4170c7ead9db208b4e40539311eb88c8ebb998aa8c2e123641eb81a9c0d8b7"}, "9761d9a9-b545-4f33-81ed-79a3ff9b5997": {"doc_hash": "5719623d31bc1c206d661c920320c0e0c387a04456c996e99a3607caf7435d85"}, "e9633413-2f6b-4dc5-b7c3-e4cfd74e55fe": {"doc_hash": "0dd369615181f0597dc335f2b8a6fe89d59d5b890941869681c5b0a9e3411a1e"}}, "docstore/data": {"50eaa830-2996-4f4e-8e1e-a687335b8c2a": {"__data__": {"id_": "50eaa830-2996-4f4e-8e1e-a687335b8c2a", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_cloud_llamacloud_integrations_data_sinks_pinecone.md", "file_name": "python_cloud_llamacloud_integrations_data_sinks_pinecone.md", "file_type": "text/markdown", "file_size": 2076, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#_top)\n# Pinecone\nConfigure your own Pinecone instance as data sink.\n## Configure via UI\n[Section titled \u201cConfigure via UI\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-ui)\n## Configure via API / Client\n[Section titled \u201cConfigure via API / Client\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-api--client)\n  * [ TypeScript Client ](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#tab-panel-84)\n\n\n```\n\n\nfrom llama_cloud.types import CloudPineconeVectorStore\n\n\n\n\n\nds = {\n\n\n\n\n'name': '<your-name>',\n\n\n\n\n'sink_type': 'PINECONE',\n\n\n\n\n'component': CloudPineconeVectorStore(\n\n\n\n\napi_key='<api_key>',\n\n\n\n\nindex_name='<index_name>',\n\n\n\n\nname_space='<name_space>',# optional\n\n\n\n\ninsert_kwargs='<insert_kwargs>'# optional\n\n\n\n\n\n\ndata_sink = client.data_sinks.create_data_sink(request=ds)\n\n\n```\n\n```\n\n\nconst ds = {\n\n\n\n\n'name': 'pinecone',\n\n\n\n\n'sinkType': 'PINECONE',\n\n\n\n\n'component': {\n\n\n\n\n'api_key': '<api_key>',\n\n\n\n\n'index_name': '<index_name>',\n\n\n\n\n'name_space': '<name_space>'// optional\n\n\n\n\n'insert_kwargs': '<insert_kwargs>'// optional\n\n\n\n\n\n\n\ndata_sink=awaitclient.dataSinks.createDataSink({\n\n\n\n\nprojectId: projectId,\n\n\n\n\nbody: ds\n\n\n\n```\n\n## Filter Syntax\n[Section titled \u201cFilter Syntax\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#filter-syntax)\nWhen using Pinecone as a data sink, you can apply filters using the following syntax:\nFilter Operator | Pinecone Equivalent | Description  \n---|---|---  \n`$eq` | Equals  \n`$ne` | Not equal  \n`$gt` | Greater than  \n`$lt` | Less than  \n`$gte` | Greater than or equal  \n`$lte` | Less than or equal  \n`$in` | Value is in a list  \n`nin` | `$nin` | Value is not in a list  \nThese filters can be applied to metadata fields when querying your Pinecone index to refine search results based on specific criteria.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#_top)\n# Pinecone\nConfigure your own Pinecone instance as data sink.\n## Configure via UI\n[Section titled \u201cConfigure via UI\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-ui)\n## Configure via API / Client\n[Section titled \u201cConfigure via API / Client\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-api--client)\n  * [ TypeScript Client ](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#tab-panel-84)\n\n\n```\n\n\nfrom llama_cloud.types import CloudPineconeVectorStore\n\n\n\n\n\nds = {\n\n\n\n\n'name': '<your-name>',\n\n\n\n\n'sink_type': 'PINECONE',\n\n\n\n\n'component': CloudPineconeVectorStore(\n\n\n\n\napi_key='<api_key>',\n\n\n\n\nindex_name='<index_name>',\n\n\n\n\nname_space='<name_space>',# optional\n\n\n\n\ninsert_kwargs='<insert_kwargs>'# optional\n\n\n\n\n\n\ndata_sink = client.data_sinks.create_data_sink(request=ds)\n\n\n```\n\n```\n\n\nconst ds = {\n\n\n\n\n'name': 'pinecone',\n\n\n\n\n'sinkType': 'PINECONE',\n\n\n\n\n'component': {\n\n\n\n\n'api_key': '<api_key>',\n\n\n\n\n'index_name': '<index_name>',\n\n\n\n\n'name_space': '<name_space>'// optional\n\n\n\n\n'insert_kwargs': '<insert_kwargs>'// optional\n\n\n\n\n\n\n\ndata_sink=awaitclient.dataSinks.createDataSink({\n\n\n\n\nprojectId: projectId,\n\n\n\n\nbody: ds\n\n\n\n```\n\n## Filter Syntax\n[Section titled \u201cFilter Syntax\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#filter-syntax)\nWhen using Pinecone as a data sink, you can apply filters using the following syntax:\nFilter Operator | Pinecone Equivalent | Description  \n---|---|---  \n`$eq` | Equals  \n`$ne` | Not equal  \n`$gt` | Greater than  \n`$lt` | Less than  \n`$gte` | Greater than or equal  \n`$lte` | Less than or equal  \n`$in` | Value is in a list  \n`nin` | `$nin` | Value is not in a list  \nThese filters can be applied to metadata fields when querying your Pinecone index to refine search results based on specific criteria.\n"}, "__type__": "4"}, "4dfa9ad1-5cef-461d-b13d-e9bbf9eb6448": {"__data__": {"id_": "4dfa9ad1-5cef-461d-b13d-e9bbf9eb6448", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_cloud_self_hosting_configuration_file-storage.md", "file_name": "python_cloud_self_hosting_configuration_file-storage.md", "file_type": "text/markdown", "file_size": 6251, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#_top)\n# File Storage\n##  Self-Hosting Documentation Access \nThis section requires a password to access. Interested in self-hosting? [Contact sales](https://www.llamaindex.ai/contact) to learn more. \nSelf-Hosting Documentation Access Granted  Logout \nFile storage is an integral part of LlamaCloud. Without it, many key features would not be possible. This page walks through how to configure file storage for your deployment \u2014 which buckets you need to create and for non-AWS deployments, how to configure the S3 Proxy to interact with them.\n## Requirements\n[Section titled \u201cRequirements\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#requirements)\n  * A valid blob storage service. We recommend the following: \n    * [Google Cloud Storage](https://cloud.google.com/storage)\n  * Because LlamaCloud heavily relies on file storage, you will need to create the following buckets: \n    * `llama-platform-parsed-documents`\n    * `llama-platform-etl`\n    * `llama-platform-external-components`\n    * `llama-platform-file-parsing`\n    * `llama-platform-raw-files`\n    * `llama-cloud-parse-output`\n    * `llama-platform-file-screenshots`\n    * `llama-platform-extract-output` (for `LlamaExtract`)\n\n\n## Connecting to AWS S3\n[Section titled \u201cConnecting to AWS S3\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-aws-s3)\nBelow are two ways to configure a connection to AWS S3:\n### (Recommended) IAM Role for Service Accounts\n[Section titled \u201c(Recommended) IAM Role for Service Accounts\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#recommended-iam-role-for-service-accounts)\nWe recommend that users create a new IAM Role and Policy for LlamaCloud. You can then attach the role ARN as a service account annotation.\n```\n\n// Example IAM Policy\n\n\n\n\n\"Version\": \"2012-10-17\",\n\n\n\n\n\"Statement\": [\n\n\n\n\n\n\"Effect\": \"Allow\",\n\n\n\n\n\"Action\": [\"s3:*\"], // this is not secure\n\n\n\n\n\"Resource\": [\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents\",\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents/*\",\n\n\n\n\n\n\n\n```\n\nAfter creating something similar to the above policy, update the `backend`, `jobsService`, `jobsWorker`, and `llamaParse` service accounts with the EKS annotation.\n```\n\n# Example for the backend service account. Repeat for each of the services listed above.\n\n\n\nbackend:\n\n\n\n\nserviceAccountAnnotations:\n\n\n\n\neks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/<role-name>\n\n\n```\n\nFor more information, feel free to refer to the [official AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) about this topic.\n### AWS Credentials\n[Section titled \u201cAWS Credentials\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#aws-credentials)\nCreate a user with a policy attached for the aforementioned s3 buckets. Afterwards, you can configure the platform to use the aws credentials of that user by setting the following values in your `values.yaml` file:\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"aws\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nJCLOUDS_PROVIDER: \"aws-s3\"\n\n\n\n\nJCLOUDS_IDENTITY: <AWS-ACCESS-KEY>\n\n\n\n\nJCLOUDS_CREDENTIAL: <AWS-SECRET-KEY>\n\n\n\n\nJCLOUDS_REGION: <AWS-REGION># e.g. \"us-east-1\"\n\n\n\n\nJCLOUDS_ENDPOINT: \"https://s3.<AWS-REGION>.amazonaws.com\"\n\n\n```\n\n## Overriding Default Bucket Names\n[Section titled \u201cOverriding Default Bucket Names\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#overriding-default-bucket-names)\nWe allow users to override the default bucket names in the `values.yaml` file.\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nparsedDocuments: \"<your-bucket-name>\"\n\n\n\n\nparsedEtl: \"<your-bucket-name>\"\n\n\n\n\nparsedExternalComponents: \"<your-bucket-name>\"\n\n\n\n\nparsedFileParsing: \"<your-bucket-name>\"\n\n\n\n\nparsedRawFile: \"<your-bucket-name>\"\n\n\n\n\nparseOutput: \"<your-bucket-name>\"\n\n\n\n\nparsedFileScreenshot: \"<your-bucket-name>\"\n\n\n\n\nextractOutput: \"<your-bucket-name>\"\n\n\n\n\nparseFileUpload: \"<your-bucket-name>\"\n\n\n\n\nparseFileOutput: \"<your-bucket-name>\"\n\n\n```\n\n## Connecting to Azure Blob Storage or Other Providers with S3Proxy\n[Section titled \u201cConnecting to Azure Blob Storage or Other Providers with S3Proxy\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-azure-blob-storage-or-other-providers-with-s3proxy)\nLlamaCloud was first developed on AWS, which means that we started by natively supporting S3. However, to make a self-hosted solution possible, we need a way for the platform to interact with other providers.\nWe leverage the open-source project [S3Proxy](https://github.com/gaul/s3proxy) to translate the S3 API requests into requests to other storage providers. A containerized deployment of S3Proxy is supported out of the box in our helm charts.\nS3Proxy should always be set to `enabled: true`, even when deploying LlamaCloud on AWS. This causes S3Proxy to be deployed as a sidecar on several of the LlamaCloud pods.\nThe following is an example for how to connect your LlamaCloud deployment to Azure Blob Storage. For more examples of connecting to different providers, please refer to the project\u2019s [Examples](https://github.com/gaul/s3proxy/wiki/Storage-backend-examples) page.\n  * [ Azure Blob Storage with S3 Proxy ](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#tab-panel-56)\n\n\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"azure\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nS3PROXY_ENDPOINT: \"http://0.0.0.0:80\"\n\n\n\n\nS3PROXY_AUTHORIZATION: \"none\"\n\n\n\n\nS3PROXY_IGNORE_UNKNOWN_HEADERS: \"true\"\n\n\n\n\nS3PROXY_CORS_ALLOW_ORIGINS: \"*\"\n\n\n\n\nJCLOUDS_PROVIDER: \"azureblob\"\n\n\n\n\nJCLOUDS_REGION: \"eastus\"# Change to your region\n\n\n\n\nJCLOUDS_AZUREBLOB_AUTH: \"azureKey\"\n\n\n\n\nJCLOUDS_IDENTITY: \"fill-out\"# Change to your storage account name\n\n\n\n\nJCLOUDS_CREDENTIAL: \"fill-out\"# Change to your storage account key\n\n\n\n\nJCLOUDS_ENDPOINT: \"fill-out\"# Change to your storage account endpoint\n\n\n```\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#_top)\n# File Storage\n##  Self-Hosting Documentation Access \nThis section requires a password to access. Interested in self-hosting? [Contact sales](https://www.llamaindex.ai/contact) to learn more. \nSelf-Hosting Documentation Access Granted  Logout \nFile storage is an integral part of LlamaCloud. Without it, many key features would not be possible. This page walks through how to configure file storage for your deployment \u2014 which buckets you need to create and for non-AWS deployments, how to configure the S3 Proxy to interact with them.\n## Requirements\n[Section titled \u201cRequirements\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#requirements)\n  * A valid blob storage service. We recommend the following: \n    * [Google Cloud Storage](https://cloud.google.com/storage)\n  * Because LlamaCloud heavily relies on file storage, you will need to create the following buckets: \n    * `llama-platform-parsed-documents`\n    * `llama-platform-etl`\n    * `llama-platform-external-components`\n    * `llama-platform-file-parsing`\n    * `llama-platform-raw-files`\n    * `llama-cloud-parse-output`\n    * `llama-platform-file-screenshots`\n    * `llama-platform-extract-output` (for `LlamaExtract`)\n\n\n## Connecting to AWS S3\n[Section titled \u201cConnecting to AWS S3\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-aws-s3)\nBelow are two ways to configure a connection to AWS S3:\n### (Recommended) IAM Role for Service Accounts\n[Section titled \u201c(Recommended) IAM Role for Service Accounts\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#recommended-iam-role-for-service-accounts)\nWe recommend that users create a new IAM Role and Policy for LlamaCloud. You can then attach the role ARN as a service account annotation.\n```\n\n// Example IAM Policy\n\n\n\n\n\"Version\": \"2012-10-17\",\n\n\n\n\n\"Statement\": [\n\n\n\n\n\n\"Effect\": \"Allow\",\n\n\n\n\n\"Action\": [\"s3:*\"], // this is not secure\n\n\n\n\n\"Resource\": [\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents\",\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents/*\",\n\n\n\n\n\n\n\n```\n\nAfter creating something similar to the above policy, update the `backend`, `jobsService`, `jobsWorker`, and `llamaParse` service accounts with the EKS annotation.\n```\n\n# Example for the backend service account. Repeat for each of the services listed above.\n\n\n\nbackend:\n\n\n\n\nserviceAccountAnnotations:\n\n\n\n\neks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/<role-name>\n\n\n```\n\nFor more information, feel free to refer to the [official AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) about this topic.\n### AWS Credentials\n[Section titled \u201cAWS Credentials\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#aws-credentials)\nCreate a user with a policy attached for the aforementioned s3 buckets. Afterwards, you can configure the platform to use the aws credentials of that user by setting the following values in your `values.yaml` file:\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"aws\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nJCLOUDS_PROVIDER: \"aws-s3\"\n\n\n\n\nJCLOUDS_IDENTITY: <AWS-ACCESS-KEY>\n\n\n\n\nJCLOUDS_CREDENTIAL: <AWS-SECRET-KEY>\n\n\n\n\nJCLOUDS_REGION: <AWS-REGION># e.g. \"us-east-1\"\n\n\n\n\nJCLOUDS_ENDPOINT: \"https://s3.<AWS-REGION>.amazonaws.com\"\n\n\n```\n\n## Overriding Default Bucket Names\n[Section titled \u201cOverriding Default Bucket Names\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#overriding-default-bucket-names)\nWe allow users to override the default bucket names in the `values.yaml` file.\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nparsedDocuments: \"<your-bucket-name>\"\n\n\n\n\nparsedEtl: \"<your-bucket-name>\"\n\n\n\n\nparsedExternalComponents: \"<your-bucket-name>\"\n\n\n\n\nparsedFileParsing: \"<your-bucket-name>\"\n\n\n\n\nparsedRawFile: \"<your-bucket-name>\"\n\n\n\n\nparseOutput: \"<your-bucket-name>\"\n\n\n\n\nparsedFileScreenshot: \"<your-bucket-name>\"\n\n\n\n\nextractOutput: \"<your-bucket-name>\"\n\n\n\n\nparseFileUpload: \"<your-bucket-name>\"\n\n\n\n\nparseFileOutput: \"<your-bucket-name>\"\n\n\n```\n\n## Connecting to Azure Blob Storage or Other Providers with S3Proxy\n[Section titled \u201cConnecting to Azure Blob Storage or Other Providers with S3Proxy\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-azure-blob-storage-or-other-providers-with-s3proxy)\nLlamaCloud was first developed on AWS, which means that we started by natively supporting S3. However, to make a self-hosted solution possible, we need a way for the platform to interact with other providers.\nWe leverage the open-source project [S3Proxy](https://github.com/gaul/s3proxy) to translate the S3 API requests into requests to other storage providers. A containerized deployment of S3Proxy is supported out of the box in our helm charts.\nS3Proxy should always be set to `enabled: true`, even when deploying LlamaCloud on AWS. This causes S3Proxy to be deployed as a sidecar on several of the LlamaCloud pods.\nThe following is an example for how to connect your LlamaCloud deployment to Azure Blob Storage. For more examples of connecting to different providers, please refer to the project\u2019s [Examples](https://github.com/gaul/s3proxy/wiki/Storage-backend-examples) page.\n  * [ Azure Blob Storage with S3 Proxy ](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#tab-panel-56)\n\n\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"azure\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nS3PROXY_ENDPOINT: \"http://0.0.0.0:80\"\n\n\n\n\nS3PROXY_AUTHORIZATION: \"none\"\n\n\n\n\nS3PROXY_IGNORE_UNKNOWN_HEADERS: \"true\"\n\n\n\n\nS3PROXY_CORS_ALLOW_ORIGINS: \"*\"\n\n\n\n\nJCLOUDS_PROVIDER: \"azureblob\"\n\n\n\n\nJCLOUDS_REGION: \"eastus\"# Change to your region\n\n\n\n\nJCLOUDS_AZUREBLOB_AUTH: \"azureKey\"\n\n\n\n\nJCLOUDS_IDENTITY: \"fill-out\"# Change to your storage account name\n\n\n\n\nJCLOUDS_CREDENTIAL: \"fill-out\"# Change to your storage account key\n\n\n\n\nJCLOUDS_ENDPOINT: \"fill-out\"# Change to your storage account endpoint\n\n\n```\n\n"}, "__type__": "4"}, "eb1d4855-f305-44b9-b417-5e9ebc4ba70b": {"__data__": {"id_": "eb1d4855-f305-44b9-b417-5e9ebc4ba70b", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_docstore_tablestoredocstoredemo.md", "file_name": "python_examples_docstore_tablestoredocstoredemo.md", "file_type": "text/markdown", "file_size": 9752, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#_top)\n# Tablestore Demo \nThis guide shows you how to directly use our `DocumentStore` abstraction backed by Tablestore. By putting nodes in the docstore, this allows you to define multiple indices over the same underlying docstore, instead of duplicating data across indices.\n```\n\n\n%pip install llama-index-storage-docstore-tablestore\n\n\n\n\n%pip install llama-index-storage-index-store-tablestore\n\n\n\n\n%pip install llama-index-vector-stores-tablestore\n\n\n\n\n\n%pip install llama-index-llms-dashscope\n\n\n\n\n%pip install llama-index-embeddings-dashscope\n\n\n\n\n\n%pip install llama-index\n\n\n\n\n%pip install matplotlib\n\n\n```\n\n```\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n```\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n```\n\n\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\n\n\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex\n\n\n\n\nfrom llama_index.core import SummaryIndex\n\n\n\n\nfrom llama_index.core.response.notebook_utils import display_response\n\n\n\n\nfrom llama_index.core import Settings\n\n\n```\n\n#### Config Tablestore\n[Section titled \u201cConfig Tablestore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-tablestore)\nNext, we use tablestore\u2019s docsstore to perform a demo.\n```\n\n\nimport getpass\n\n\n\n\nimport os\n\n\n\n\n\nos.environ[\"tablestore_end_point\"] = getpass.getpass(\"tablestore end_point:\")\n\n\n\n\nos.environ[\"tablestore_instance_name\"] = getpass.getpass(\n\n\n\n\n\"tablestore instance_name:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_id\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_id:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_secret\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_secret:\"\n\n\n\n```\n\n#### Config DashScope LLM\n[Section titled \u201cConfig DashScope LLM\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-dashscope-llm)\nNext, we use dashscope\u2019s llm to perform a demo.\n```\n\n\nimport os\n\n\n\n\nimport getpass\n\n\n\n\n\nos.environ[\"DASHSCOPE_API_KEY\"] = getpass.getpass(\"DashScope api key:\")\n\n\n```\n\n#### Download Data\n[Section titled \u201cDownload Data\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#download-data)\n```\n\n\n!mkdir -p 'data/paul_graham/'\n\n\n\n\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt'-O 'data/paul_graham/paul_graham_essay.txt'\n\n\n```\n\n#### Load Documents\n[Section titled \u201cLoad Documents\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#load-documents)\n```\n\n\nreader =SimpleDirectoryReader(\"./data/paul_graham/\")\n\n\n\n\ndocuments = reader.load_data()\n\n\n```\n\n#### Parse into Nodes\n[Section titled \u201cParse into Nodes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#parse-into-nodes)\n```\n\n\nfrom llama_index.core.node_parser import SentenceSplitter\n\n\n\n\n\nnodes =SentenceSplitter().get_nodes_from_documents(documents)\n\n\n```\n\n#### Init Store/Embedding/LLM/StorageContext\n[Section titled \u201cInit Store/Embedding/LLM/StorageContext\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#init-storeembeddingllmstoragecontext)\n```\n\n\nfrom llama_index.storage.docstore.tablestore import TablestoreDocumentStore\n\n\n\n\nfrom llama_index.storage.index_store.tablestore import TablestoreIndexStore\n\n\n\n\nfrom llama_index.vector_stores.tablestore import TablestoreVectorStore\n\n\n\n\nfrom llama_index.embeddings.dashscope import (\n\n\n\n\nDashScopeEmbedding,\n\n\n\n\nDashScopeTextEmbeddingModels,\n\n\n\n\nDashScopeTextEmbeddingType,\n\n\n\n\n\nfrom llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n\n\n\n\n\nembedder =DashScopeEmbedding(\n\n\n\n\nmodel_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,# default demiension is 1024\n\n\n\n\ntext_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,\n\n\n\n\n\n\ndashscope_llm =DashScope(\n\n\n\n\nmodel_name=DashScopeGenerationModels.QWEN_MAX,\n\n\n\n\napi_key=os.environ[\"DASHSCOPE_API_KEY\"],\n\n\n\n\n\nSettings.llm = dashscope_llm\n\n\n\n\n\ndocstore = TablestoreDocumentStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nindex_store = TablestoreIndexStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nvector_store =TablestoreVectorStore(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\nvector_dimension=1024,# embedder dimension is 1024\n\n\n\n\n\nvector_store.create_table_if_not_exist()\n\n\n\n\nvector_store.create_search_index_if_not_exist()\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n```\n\n#### Add to docStore\n[Section titled \u201cAdd to docStore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#add-to-docstore)\n```\n\n\nstorage_context.docstore.add_documents(nodes)\n\n\n```\n\n#### Define & Add Multiple Indexes\n[Section titled \u201cDefine & Add Multiple Indexes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#define--add-multiple-indexes)\nEach index uses the same underlying Node.\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/list.html\n\n\n\nsummary_index =SummaryIndex(nodes,storage_context=storage_context)\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/vector_store.html\n\n\n\nvector_index =VectorStoreIndex(\n\n\n\n\nnodes,\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/table.html\n\n\n\nkeyword_table_index =SimpleKeywordTableIndex(\n\n\n\n\nnodes=nodes,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nllm=dashscope_llm,\n\n\n\n```\n\n```\n\n\n# NOTE: the docstore still has the same nodes\n\n\n\n\nlen(storage_context.docstore.docs)\n\n\n```\n\n#### Test out saving and loading\n[Section titled \u201cTest out saving and loading\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-saving-and-loading)\n```\n\n\n# NOTE: docstore and index_store is persisted in Tablestore by default\n\n\n\n\n# NOTE: here only need to persist simple vector store to disk\n\n\n\n\nstorage_context.persist()\n\n\n```\n\n```\n\n# note down index IDs\n\n\n\nlist_id = summary_index.index_id\n\n\n\n\nvector_id = vector_index.index_id\n\n\n\n\nkeyword_id = keyword_table_index.index_id\n\n\n\n\nprint(list_id, vector_id, keyword_id)\n\n\n```\n\n```\n\nc05fec2a-ac87-4761-beeb-0901f9e6530e d0b021ed-3427-46ad-927d-12d72752dbc4 2e9bfc3a-5e69-408a-9430-7b0c8baf3d77\n\n```\n\n```\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n# re-create storage context\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n\n\n\nsummary_index =load_index_from_storage(\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=list_id,\n\n\n\n\n\nkeyword_table_index =load_index_from_storage(\n\n\n\n\nllm=dashscope_llm,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=keyword_id,\n\n\n\n\n# You need to add \"vector_store=xxx\" to StorageContext to load vector index from Tablestore\n\n\n\nvector_index =load_index_from_storage(\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=vector_id,\n\n\n\n```\n\n#### Test out some Queries\n[Section titled \u201cTest out some Queries\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-some-queries)\n```\n\n\nSettings.llm = dashscope_llm\n\n\n\n\nSettings.chunk_size =1024\n\n\n```\n\n```\n\n\nquery_engine = summary_index.as_query_engine()\n\n\n\n\nlist_response = query_engine.query(\"What is a summary of this document?\")\n\n\n```\n\n```\n\n\ndisplay_response(list_response)\n\n\n```\n\n```\n\n\nquery_engine = vector_index.as_query_engine()\n\n\n\n\nvector_response = query_engine.query(\"What did the author do growing up?\")\n\n\n```\n\n```\n\n\ndisplay_response(vector_response)\n\n\n```\n\n**`Final Response:`**Growing up, the author was involved in writing and programming outside of school. Initially, they wrote short stories, which they now consider to be not very good, as they lacked much plot and focused more on characters\u2019 emotions. In terms of programming, the author started with an IBM 1401 at their junior high school, where they attempted to write basic programs in Fortran using punch cards. Later, after getting a TRS-80 microcomputer, the author delved deeper into programming, creating simple games, a program to predict the flight height of model rockets, and even a word processor that their father used for writing.\n```\n\n\nquery_engine = keyword_table_index.as_query_engine()\n\n\n\n\nkeyword_response = query_engine.query(\n\n\n\n\n\"What did the author do after his time at YC?\"\n\n\n\n```\n\n```\n\n\ndisplay_response(keyword_response)\n\n\n```\n\n**`Final Response:`**After his time at YC, the author decided to take up painting, dedicating himself to it to see how good he could become. He spent most of 2014 focused on this. However, by November, he lost interest and stopped. Following this, he returned to writing essays and even ventured into topics beyond startups. In March 2015, he also began working on Lisp again.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#_top)\n# Tablestore Demo \nThis guide shows you how to directly use our `DocumentStore` abstraction backed by Tablestore. By putting nodes in the docstore, this allows you to define multiple indices over the same underlying docstore, instead of duplicating data across indices.\n```\n\n\n%pip install llama-index-storage-docstore-tablestore\n\n\n\n\n%pip install llama-index-storage-index-store-tablestore\n\n\n\n\n%pip install llama-index-vector-stores-tablestore\n\n\n\n\n\n%pip install llama-index-llms-dashscope\n\n\n\n\n%pip install llama-index-embeddings-dashscope\n\n\n\n\n\n%pip install llama-index\n\n\n\n\n%pip install matplotlib\n\n\n```\n\n```\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n```\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n```\n\n\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\n\n\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex\n\n\n\n\nfrom llama_index.core import SummaryIndex\n\n\n\n\nfrom llama_index.core.response.notebook_utils import display_response\n\n\n\n\nfrom llama_index.core import Settings\n\n\n```\n\n#### Config Tablestore\n[Section titled \u201cConfig Tablestore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-tablestore)\nNext, we use tablestore\u2019s docsstore to perform a demo.\n```\n\n\nimport getpass\n\n\n\n\nimport os\n\n\n\n\n\nos.environ[\"tablestore_end_point\"] = getpass.getpass(\"tablestore end_point:\")\n\n\n\n\nos.environ[\"tablestore_instance_name\"] = getpass.getpass(\n\n\n\n\n\"tablestore instance_name:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_id\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_id:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_secret\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_secret:\"\n\n\n\n```\n\n#### Config DashScope LLM\n[Section titled \u201cConfig DashScope LLM\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-dashscope-llm)\nNext, we use dashscope\u2019s llm to perform a demo.\n```\n\n\nimport os\n\n\n\n\nimport getpass\n\n\n\n\n\nos.environ[\"DASHSCOPE_API_KEY\"] = getpass.getpass(\"DashScope api key:\")\n\n\n```\n\n#### Download Data\n[Section titled \u201cDownload Data\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#download-data)\n```\n\n\n!mkdir -p 'data/paul_graham/'\n\n\n\n\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt'-O 'data/paul_graham/paul_graham_essay.txt'\n\n\n```\n\n#### Load Documents\n[Section titled \u201cLoad Documents\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#load-documents)\n```\n\n\nreader =SimpleDirectoryReader(\"./data/paul_graham/\")\n\n\n\n\ndocuments = reader.load_data()\n\n\n```\n\n#### Parse into Nodes\n[Section titled \u201cParse into Nodes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#parse-into-nodes)\n```\n\n\nfrom llama_index.core.node_parser import SentenceSplitter\n\n\n\n\n\nnodes =SentenceSplitter().get_nodes_from_documents(documents)\n\n\n```\n\n#### Init Store/Embedding/LLM/StorageContext\n[Section titled \u201cInit Store/Embedding/LLM/StorageContext\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#init-storeembeddingllmstoragecontext)\n```\n\n\nfrom llama_index.storage.docstore.tablestore import TablestoreDocumentStore\n\n\n\n\nfrom llama_index.storage.index_store.tablestore import TablestoreIndexStore\n\n\n\n\nfrom llama_index.vector_stores.tablestore import TablestoreVectorStore\n\n\n\n\nfrom llama_index.embeddings.dashscope import (\n\n\n\n\nDashScopeEmbedding,\n\n\n\n\nDashScopeTextEmbeddingModels,\n\n\n\n\nDashScopeTextEmbeddingType,\n\n\n\n\n\nfrom llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n\n\n\n\n\nembedder =DashScopeEmbedding(\n\n\n\n\nmodel_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,# default demiension is 1024\n\n\n\n\ntext_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,\n\n\n\n\n\n\ndashscope_llm =DashScope(\n\n\n\n\nmodel_name=DashScopeGenerationModels.QWEN_MAX,\n\n\n\n\napi_key=os.environ[\"DASHSCOPE_API_KEY\"],\n\n\n\n\n\nSettings.llm = dashscope_llm\n\n\n\n\n\ndocstore = TablestoreDocumentStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nindex_store = TablestoreIndexStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nvector_store =TablestoreVectorStore(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\nvector_dimension=1024,# embedder dimension is 1024\n\n\n\n\n\nvector_store.create_table_if_not_exist()\n\n\n\n\nvector_store.create_search_index_if_not_exist()\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n```\n\n#### Add to docStore\n[Section titled \u201cAdd to docStore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#add-to-docstore)\n```\n\n\nstorage_context.docstore.add_documents(nodes)\n\n\n```\n\n#### Define & Add Multiple Indexes\n[Section titled \u201cDefine & Add Multiple Indexes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#define--add-multiple-indexes)\nEach index uses the same underlying Node.\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/list.html\n\n\n\nsummary_index =SummaryIndex(nodes,storage_context=storage_context)\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/vector_store.html\n\n\n\nvector_index =VectorStoreIndex(\n\n\n\n\nnodes,\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/table.html\n\n\n\nkeyword_table_index =SimpleKeywordTableIndex(\n\n\n\n\nnodes=nodes,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nllm=dashscope_llm,\n\n\n\n```\n\n```\n\n\n# NOTE: the docstore still has the same nodes\n\n\n\n\nlen(storage_context.docstore.docs)\n\n\n```\n\n#### Test out saving and loading\n[Section titled \u201cTest out saving and loading\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-saving-and-loading)\n```\n\n\n# NOTE: docstore and index_store is persisted in Tablestore by default\n\n\n\n\n# NOTE: here only need to persist simple vector store to disk\n\n\n\n\nstorage_context.persist()\n\n\n```\n\n```\n\n# note down index IDs\n\n\n\nlist_id = summary_index.index_id\n\n\n\n\nvector_id = vector_index.index_id\n\n\n\n\nkeyword_id = keyword_table_index.index_id\n\n\n\n\nprint(list_id, vector_id, keyword_id)\n\n\n```\n\n```\n\nc05fec2a-ac87-4761-beeb-0901f9e6530e d0b021ed-3427-46ad-927d-12d72752dbc4 2e9bfc3a-5e69-408a-9430-7b0c8baf3d77\n\n```\n\n```\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n# re-create storage context\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n\n\n\nsummary_index =load_index_from_storage(\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=list_id,\n\n\n\n\n\nkeyword_table_index =load_index_from_storage(\n\n\n\n\nllm=dashscope_llm,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=keyword_id,\n\n\n\n\n# You need to add \"vector_store=xxx\" to StorageContext to load vector index from Tablestore\n\n\n\nvector_index =load_index_from_storage(\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=vector_id,\n\n\n\n```\n\n#### Test out some Queries\n[Section titled \u201cTest out some Queries\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-some-queries)\n```\n\n\nSettings.llm = dashscope_llm\n\n\n\n\nSettings.chunk_size =1024\n\n\n```\n\n```\n\n\nquery_engine = summary_index.as_query_engine()\n\n\n\n\nlist_response = query_engine.query(\"What is a summary of this document?\")\n\n\n```\n\n```\n\n\ndisplay_response(list_response)\n\n\n```\n\n```\n\n\nquery_engine = vector_index.as_query_engine()\n\n\n\n\nvector_response = query_engine.query(\"What did the author do growing up?\")\n\n\n```\n\n```\n\n\ndisplay_response(vector_response)\n\n\n```\n\n**`Final Response:`**Growing up, the author was involved in writing and programming outside of school. Initially, they wrote short stories, which they now consider to be not very good, as they lacked much plot and focused more on characters\u2019 emotions. In terms of programming, the author started with an IBM 1401 at their junior high school, where they attempted to write basic programs in Fortran using punch cards. Later, after getting a TRS-80 microcomputer, the author delved deeper into programming, creating simple games, a program to predict the flight height of model rockets, and even a word processor that their father used for writing.\n```\n\n\nquery_engine = keyword_table_index.as_query_engine()\n\n\n\n\nkeyword_response = query_engine.query(\n\n\n\n\n\"What did the author do after his time at YC?\"\n\n\n\n```\n\n```\n\n\ndisplay_response(keyword_response)\n\n\n```\n\n**`Final Response:`**After his time at YC, the author decided to take up painting, dedicating himself to it to see how good he could become. He spent most of 2014 focused on this. However, by November, he lost interest and stopped. Following this, he returned to writing essays and even ventured into topics beyond startups. In March 2015, he also began working on Lisp again.\n"}, "__type__": "4"}, "bee08983-b1a0-4337-89e8-d6c7c60eec90": {"__data__": {"id_": "bee08983-b1a0-4337-89e8-d6c7c60eec90", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_embeddings_huggingface.md", "file_name": "python_examples_embeddings_huggingface.md", "file_type": "text/markdown", "file_size": 6622, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#_top)\n# Local Embeddings with HuggingFace \nLlamaIndex has support for HuggingFace embedding models, including Sentence Transformer models like BGE, Mixedbread, Nomic, Jina, E5, etc. We can use these models to create embeddings for our documents and queries for retrieval.\nFurthermore, we provide utilities to create and use ONNX and OpenVINO models using the [Optimum library](https://huggingface.co/docs/optimum) from HuggingFace.\n## HuggingFaceEmbedding\n[Section titled \u201cHuggingFaceEmbedding\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#huggingfaceembedding)\nThe base `HuggingFaceEmbedding` class is a generic wrapper around any HuggingFace model for embeddings. All [embedding models](https://huggingface.co/models?library=sentence-transformers) on Hugging Face should work. You can refer to the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more recommendations.\nThis class depends on the sentence-transformers package, which you can install with `pip install sentence-transformers`.\nNOTE: if you were previously using a `HuggingFaceEmbeddings` from LangChain, this should give equivalent results.\nIf you\u2019re opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.\n```\n\n\n%pip install llama-index-embeddings-huggingface\n\n\n```\n\n```\n\n\n!pip install llama-index\n\n\n```\n\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads https://huggingface.co/BAAI/bge-small-en-v1.5\n\n\n\nembed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\n\n```\n\n```\n\n\nembeddings = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\nprint(len(embeddings))\n\n\n\n\nprint(embeddings[:5])\n\n\n```\n\n```\n\n384\n\n\n[-0.003275700844824314, -0.011690810322761536, 0.041559211909770966, -0.03814814239740372, 0.024183044210076332]\n\n```\n\n## Benchmarking\n[Section titled \u201cBenchmarking\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#benchmarking)\nLet\u2019s try comparing using a classic large document \u2014 the IPCC climate report, chapter 3.\n```\n\n\n!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf\n\n\n```\n\n```\n\n\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n\n\n\n\nDload  Upload   Total   Spent    Left  Speed\n\n\n\n\n\n0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\n\n\n100 20.7M  100 20.7M    0     0  69.6M      0 --:--:-- --:--:-- --:--:-- 70.0M\n\n```\n\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n\n\n\n\n).load_data()\n\n\n```\n\n### Base HuggingFace Embeddings\n[Section titled \u201cBase HuggingFace Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#base-huggingface-embeddings)\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the default torch backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nembed_batch_size=8,\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 428.44it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:19<00:00, 23.32it/s]\n\n\n\n\n20.2 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### ONNX Embeddings\n[Section titled \u201cONNX Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#onnx-embeddings)\n```\n\n# pip install sentence-transformers[onnx]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the onnx backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"onnx\",\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"provider\": \"CPUExecutionProvider\"\n\n\n\n\n},# For ONNX, you can specify the provider, see https://sbert.net/docs/sentence_transformer/usage/efficiency.html\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 421.63it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:31<00:00, 14.53it/s]\n\n\n\n32.1 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### OpenVINO Embeddings\n[Section titled \u201cOpenVINO Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#openvino-embeddings)\n```\n\n# pip install sentence-transformers[openvino]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the openvino backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"openvino\",# OpenVINO is very strong on CPUs\n\n\n\n\nrevision=\"refs/pr/16\",# BAAI/bge-small-en-v1.5 itself doesn't have an OpenVINO model currently, but there's a PR with it that we can load: https://huggingface.co/BAAI/bge-small-en-v1.5/discussions/16\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"file_name\": \"openvino_model_qint8_quantized.xml\"\n\n\n\n\n},# If we're using an optimized/quantized model, we need to specify the file name like this\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 403.15it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:08<00:00, 53.83it/s]\n\n\n\n\n9.03 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### References\n[Section titled \u201cReferences\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#references)\n  * [Local Embedding Models](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#local-embedding-models) explains more about using local models like these.\n  * [Sentence Transformers > Speeding up Inference](https://sbert.net/docs/sentence_transformer/usage/efficiency.html) contains extensive documentation on how to use the backend options effectively, including optimization and quantization for ONNX and OpenVINO.\n\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#_top)\n# Local Embeddings with HuggingFace \nLlamaIndex has support for HuggingFace embedding models, including Sentence Transformer models like BGE, Mixedbread, Nomic, Jina, E5, etc. We can use these models to create embeddings for our documents and queries for retrieval.\nFurthermore, we provide utilities to create and use ONNX and OpenVINO models using the [Optimum library](https://huggingface.co/docs/optimum) from HuggingFace.\n## HuggingFaceEmbedding\n[Section titled \u201cHuggingFaceEmbedding\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#huggingfaceembedding)\nThe base `HuggingFaceEmbedding` class is a generic wrapper around any HuggingFace model for embeddings. All [embedding models](https://huggingface.co/models?library=sentence-transformers) on Hugging Face should work. You can refer to the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more recommendations.\nThis class depends on the sentence-transformers package, which you can install with `pip install sentence-transformers`.\nNOTE: if you were previously using a `HuggingFaceEmbeddings` from LangChain, this should give equivalent results.\nIf you\u2019re opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.\n```\n\n\n%pip install llama-index-embeddings-huggingface\n\n\n```\n\n```\n\n\n!pip install llama-index\n\n\n```\n\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads https://huggingface.co/BAAI/bge-small-en-v1.5\n\n\n\nembed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\n\n```\n\n```\n\n\nembeddings = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\nprint(len(embeddings))\n\n\n\n\nprint(embeddings[:5])\n\n\n```\n\n```\n\n384\n\n\n[-0.003275700844824314, -0.011690810322761536, 0.041559211909770966, -0.03814814239740372, 0.024183044210076332]\n\n```\n\n## Benchmarking\n[Section titled \u201cBenchmarking\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#benchmarking)\nLet\u2019s try comparing using a classic large document \u2014 the IPCC climate report, chapter 3.\n```\n\n\n!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf\n\n\n```\n\n```\n\n\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n\n\n\n\nDload  Upload   Total   Spent    Left  Speed\n\n\n\n\n\n0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\n\n\n100 20.7M  100 20.7M    0     0  69.6M      0 --:--:-- --:--:-- --:--:-- 70.0M\n\n```\n\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n\n\n\n\n).load_data()\n\n\n```\n\n### Base HuggingFace Embeddings\n[Section titled \u201cBase HuggingFace Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#base-huggingface-embeddings)\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the default torch backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nembed_batch_size=8,\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 428.44it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:19<00:00, 23.32it/s]\n\n\n\n\n20.2 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### ONNX Embeddings\n[Section titled \u201cONNX Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#onnx-embeddings)\n```\n\n# pip install sentence-transformers[onnx]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the onnx backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"onnx\",\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"provider\": \"CPUExecutionProvider\"\n\n\n\n\n},# For ONNX, you can specify the provider, see https://sbert.net/docs/sentence_transformer/usage/efficiency.html\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 421.63it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:31<00:00, 14.53it/s]\n\n\n\n32.1 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### OpenVINO Embeddings\n[Section titled \u201cOpenVINO Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#openvino-embeddings)\n```\n\n# pip install sentence-transformers[openvino]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the openvino backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"openvino\",# OpenVINO is very strong on CPUs\n\n\n\n\nrevision=\"refs/pr/16\",# BAAI/bge-small-en-v1.5 itself doesn't have an OpenVINO model currently, but there's a PR with it that we can load: https://huggingface.co/BAAI/bge-small-en-v1.5/discussions/16\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"file_name\": \"openvino_model_qint8_quantized.xml\"\n\n\n\n\n},# If we're using an optimized/quantized model, we need to specify the file name like this\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 403.15it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:08<00:00, 53.83it/s]\n\n\n\n\n9.03 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### References\n[Section titled \u201cReferences\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#references)\n  * [Local Embedding Models](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#local-embedding-models) explains more about using local models like these.\n  * [Sentence Transformers > Speeding up Inference](https://sbert.net/docs/sentence_transformer/usage/efficiency.html) contains extensive documentation on how to use the backend options effectively, including optimization and quantization for ONNX and OpenVINO.\n\n\n"}, "__type__": "4"}, "30b7950d-f176-4da1-92b1-47db9ff103d6": {"__data__": {"id_": "30b7950d-f176-4da1-92b1-47db9ff103d6", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_vector_stores_moorchehdemo.md", "file_name": "python_examples_vector_stores_moorchehdemo.md", "file_type": "text/markdown", "file_size": 5135, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#_top)\n# Moorcheh Vector Store Demo \n## Install Required Packages\n[Section titled \u201cInstall Required Packages\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#install-required-packages)\n```\n\n\n!pip install llama_index\n\n\n\n\n!pip install moorcheh_sdk\n\n\n```\n\n## Import Required Libraries\n[Section titled \u201cImport Required Libraries\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#import-required-libraries)\ndemo.py```\n\n# --- Welcome to the Demo of the Moorcheh Vector Store ---\n\n\n# --- Import the following packages --\n\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\nimport os\n\n\n\n\nfrom moorcheh_sdk import MoorchehClient\n\n\n\n\nfrom IPython.display import Markdown, display\n\n\n\n\nfrom typing import Any, Callable, Dict, List, Optional, cast\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nStorageContext,\n\n\n\n\nSettings,\n\n\n\n\n\nfrom llama_index.core.base.embeddings.base_sparse import BaseSparseEmbedding\n\n\n\n\nfrom llama_index.core.bridge.pydantic import PrivateAttr\n\n\n\n\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nBasePydanticVectorStore,\n\n\n\n\nMetadataFilters,\n\n\n\n\nVectorStoreQuery,\n\n\n\n\nVectorStoreQueryMode,\n\n\n\n\nVectorStoreQueryResult,\n\n\n\n\n\nfrom llama_index.core.vector_stores.utils import (\n\n\n\n\nDEFAULT_TEXT_KEY,\n\n\n\n\nlegacy_metadata_dict_to_node,\n\n\n\n\nmetadata_dict_to_node,\n\n\n\n\nnode_to_metadata_dict,\n\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nMetadataFilter,\n\n\n\n\nMetadataFilters,\n\n\n\n\nFilterOperator,\n\n\n\n\nFilterCondition,\n\n\n\n```\n\n## Configure Logging\n[Section titled \u201cConfigure Logging\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#configure-logging)\n```\n\n# --- Logging Setup ---\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n## Load Moorcheh API Key\n[Section titled \u201cLoad Moorcheh API Key\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-moorcheh-api-key)\n```\n\n# --- Set the values of the API Keys in your Environment Variables ---\n\n\n\nfrom google.colab import userdata\n\n\n\n\n\napi_key = os.environ[\"MOORCHEH_API_KEY\"] = userdata.get(\"MOORCHEH_API_KEY\")\n\n\n\n\n\nif\"MOORCHEH_API_KEY\"notin os.environ:\n\n\n\n\nraiseEnvironmentError(f\"Environment variable MOORCHEH_API_KEY is not set\")\n\n\n```\n\n## Load and Chunk Documents\n[Section titled \u201cLoad and Chunk Documents\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-and-chunk-documents)\n```\n\n# --- Load Documents ---\n\n\n\ndocuments =SimpleDirectoryReader(\"./documents\").load_data()\n\n\n\n\n# --- Set chunk size and overlap ---\n\n\n\nSettings.chunk_size =1024\n\n\n\n\nSettings.chunk_overlap =20\n\n\n```\n\n## Initialize Vector Store and Create Index\n[Section titled \u201cInitialize Vector Store and Create Index\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#initialize-vector-store-and-create-index)\n```\n\n# --- Initialize the Moorcheh Vector Store ---\n\n\n\n__all__ =[\"MoorchehVectorStore\"]\n\n\n\n\n# Creates a Moorcheh Vector Store with the following parameters\n\n\n# For text-based namespaces, set namespace_type to \"text\" and vector_dimension to None\n\n\n# For vector-based namespaces, set namespace_type to \"vector\" and vector_dimension to the dimension of your uploaded vectors\n\n\n\nvector_store =MoorchehVectorStore(\n\n\n\n\napi_key=api_key,\n\n\n\n\nnamespace=\"llamaindex_moorcheh\",\n\n\n\n\nnamespace_type=\"text\",\n\n\n\n\nvector_dimension=None,\n\n\n\n\nadd_sparse_vector=False,\n\n\n\n\nbatch_size=100,\n\n\n\n\n\n# --- Create a Vector Store Index using the Vector Store and given Documents ---\n\n\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,storage_context=storage_context\n\n\n\n```\n\n## Query the Vector Store\n[Section titled \u201cQuery the Vector Store\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#query-the-vector-store)\n```\n\n# --- Generate Response ---\n\n\n# --- Set Logging to DEBUG for more Detailed Outputs ---\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nresponse = vector_store.generate_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\"\n\n\n\n\n\nmoorcheh_response = vector_store.get_generative_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\",\n\n\n\n\nai_model=\"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n\n\n\n\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nresponse,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nmoorcheh_response,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\n# --- Filters for Metadata ---\n\n\n\nfilter=MetadataFilters(\n\n\n\n\nfilters=[\n\n\n\n\nMetadataFilter(\n\n\n\n\nkey=\"file_path\",\n\n\n\n\nvalue=\"insert the file path to the document here\",\n\n\n\n\noperator=FilterOperator.EQ,\n\n\n\n\n\n\ncondition=FilterCondition.AND,\n\n\n\n```\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#_top)\n# Moorcheh Vector Store Demo \n## Install Required Packages\n[Section titled \u201cInstall Required Packages\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#install-required-packages)\n```\n\n\n!pip install llama_index\n\n\n\n\n!pip install moorcheh_sdk\n\n\n```\n\n## Import Required Libraries\n[Section titled \u201cImport Required Libraries\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#import-required-libraries)\ndemo.py```\n\n# --- Welcome to the Demo of the Moorcheh Vector Store ---\n\n\n# --- Import the following packages --\n\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\nimport os\n\n\n\n\nfrom moorcheh_sdk import MoorchehClient\n\n\n\n\nfrom IPython.display import Markdown, display\n\n\n\n\nfrom typing import Any, Callable, Dict, List, Optional, cast\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nStorageContext,\n\n\n\n\nSettings,\n\n\n\n\n\nfrom llama_index.core.base.embeddings.base_sparse import BaseSparseEmbedding\n\n\n\n\nfrom llama_index.core.bridge.pydantic import PrivateAttr\n\n\n\n\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nBasePydanticVectorStore,\n\n\n\n\nMetadataFilters,\n\n\n\n\nVectorStoreQuery,\n\n\n\n\nVectorStoreQueryMode,\n\n\n\n\nVectorStoreQueryResult,\n\n\n\n\n\nfrom llama_index.core.vector_stores.utils import (\n\n\n\n\nDEFAULT_TEXT_KEY,\n\n\n\n\nlegacy_metadata_dict_to_node,\n\n\n\n\nmetadata_dict_to_node,\n\n\n\n\nnode_to_metadata_dict,\n\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nMetadataFilter,\n\n\n\n\nMetadataFilters,\n\n\n\n\nFilterOperator,\n\n\n\n\nFilterCondition,\n\n\n\n```\n\n## Configure Logging\n[Section titled \u201cConfigure Logging\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#configure-logging)\n```\n\n# --- Logging Setup ---\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n## Load Moorcheh API Key\n[Section titled \u201cLoad Moorcheh API Key\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-moorcheh-api-key)\n```\n\n# --- Set the values of the API Keys in your Environment Variables ---\n\n\n\nfrom google.colab import userdata\n\n\n\n\n\napi_key = os.environ[\"MOORCHEH_API_KEY\"] = userdata.get(\"MOORCHEH_API_KEY\")\n\n\n\n\n\nif\"MOORCHEH_API_KEY\"notin os.environ:\n\n\n\n\nraiseEnvironmentError(f\"Environment variable MOORCHEH_API_KEY is not set\")\n\n\n```\n\n## Load and Chunk Documents\n[Section titled \u201cLoad and Chunk Documents\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-and-chunk-documents)\n```\n\n# --- Load Documents ---\n\n\n\ndocuments =SimpleDirectoryReader(\"./documents\").load_data()\n\n\n\n\n# --- Set chunk size and overlap ---\n\n\n\nSettings.chunk_size =1024\n\n\n\n\nSettings.chunk_overlap =20\n\n\n```\n\n## Initialize Vector Store and Create Index\n[Section titled \u201cInitialize Vector Store and Create Index\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#initialize-vector-store-and-create-index)\n```\n\n# --- Initialize the Moorcheh Vector Store ---\n\n\n\n__all__ =[\"MoorchehVectorStore\"]\n\n\n\n\n# Creates a Moorcheh Vector Store with the following parameters\n\n\n# For text-based namespaces, set namespace_type to \"text\" and vector_dimension to None\n\n\n# For vector-based namespaces, set namespace_type to \"vector\" and vector_dimension to the dimension of your uploaded vectors\n\n\n\nvector_store =MoorchehVectorStore(\n\n\n\n\napi_key=api_key,\n\n\n\n\nnamespace=\"llamaindex_moorcheh\",\n\n\n\n\nnamespace_type=\"text\",\n\n\n\n\nvector_dimension=None,\n\n\n\n\nadd_sparse_vector=False,\n\n\n\n\nbatch_size=100,\n\n\n\n\n\n# --- Create a Vector Store Index using the Vector Store and given Documents ---\n\n\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,storage_context=storage_context\n\n\n\n```\n\n## Query the Vector Store\n[Section titled \u201cQuery the Vector Store\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#query-the-vector-store)\n```\n\n# --- Generate Response ---\n\n\n# --- Set Logging to DEBUG for more Detailed Outputs ---\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nresponse = vector_store.generate_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\"\n\n\n\n\n\nmoorcheh_response = vector_store.get_generative_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\",\n\n\n\n\nai_model=\"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n\n\n\n\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nresponse,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nmoorcheh_response,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\n# --- Filters for Metadata ---\n\n\n\nfilter=MetadataFilters(\n\n\n\n\nfilters=[\n\n\n\n\nMetadataFilter(\n\n\n\n\nkey=\"file_path\",\n\n\n\n\nvalue=\"insert the file path to the document here\",\n\n\n\n\noperator=FilterOperator.EQ,\n\n\n\n\n\n\ncondition=FilterCondition.AND,\n\n\n\n```\n\n"}, "__type__": "4"}, "666f45c1-93d4-4081-8d75-7275a24aceff": {"__data__": {"id_": "666f45c1-93d4-4081-8d75-7275a24aceff", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_workflow_sub_question_query_engine.md", "file_name": "python_examples_workflow_sub_question_query_engine.md", "file_type": "text/markdown", "file_size": 38883, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#_top)\n# Sub Question Query Engine as a workflow \nLlamaIndex has a built-in Sub-Question Query Engine. Here, we replace it with a Workflow-based equivalent.\nFirst we install our dependencies:\n  * LlamaIndex core for most things\n  * OpenAI LLM and embeddings for LLM actions\n  * `llama-index-readers-file` to power the PDF reader in `SimpleDirectoryReader`\n\n\n```\n\n\n!pip install llama-index-core llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file llama-index-utils-workflow\n\n\n```\n\nBring in our dependencies as imports:\n```\n\n\nimport os, json\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nStorageContext,\n\n\n\n\nload_index_from_storage,\n\n\n\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\nfrom llama_index.core.workflow import (\n\n\n\n\nstep,\n\n\n\n\nContext,\n\n\n\n\nWorkflow,\n\n\n\n\nEvent,\n\n\n\n\nStartEvent,\n\n\n\n\nStopEvent,\n\n\n\n\n\nfrom llama_index.core.agent import ReActAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.utils.workflow import draw_all_possible_flows\n\n\n```\n\n# Define the Sub Question Query Engine as a Workflow\n[Section titled \u201cDefine the Sub Question Query Engine as a Workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#define-the-sub-question-query-engine-as-a-workflow)\n  * Our StartEvent goes to `query()`, which takes care of several things:\n    * Accepts and stores the original query\n    * Stores the LLM to handle the queries\n    * Stores the list of tools to enable sub-questions\n    * Passes the original question to the LLM, asking it to split up the question into sub-questions\n    * Fires off a `QueryEvent` for every sub-question generated\n  * QueryEvents go to `sub_question()`, which instantiates a new ReAct agent with the full list of tools available and lets it select which one to use.\n    * This is slightly better than the actual SQQE built-in to LlamaIndex, which cannot use multiple tools\n    * Each QueryEvent generates an `AnswerEvent`\n  * AnswerEvents go to `combine_answers()`.\n    * This uses `self.collect_events()` to wait for every QueryEvent to return an answer.\n    * All the answers are then combined into a final prompt for the LLM to consolidate them into a single response\n    * A StopEvent is generated to return the final result\n\n\n```\n\n\nclassQueryEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\n\n\nclassAnswerEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\nanswer: str\n\n\n\n\n\n\nclassSubQuestionQueryEngine(Workflow):\n\n\n\n\n@step\n\n\n\n\nasyncdefquery(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n\n\n\n\nifhasattr(ev,\"query\"):\n\n\n\n\nawait ctx.store.set(\"original_query\", ev.query)\n\n\n\n\nprint(f\"Query is {await ctx.store.get('original_query')}\")\n\n\n\n\n\nifhasattr(ev,\"llm\"):\n\n\n\n\nawait ctx.store.set(\"llm\", ev.llm)\n\n\n\n\n\nifhasattr(ev,\"tools\"):\n\n\n\n\nawait ctx.store.set(\"tools\", ev.tools)\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(\n\n\n\n\nf\"\"\"\n\n\n\n\nGiven a user question, and a list of tools, output a list of\n\n\n\n\nrelevant sub-questions, such that the answers to all the\n\n\n\n\nsub-questions put together will answer the question. Respond\n\n\n\n\nin pure JSON without any markdown, like this:\n\n\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What is the population of San Francisco?\",\n\n\n\n\n\"What is the budget of San Francisco?\",\n\n\n\n\n\"What is the GDP of San Francisco?\"\n\n\n\n\n\n\nHere is the user question: {await ctx.store.get('original_query')}\n\n\n\n\n\nAnd here is the list of tools: {await ctx.store.get('tools')}\n\n\n\n\n\n\n\nprint(f\"Sub-questions are {response}\")\n\n\n\n\n\nresponse_obj = json.loads(str(response))\n\n\n\n\nsub_questions = response_obj[\"sub_questions\"]\n\n\n\n\n\nawait ctx.store.set(\"sub_question_count\",(sub_questions))\n\n\n\n\n\nfor question in sub_questions:\n\n\n\n\nself.send_event(QueryEvent(question=question))\n\n\n\n\n\nreturnNone\n\n\n\n\n\n@step\n\n\n\n\nasyncdefsub_question(self, ctx: Context, ev: QueryEvent) -> AnswerEvent:\n\n\n\n\nprint(f\"Sub-question is {ev.question}\")\n\n\n\n\n\nagent = ReActAgent.from_tools(\n\n\n\n\nawait ctx.store.get(\"tools\"),\n\n\n\n\nllm=await ctx.store.get(\"llm\"),\n\n\n\n\nverbose=True,\n\n\n\n\n\nresponse = agent.chat(ev.question)\n\n\n\n\n\nreturnAnswerEvent(question=ev.question,answer=str(response))\n\n\n\n\n\n@step\n\n\n\n\nasyncdefcombine_answers(\n\n\n\n\nself, ctx: Context, ev: AnswerEvent\n\n\n\n\n) -> StopEvent |None:\n\n\n\n\nready = ctx.collect_events(\n\n\n\n\nev,[AnswerEvent]*await ctx.store.get(\"sub_question_count\")\n\n\n\n\n\nif ready isNone:\n\n\n\n\nreturnNone\n\n\n\n\n\nanswers =\"\\n\\n\".join(\n\n\n\n\n\nf\"Question: {event.question}: \\n Answer: {event.answer}\"\n\n\n\n\nfor event in ready\n\n\n\n\n\n\n\nprompt =f\"\"\"\n\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: {await ctx.store.get('original_query')}\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\n{answers}\n\n\n\n\n\n\nprint(f\"Final prompt is {prompt}\")\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(prompt)\n\n\n\n\n\nprint(\"Final response is\", response)\n\n\n\n\n\nreturnStopEvent(result=str(response))\n\n\n```\n\n```\n\n\ndraw_all_possible_flows(\n\n\n\n\nSubQuestionQueryEngine,filename=\"sub_question_query_engine.html\"\n\n\n\n```\n\n```\n\nsub_question_query_engine.html\n\n```\n\nVisualizing this flow looks pretty linear, since it doesn\u2019t capture that `query()` can generate multiple parallel `QueryEvents` which get collected into `combine_answers`.\n# Download data to demo\n[Section titled \u201cDownload data to demo\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#download-data-to-demo)\n```\n\n\n!mkdir -p \"./data/sf_budgets/\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\"-O \"./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\"-O \"./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\"-O \"./data/sf_budgets/2018 - 2018-o0181-18.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\"-O \"./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\"-O \"./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\"-O \"./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\"-O \"./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\"\n\n\n```\n\n```\n\n--2024-08-07 18:21:11--  https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file# [following]\n\n\n--2024-08-07 18:21:12--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file\n\n\nResolving ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file [following]\n\n\n--2024-08-07 18:21:13--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file\n\n\nReusing existing connection to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 29467998 (28M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  28.10M   173MB/s    in 0.2s\n\n\n\n2024-08-07 18:21:14 (173 MB/s) - \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019 saved [29467998/29467998]\n\n\n\n--2024-08-07 18:21:14--  https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file# [following]\n\n\n--2024-08-07 18:21:15--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file\n\n\nResolving uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file [following]\n\n\n--2024-08-07 18:21:16--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file\n\n\nReusing existing connection to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13463517 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.84M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:17 (136 MB/s) - \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019 saved [13463517/13463517]\n\n\n\n--2024-08-07 18:21:17--  https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file# [following]\n\n\n--2024-08-07 18:21:17--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file\n\n\nResolving uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file [following]\n\n\n--2024-08-07 18:21:18--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file\n\n\nReusing existing connection to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 18487865 (18M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  17.63M  --.-KB/s    in 0.1s\n\n\n\n2024-08-07 18:21:19 (149 MB/s) - \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019 saved [18487865/18487865]\n\n\n\n--2024-08-07 18:21:19--  https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file# [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file\n\n\nResolving uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file\n\n\nReusing existing connection to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13123938 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.52M  --.-KB/s    in 0.08s\n\n\n\n2024-08-07 18:21:22 (161 MB/s) - \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019 saved [13123938/13123938]\n\n\n\n--2024-08-07 18:21:22--  https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file# [following]\n\n\n--2024-08-07 18:21:22--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file\n\n\nResolving uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file [following]\n\n\n--2024-08-07 18:21:24--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file\n\n\nReusing existing connection to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3129122 (3.0M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   2.98M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:24 (66.3 MB/s) - \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019 saved [3129122/3129122]\n\n\n\n--2024-08-07 18:21:24--  https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file# [following]\n\n\n--2024-08-07 18:21:26--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file\n\n\nResolving uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file [following]\n\n\n--2024-08-07 18:21:27--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file\n\n\nReusing existing connection to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3233272 (3.1M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   3.08M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:28 (61.4 MB/s) - \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019 saved [3233272/3233272]\n\n\n\n--2024-08-07 18:21:28--  https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file# [following]\n\n\n--2024-08-07 18:21:29--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file\n\n\nResolving uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file [following]\n\n\n--2024-08-07 18:21:30--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file\n\n\nReusing existing connection to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 10550407 (10M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  10.06M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:31 (110 MB/s) - \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019 saved [10550407/10550407]\n\n```\n\n# Load data and run the workflow\n[Section titled \u201cLoad data and run the workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#load-data-and-run-the-workflow)\nJust like using the built-in Sub-Question Query Engine, we create our query tools and instantiate an LLM and pass them in.\nEach tool is its own query engine based on a single (very lengthy) San Francisco budget document, each of which is 300+ pages. To save time on repeated runs, we persist our generated indexes to disk.\n```\n\n\nfrom google.colab import userdata\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = userdata.get(\"openai-key\")\n\n\n\n\n\nfolder =\"./data/sf_budgets/\"\n\n\n\n\nfiles = os.listdir(folder)\n\n\n\n\n\nquery_engine_tools =[]\n\n\n\n\nforfilein files:\n\n\n\n\nyear =file.split(\"\")[0]\n\n\n\n\nindex_persist_path =f\"./storage/budget-{year}/\"\n\n\n\n\n\nif os.path.exists(index_persist_path):\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=index_persist_path\n\n\n\n\n\nindex =load_index_from_storage(storage_context)\n\n\n\n\nelse:\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[folder +]\n\n\n\n\n).load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nindex.storage_context.persist(index_persist_path)\n\n\n\n\n\nengine = index.as_query_engine()\n\n\n\n\nquery_engine_tools.append(\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"budget_{year}\",\n\n\n\n\ndescription=f\"Information about San Francisco's budget in {year}\",\n\n\n\n\n\n\n\n\nengine =SubQuestionQueryEngine(timeout=120,verbose=True)\n\n\n\n\nllm =OpenAI(model=\"gpt-4o\")\n\n\n\n\nresult =await engine.run(\n\n\n\n\nllm=llm,\n\n\n\n\ntools=query_engine_tools,\n\n\n\n\nquery=\"How has the total amount of San Francisco's budget changed from 2016 to 2023?\",\n\n\n\n\n\n\nprint(result)\n\n\n```\n\n```\n\nRunning step query\n\n\nQuery is How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\nSub-questions are {\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2016?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2017?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2018?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2019?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2020?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2021?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2022?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2023?\"\n\n\n\n\n\nStep query produced no event\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2016?\n\n\n> Running step 61365946-614c-4895-8fc3-0968f2d63387. Step input: What was the total amount of San Francisco's budget in 2016?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2016\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2016\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0m> Running step a85aa30e-a980-4897-a52e-e82b8fb25c72. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2017?\n\n\n> Running step 5d14466c-1400-4a26-ac42-021e7143d3b1. Step input: What was the total amount of San Francisco's budget in 2017?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2017\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2017\"}\n\n\n[0m[1;3;34mObservation: $10,106.9 million\n\n\n[0m> Running step 586a5fab-95ee-44e9-9a35-fcf19993b13e. Step input: None\n\n\n[1;3;38;5;200mThought: I have the information needed to answer the question.\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2018?\n\n\n> Running step d39f64d0-65f6-4571-95ad-d28a16198ea5. Step input: What was the total amount of San Francisco's budget in 2018?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2018\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2018\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0m> Running step 3f67feee-489c-4b9e-8f27-37f0d48e3b0d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2019?\n\n\n> Running step d5ac0866-b02c-4c4c-94c6-f0e047ebb0fe. Step input: What was the total amount of San Francisco's budget in 2019?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2019\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2019\"}\n\n\n[0m[1;3;34mObservation: $12.3 billion\n\n\n[0m> Running step 3b62859b-bbd3-4dce-b284-f9b398e370c2. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2020?\n\n\n> Running step 41f6ed9f-d695-43df-8743-39dfcc3d919d. Step input: What was the total amount of San Francisco's budget in 2020?\n\n\n[1;3;38;5;200mThought: The user is asking for the total amount of San Francisco's budget in 2020. I do not have a tool specifically for the 2020 budget. I will check the available tools to see if they provide any relevant information or if I can infer the 2020 budget from adjacent years.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"What was the total amount of San Francisco's budget in 2020?\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0m> Running step ea39f7c6-e942-4a41-8963-f37d4a27d559. Step input: None\n\n\n[1;3;38;5;200mThought: I now have the information needed to answer the user's question about the total amount of San Francisco's budget in 2020.\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2021?\n\n\n> Running step 6662fd06-e86a-407c-bb89-4828f63caa72. Step input: What was the total amount of San Francisco's budget in 2021?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2021\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2021 is $14,166,496,000.\n\n\n[0m> Running step 5d0cf9da-2c14-407c-8ae5-5cd638c1fb5c. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2022?\n\n\n> Running step 62fa9a5f-f40b-489d-9773-5ee4e4eaba9e. Step input: What was the total amount of San Francisco's budget in 2022?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2022\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2022\"}\n\n\n[0m[1;3;34mObservation: $14,550,060\n\n\n[0m> Running step 7a2d5623-cc75-4c9d-8c58-3dbc2faf163d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2023?\n\n\n> Running step 839eb994-b4e2-4019-a170-9471c1e0d764. Step input: What was the total amount of San Francisco's budget in 2023?\n\n\n[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2023\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2023\"}\n\n\n[0m[1;3;34mObservation: $14.6 billion\n\n\n[0m> Running step 38779f6c-d0c7-4c95-b5c4-0b170c5ed0d5. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nFinal prompt is\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2016?:\n\n\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2017?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2018?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2019?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2020?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2021?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2022?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2023?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n\n\nFinal response is From 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n\nStep combine_answers produced event StopEvent\n\n\nFrom 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n```\n\nOur debug output is lengthy! You can see the sub-questions being generated and then `sub_question()` being repeatedly invoked, each time generating a brief log of ReAct agent thoughts and actions to answer each smaller question.\nYou can see `combine_answers` running multiple times; these were triggered by each `AnswerEvent` but before all 8 `AnswerEvents` were collected. On its final run it generates a full prompt, combines the answers and returns the result.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#_top)\n# Sub Question Query Engine as a workflow \nLlamaIndex has a built-in Sub-Question Query Engine. Here, we replace it with a Workflow-based equivalent.\nFirst we install our dependencies:\n  * LlamaIndex core for most things\n  * OpenAI LLM and embeddings for LLM actions\n  * `llama-index-readers-file` to power the PDF reader in `SimpleDirectoryReader`\n\n\n```\n\n\n!pip install llama-index-core llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file llama-index-utils-workflow\n\n\n```\n\nBring in our dependencies as imports:\n```\n\n\nimport os, json\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nStorageContext,\n\n\n\n\nload_index_from_storage,\n\n\n\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\nfrom llama_index.core.workflow import (\n\n\n\n\nstep,\n\n\n\n\nContext,\n\n\n\n\nWorkflow,\n\n\n\n\nEvent,\n\n\n\n\nStartEvent,\n\n\n\n\nStopEvent,\n\n\n\n\n\nfrom llama_index.core.agent import ReActAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.utils.workflow import draw_all_possible_flows\n\n\n```\n\n# Define the Sub Question Query Engine as a Workflow\n[Section titled \u201cDefine the Sub Question Query Engine as a Workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#define-the-sub-question-query-engine-as-a-workflow)\n  * Our StartEvent goes to `query()`, which takes care of several things:\n    * Accepts and stores the original query\n    * Stores the LLM to handle the queries\n    * Stores the list of tools to enable sub-questions\n    * Passes the original question to the LLM, asking it to split up the question into sub-questions\n    * Fires off a `QueryEvent` for every sub-question generated\n  * QueryEvents go to `sub_question()`, which instantiates a new ReAct agent with the full list of tools available and lets it select which one to use.\n    * This is slightly better than the actual SQQE built-in to LlamaIndex, which cannot use multiple tools\n    * Each QueryEvent generates an `AnswerEvent`\n  * AnswerEvents go to `combine_answers()`.\n    * This uses `self.collect_events()` to wait for every QueryEvent to return an answer.\n    * All the answers are then combined into a final prompt for the LLM to consolidate them into a single response\n    * A StopEvent is generated to return the final result\n\n\n```\n\n\nclassQueryEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\n\n\nclassAnswerEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\nanswer: str\n\n\n\n\n\n\nclassSubQuestionQueryEngine(Workflow):\n\n\n\n\n@step\n\n\n\n\nasyncdefquery(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n\n\n\n\nifhasattr(ev,\"query\"):\n\n\n\n\nawait ctx.store.set(\"original_query\", ev.query)\n\n\n\n\nprint(f\"Query is {await ctx.store.get('original_query')}\")\n\n\n\n\n\nifhasattr(ev,\"llm\"):\n\n\n\n\nawait ctx.store.set(\"llm\", ev.llm)\n\n\n\n\n\nifhasattr(ev,\"tools\"):\n\n\n\n\nawait ctx.store.set(\"tools\", ev.tools)\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(\n\n\n\n\nf\"\"\"\n\n\n\n\nGiven a user question, and a list of tools, output a list of\n\n\n\n\nrelevant sub-questions, such that the answers to all the\n\n\n\n\nsub-questions put together will answer the question. Respond\n\n\n\n\nin pure JSON without any markdown, like this:\n\n\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What is the population of San Francisco?\",\n\n\n\n\n\"What is the budget of San Francisco?\",\n\n\n\n\n\"What is the GDP of San Francisco?\"\n\n\n\n\n\n\nHere is the user question: {await ctx.store.get('original_query')}\n\n\n\n\n\nAnd here is the list of tools: {await ctx.store.get('tools')}\n\n\n\n\n\n\n\nprint(f\"Sub-questions are {response}\")\n\n\n\n\n\nresponse_obj = json.loads(str(response))\n\n\n\n\nsub_questions = response_obj[\"sub_questions\"]\n\n\n\n\n\nawait ctx.store.set(\"sub_question_count\",(sub_questions))\n\n\n\n\n\nfor question in sub_questions:\n\n\n\n\nself.send_event(QueryEvent(question=question))\n\n\n\n\n\nreturnNone\n\n\n\n\n\n@step\n\n\n\n\nasyncdefsub_question(self, ctx: Context, ev: QueryEvent) -> AnswerEvent:\n\n\n\n\nprint(f\"Sub-question is {ev.question}\")\n\n\n\n\n\nagent = ReActAgent.from_tools(\n\n\n\n\nawait ctx.store.get(\"tools\"),\n\n\n\n\nllm=await ctx.store.get(\"llm\"),\n\n\n\n\nverbose=True,\n\n\n\n\n\nresponse = agent.chat(ev.question)\n\n\n\n\n\nreturnAnswerEvent(question=ev.question,answer=str(response))\n\n\n\n\n\n@step\n\n\n\n\nasyncdefcombine_answers(\n\n\n\n\nself, ctx: Context, ev: AnswerEvent\n\n\n\n\n) -> StopEvent |None:\n\n\n\n\nready = ctx.collect_events(\n\n\n\n\nev,[AnswerEvent]*await ctx.store.get(\"sub_question_count\")\n\n\n\n\n\nif ready isNone:\n\n\n\n\nreturnNone\n\n\n\n\n\nanswers =\"\\n\\n\".join(\n\n\n\n\n\nf\"Question: {event.question}: \\n Answer: {event.answer}\"\n\n\n\n\nfor event in ready\n\n\n\n\n\n\n\nprompt =f\"\"\"\n\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: {await ctx.store.get('original_query')}\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\n{answers}\n\n\n\n\n\n\nprint(f\"Final prompt is {prompt}\")\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(prompt)\n\n\n\n\n\nprint(\"Final response is\", response)\n\n\n\n\n\nreturnStopEvent(result=str(response))\n\n\n```\n\n```\n\n\ndraw_all_possible_flows(\n\n\n\n\nSubQuestionQueryEngine,filename=\"sub_question_query_engine.html\"\n\n\n\n```\n\n```\n\nsub_question_query_engine.html\n\n```\n\nVisualizing this flow looks pretty linear, since it doesn\u2019t capture that `query()` can generate multiple parallel `QueryEvents` which get collected into `combine_answers`.\n# Download data to demo\n[Section titled \u201cDownload data to demo\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#download-data-to-demo)\n```\n\n\n!mkdir -p \"./data/sf_budgets/\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\"-O \"./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\"-O \"./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\"-O \"./data/sf_budgets/2018 - 2018-o0181-18.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\"-O \"./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\"-O \"./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\"-O \"./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\"-O \"./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\"\n\n\n```\n\n```\n\n--2024-08-07 18:21:11--  https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file# [following]\n\n\n--2024-08-07 18:21:12--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file\n\n\nResolving ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file [following]\n\n\n--2024-08-07 18:21:13--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file\n\n\nReusing existing connection to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 29467998 (28M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  28.10M   173MB/s    in 0.2s\n\n\n\n2024-08-07 18:21:14 (173 MB/s) - \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019 saved [29467998/29467998]\n\n\n\n--2024-08-07 18:21:14--  https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file# [following]\n\n\n--2024-08-07 18:21:15--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file\n\n\nResolving uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file [following]\n\n\n--2024-08-07 18:21:16--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file\n\n\nReusing existing connection to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13463517 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.84M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:17 (136 MB/s) - \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019 saved [13463517/13463517]\n\n\n\n--2024-08-07 18:21:17--  https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file# [following]\n\n\n--2024-08-07 18:21:17--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file\n\n\nResolving uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file [following]\n\n\n--2024-08-07 18:21:18--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file\n\n\nReusing existing connection to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 18487865 (18M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  17.63M  --.-KB/s    in 0.1s\n\n\n\n2024-08-07 18:21:19 (149 MB/s) - \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019 saved [18487865/18487865]\n\n\n\n--2024-08-07 18:21:19--  https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file# [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file\n\n\nResolving uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file\n\n\nReusing existing connection to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13123938 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.52M  --.-KB/s    in 0.08s\n\n\n\n2024-08-07 18:21:22 (161 MB/s) - \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019 saved [13123938/13123938]\n\n\n\n--2024-08-07 18:21:22--  https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file# [following]\n\n\n--2024-08-07 18:21:22--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file\n\n\nResolving uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file [following]\n\n\n--2024-08-07 18:21:24--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file\n\n\nReusing existing connection to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3129122 (3.0M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   2.98M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:24 (66.3 MB/s) - \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019 saved [3129122/3129122]\n\n\n\n--2024-08-07 18:21:24--  https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file# [following]\n\n\n--2024-08-07 18:21:26--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file\n\n\nResolving uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file [following]\n\n\n--2024-08-07 18:21:27--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file\n\n\nReusing existing connection to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3233272 (3.1M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   3.08M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:28 (61.4 MB/s) - \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019 saved [3233272/3233272]\n\n\n\n--2024-08-07 18:21:28--  https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file# [following]\n\n\n--2024-08-07 18:21:29--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file\n\n\nResolving uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file [following]\n\n\n--2024-08-07 18:21:30--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file\n\n\nReusing existing connection to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 10550407 (10M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  10.06M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:31 (110 MB/s) - \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019 saved [10550407/10550407]\n\n```\n\n# Load data and run the workflow\n[Section titled \u201cLoad data and run the workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#load-data-and-run-the-workflow)\nJust like using the built-in Sub-Question Query Engine, we create our query tools and instantiate an LLM and pass them in.\nEach tool is its own query engine based on a single (very lengthy) San Francisco budget document, each of which is 300+ pages. To save time on repeated runs, we persist our generated indexes to disk.\n```\n\n\nfrom google.colab import userdata\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = userdata.get(\"openai-key\")\n\n\n\n\n\nfolder =\"./data/sf_budgets/\"\n\n\n\n\nfiles = os.listdir(folder)\n\n\n\n\n\nquery_engine_tools =[]\n\n\n\n\nforfilein files:\n\n\n\n\nyear =file.split(\"\")[0]\n\n\n\n\nindex_persist_path =f\"./storage/budget-{year}/\"\n\n\n\n\n\nif os.path.exists(index_persist_path):\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=index_persist_path\n\n\n\n\n\nindex =load_index_from_storage(storage_context)\n\n\n\n\nelse:\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[folder +]\n\n\n\n\n).load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nindex.storage_context.persist(index_persist_path)\n\n\n\n\n\nengine = index.as_query_engine()\n\n\n\n\nquery_engine_tools.append(\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"budget_{year}\",\n\n\n\n\ndescription=f\"Information about San Francisco's budget in {year}\",\n\n\n\n\n\n\n\n\nengine =SubQuestionQueryEngine(timeout=120,verbose=True)\n\n\n\n\nllm =OpenAI(model=\"gpt-4o\")\n\n\n\n\nresult =await engine.run(\n\n\n\n\nllm=llm,\n\n\n\n\ntools=query_engine_tools,\n\n\n\n\nquery=\"How has the total amount of San Francisco's budget changed from 2016 to 2023?\",\n\n\n\n\n\n\nprint(result)\n\n\n```\n\n```\n\nRunning step query\n\n\nQuery is How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\nSub-questions are {\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2016?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2017?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2018?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2019?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2020?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2021?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2022?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2023?\"\n\n\n\n\n\nStep query produced no event\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2016?\n\n\n> Running step 61365946-614c-4895-8fc3-0968f2d63387. Step input: What was the total amount of San Francisco's budget in 2016?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2016\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2016\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0m> Running step a85aa30e-a980-4897-a52e-e82b8fb25c72. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2017?\n\n\n> Running step 5d14466c-1400-4a26-ac42-021e7143d3b1. Step input: What was the total amount of San Francisco's budget in 2017?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2017\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2017\"}\n\n\n[0m[1;3;34mObservation: $10,106.9 million\n\n\n[0m> Running step 586a5fab-95ee-44e9-9a35-fcf19993b13e. Step input: None\n\n\n[1;3;38;5;200mThought: I have the information needed to answer the question.\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2018?\n\n\n> Running step d39f64d0-65f6-4571-95ad-d28a16198ea5. Step input: What was the total amount of San Francisco's budget in 2018?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2018\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2018\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0m> Running step 3f67feee-489c-4b9e-8f27-37f0d48e3b0d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2019?\n\n\n> Running step d5ac0866-b02c-4c4c-94c6-f0e047ebb0fe. Step input: What was the total amount of San Francisco's budget in 2019?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2019\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2019\"}\n\n\n[0m[1;3;34mObservation: $12.3 billion\n\n\n[0m> Running step 3b62859b-bbd3-4dce-b284-f9b398e370c2. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2020?\n\n\n> Running step 41f6ed9f-d695-43df-8743-39dfcc3d919d. Step input: What was the total amount of San Francisco's budget in 2020?\n\n\n[1;3;38;5;200mThought: The user is asking for the total amount of San Francisco's budget in 2020. I do not have a tool specifically for the 2020 budget. I will check the available tools to see if they provide any relevant information or if I can infer the 2020 budget from adjacent years.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"What was the total amount of San Francisco's budget in 2020?\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0m> Running step ea39f7c6-e942-4a41-8963-f37d4a27d559. Step input: None\n\n\n[1;3;38;5;200mThought: I now have the information needed to answer the user's question about the total amount of San Francisco's budget in 2020.\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2021?\n\n\n> Running step 6662fd06-e86a-407c-bb89-4828f63caa72. Step input: What was the total amount of San Francisco's budget in 2021?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2021\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2021 is $14,166,496,000.\n\n\n[0m> Running step 5d0cf9da-2c14-407c-8ae5-5cd638c1fb5c. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2022?\n\n\n> Running step 62fa9a5f-f40b-489d-9773-5ee4e4eaba9e. Step input: What was the total amount of San Francisco's budget in 2022?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2022\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2022\"}\n\n\n[0m[1;3;34mObservation: $14,550,060\n\n\n[0m> Running step 7a2d5623-cc75-4c9d-8c58-3dbc2faf163d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2023?\n\n\n> Running step 839eb994-b4e2-4019-a170-9471c1e0d764. Step input: What was the total amount of San Francisco's budget in 2023?\n\n\n[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2023\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2023\"}\n\n\n[0m[1;3;34mObservation: $14.6 billion\n\n\n[0m> Running step 38779f6c-d0c7-4c95-b5c4-0b170c5ed0d5. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nFinal prompt is\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2016?:\n\n\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2017?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2018?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2019?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2020?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2021?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2022?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2023?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n\n\nFinal response is From 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n\nStep combine_answers produced event StopEvent\n\n\nFrom 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n```\n\nOur debug output is lengthy! You can see the sub-questions being generated and then `sub_question()` being repeatedly invoked, each time generating a brief log of ReAct agent thoughts and actions to answer each smaller question.\nYou can see `combine_answers` running multiple times; these were triggered by each `AnswerEvent` but before all 8 `AnswerEvents` were collected. On its final run it generates a full prompt, combines the answers and returns the result.\n"}, "__type__": "4"}, "23f40994-b97b-4321-b26c-c39a6c2361a5": {"__data__": {"id_": "23f40994-b97b-4321-b26c-c39a6c2361a5", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework-api-reference_schema.md", "file_name": "python_framework-api-reference_schema.md", "file_type": "text/markdown", "file_size": 86114, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "# Index\nBase schema for data structures.\n##  BaseComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent \"Permanent link\")\nBases: `BaseModel`\nBase component object to capture class names.\nSource code in `llama_index/core/schema.py`\n```\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n```\n| ```\nclass BaseComponent(BaseModel):\n\"\"\"Base component object to capture class names.\"\"\"\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)\n        json_schema = handler.resolve_ref_schema(json_schema)\n\n        # inject class name to help with serde\n        if \"properties\" in json_schema:\n            json_schema[\"properties\"][\"class_name\"] = {\n                \"title\": \"Class Name\",\n                \"type\": \"string\",\n                \"default\": cls.class_name(),\n            }\n        return json_schema\n\n    @classmethod\n    def class_name(cls) -> str:\n\"\"\"\n        Get the class name, used as a unique ID in serialization.\n\n        This provides a key that makes serialization robust against actual class\n        name changes.\n        \"\"\"\n        return \"base_component\"\n\n    def json(self, **kwargs: Any) -> str:\n        return self.to_json(**kwargs)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n        data = handler(self)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        return self.model_dump(**kwargs)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = super().__getstate__()\n\n        # remove attributes that are not pickleable -- kind of dangerous\n        keys_to_remove = []\n        for key, val in state[\"__dict__\"].items():\n            try:\n                pickle.dumps(val)\n            except Exception:\n                keys_to_remove.append(key)\n\n        for key in keys_to_remove:\n            logging.warning(f\"Removing unpickleable attribute {key}\")\n            del state[\"__dict__\"][key]\n\n        # remove private attributes if they aren't pickleable -- kind of dangerous\n        keys_to_remove = []\n        private_attrs = state.get(\"__pydantic_private__\", None)\n        if private_attrs:\n            for key, val in state[\"__pydantic_private__\"].items():\n                try:\n                    pickle.dumps(val)\n                except Exception:\n                    keys_to_remove.append(key)\n\n            for key in keys_to_remove:\n                logging.warning(f\"Removing unpickleable private attribute {key}\")\n                del state[\"__pydantic_private__\"][key]\n\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        # Use the __dict__ and __init__ method to set state\n        # so that all variables initialize\n        try:\n            self.__init__(**state[\"__dict__\"])  # type: ignore\n        except Exception:\n            # Fall back to the default __setstate__ method\n            # This may not work if the class had unpickleable attributes\n            super().__setstate__(state)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = self.dict(**kwargs)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def to_json(self, **kwargs: Any) -> str:\n        data = self.to_dict(**kwargs)\n        return json.dumps(data)\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        # In SimpleKVStore we rely on shallow coping. Hence, the data will be modified in the store directly.\n        # And it is the same when the user is passing a dictionary to create a component. We can't modify the passed down dictionary.\n        data = dict(data)\n        if isinstance(kwargs, dict):\n            data.update(kwargs)\n        data.pop(\"class_name\", None)\n        return cls(**data)\n\n    @classmethod\n    def from_json(cls, data_str: str, **kwargs: Any) -> Self:  # type: ignore\n        data = json.loads(data_str)\n        return cls.from_dict(data, **kwargs)\n\n```\n  \n---|---  \n###  class_name `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent.class_name \"Permanent link\")\n```\nclass_name() -> \n\n```\n\nGet the class name, used as a unique ID in serialization.\nThis provides a key that makes serialization robust against actual class name changes.\nSource code in `llama_index/core/schema.py`\n```\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n```\n| ```\n@classmethod\ndef class_name(cls) -> str:\n\"\"\"\n    Get the class name, used as a unique ID in serialization.\n\n    This provides a key that makes serialization robust against actual class\n    name changes.\n    \"\"\"\n    return \"base_component\"\n\n```\n  \n---|---  \n##  TransformComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent \"Permanent link\")\nBases: , `DispatcherSpanMixin`\nBase class for transform components.\nSource code in `llama_index/core/schema.py`\n```\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n```\n| ```\nclass TransformComponent(BaseComponent, DispatcherSpanMixin):\n\"\"\"Base class for transform components.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:\n\"\"\"Transform nodes.\"\"\"\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n        return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n###  acall `async` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent.acall \"Permanent link\")\n```\nacall(nodes: Sequence[], **kwargs: ) -> Sequence[]\n\n```\n\nAsync transform nodes.\nSource code in `llama_index/core/schema.py`\n```\n199\n200\n201\n202\n203\n```\n| ```\nasync def acall(\n    self, nodes: Sequence[BaseNode], **kwargs: Any\n) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n    return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n##  NodeRelationship [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeRelationship \"Permanent link\")\nBases: `str`, `Enum`\nNode relationships used in `BaseNode` class.\nAttributes:\nName | Type | Description  \n---|---|---  \n`SOURCE` |  The node is the source document.  \n`PREVIOUS` |  The node is the previous node in the document.  \nThe node is the next node in the document.  \n`PARENT` |  The node is the parent node in the document.  \n`CHILD` |  The node is a child node in the document.  \nSource code in `llama_index/core/schema.py`\n```\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n```\n| ```\nclass NodeRelationship(str, Enum):\n\"\"\"\n    Node relationships used in `BaseNode` class.\n\n    Attributes:\n        SOURCE: The node is the source document.\n        PREVIOUS: The node is the previous node in the document.\n        NEXT: The node is the next node in the document.\n        PARENT: The node is the parent node in the document.\n        CHILD: The node is a child node in the document.\n\n    \"\"\"\n\n    SOURCE = auto()\n    PREVIOUS = auto()\n    NEXT = auto()\n    PARENT = auto()\n    CHILD = auto()\n\n```\n  \n---|---  \n##  RelatedNodeInfo [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.RelatedNodeInfo \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node_id` |  _required_  \n`node_type` |  `Annotated[ObjectType, PlainSerializer] | str | None` |  `None`  \n`hash` |  `str | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n248\n249\n250\n251\n252\n253\n254\n255\n256\n```\n| ```\nclass RelatedNodeInfo(BaseComponent):\n    node_id: str\n    node_type: Annotated[ObjectType, EnumNameSerializer] | str | None = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    hash: Optional[str] = None\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"RelatedNodeInfo\"\n\n```\n  \n---|---  \n##  BaseNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode \"Permanent link\")\nBases: \nBase node Object.\nGeneric abstract interface for retrievable nodes\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`id_` |  Unique ID of the node. |  `'f64e9cec-5009-4429-bf4d-fa73983e3f01'`  \n`embedding` |  `List[float] | None` |  Embedding of the node. |  `None`  \n`excluded_embed_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the embed model. |  `<dynamic>`  \n`excluded_llm_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the LLM. |  `<dynamic>`  \n`metadata_template` |  Template for how metadata is formatted, with {key} and {value} placeholders. |  `'{key}: {value}'`  \n`metadata_separator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \nSource code in `llama_index/core/schema.py`\n```\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\nclass BaseNode(BaseComponent):\n\"\"\"\n    Base node Object.\n\n    Generic abstract interface for retrievable nodes\n\n    \"\"\"\n\n    # hash is computed on local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\"\n    )\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"Embedding of the node.\"\n    )\n\n\"\"\"\"\n    metadata fields\n    - injected as part of the text shown to LLMs as context\n    - injected as part of the text for generating embeddings\n    - used by vector DBs for metadata filtering\n\n    \"\"\"\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"A flat dictionary of metadata fields\",\n        alias=\"extra_info\",\n    )\n    excluded_embed_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the embed model.\",\n    )\n    excluded_llm_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the LLM.\",\n    )\n    relationships: Dict[\n        Annotated[NodeRelationship, EnumNameSerializer],\n        RelatedNodeType,\n    ] = Field(\n        default_factory=dict,\n        description=\"A mapping of relationships to other node information.\",\n    )\n    metadata_template: str = Field(\n        default=DEFAULT_METADATA_TMPL,\n        description=(\n            \"Template for how metadata is formatted, with {key} and \"\n            \"{value} placeholders.\"\n        ),\n    )\n    metadata_separator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n        alias=\"metadata_seperator\",\n    )\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n    @abstractmethod\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_separator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    @abstractmethod\n    def set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n    @property\n    @abstractmethod\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n\n    @property\n    def node_id(self) -> str:\n        return self.id_\n\n    @node_id.setter\n    def node_id(self, value: str) -> None:\n        self.id_ = value\n\n    @property\n    def source_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"\n        Source object node.\n\n        Extracted from the relationships field.\n\n        \"\"\"\n        if NodeRelationship.SOURCE not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.SOURCE]\n        if isinstance(relation, list):\n            raise ValueError(\"Source object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def prev_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Prev node.\"\"\"\n        if NodeRelationship.PREVIOUS not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PREVIOUS]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Previous object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def next_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Next node.\"\"\"\n        if NodeRelationship.NEXT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.NEXT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Next object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def parent_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Parent node.\"\"\"\n        if NodeRelationship.PARENT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PARENT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Parent object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def child_nodes(self) -> Optional[List[RelatedNodeInfo]]:\n\"\"\"Child nodes.\"\"\"\n        if NodeRelationship.CHILD not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.CHILD]\n        if not isinstance(relation, list):\n            raise ValueError(\"Child objects must be a list of RelatedNodeInfo objects.\")\n        return relation\n\n    @property\n    def ref_doc_id(self) -> Optional[str]:  # pragma: no cover\n\"\"\"Deprecated: Get ref doc id.\"\"\"\n        source_node = self.source_node\n        if source_node is None:\n            return None\n        return source_node.node_id\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self) -> dict[str, Any]:  # pragma: no coverde\n        return self.metadata\n\n    @extra_info.setter\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self, extra_info: dict[str, Any]) -> None:  # pragma: no coverde\n        self.metadata = extra_info\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Node ID: {self.node_id}\\n{source_text_wrapped}\"\n\n    def get_embedding(self) -> List[float]:\n\"\"\"\n        Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    def as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n        return RelatedNodeInfo(\n            node_id=self.node_id,\n            node_type=self.get_type(),\n            metadata=self.metadata,\n            hash=self.hash,\n        )\n\n```\n  \n---|---  \n###  embedding `class-attribute` `instance-attribute` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.embedding \"Permanent link\")\n```\nembedding: Optional[[float]] = (default=None, description='Embedding of the node.')\n\n```\n\n\" metadata fields - injected as part of the text shown to LLMs as context - injected as part of the text for generating embeddings - used by vector DBs for metadata filtering\n###  hash `abstractmethod` `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  source_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.source_node \"Permanent link\")\n```\nsource_node: Optional[]\n\n```\n\nSource object node.\nExtracted from the relationships field.\n###  prev_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.prev_node \"Permanent link\")\n```\nprev_node: Optional[]\n\n```\n\nPrev node.\n###  next_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.next_node \"Permanent link\")\n```\nnext_node: Optional[]\n\n```\n\nNext node.\n###  parent_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.parent_node \"Permanent link\")\n```\nparent_node: Optional[]\n\n```\n\nParent node.\n###  child_nodes `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.child_nodes \"Permanent link\")\n```\nchild_nodes: Optional[[]]\n\n```\n\nChild nodes.\n###  ref_doc_id `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.ref_doc_id \"Permanent link\")\n```\nref_doc_id: Optional[]\n\n```\n\nDeprecated: Get ref doc id.\n###  get_type `abstractmethod` `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n321\n322\n323\n324\n```\n| ```\n@classmethod\n@abstractmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n```\n  \n---|---  \n###  get_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n326\n327\n328\n```\n| ```\n@abstractmethod\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_separator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n353\n354\n355\n```\n| ```\n@abstractmethod\ndef set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n```\n  \n---|---  \n###  get_embedding [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_embedding \"Permanent link\")\n```\nget_embedding() -> [float]\n\n```\n\nGet embedding.\nErrors if embedding is None.\nSource code in `llama_index/core/schema.py`\n```\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n```\n| ```\ndef get_embedding(self) -> List[float]:\n\"\"\"\n    Get embedding.\n\n    Errors if embedding is None.\n\n    \"\"\"\n    if self.embedding is None:\n        raise ValueError(\"embedding not set.\")\n    return self.embedding\n\n```\n  \n---|---  \n###  as_related_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.as_related_node_info \"Permanent link\")\n```\nas_related_node_info() -> \n\n```\n\nGet node as RelatedNodeInfo.\nSource code in `llama_index/core/schema.py`\n```\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\ndef as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n    return RelatedNodeInfo(\n        node_id=self.node_id,\n        node_type=self.get_type(),\n        metadata=self.metadata,\n        hash=self.hash,\n    )\n\n```\n  \n---|---  \n##  MediaResource [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"Permanent link\")\nBases: `BaseModel`\nA container class for media content.\nThis class represents a generic media resource that can be stored and accessed in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports storing vector embeddings for the media content.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`embeddings` |  `dict[Literal['sparse', 'dense'], list[float]] | None` |  Vector representation of this resource. |  `None`  \n`data` |  `bytes | None` |  base64 binary representation of this resource. |  `None`  \n`text` |  `str | None` |  Text representation of this resource. |  `None`  \n`path` |  `Path | None` |  Filesystem path of this resource. |  `None`  \n`url` |  `AnyUrl | None` |  URL to reach this resource. |  `None`  \n`mimetype` |  `str | None` |  MIME type of this resource. |  `None`  \nAttributes:\nName | Type | Description  \n---|---|---  \n`embeddings` |  Multi-vector dict representation of this resource for embedding-based search/retrieval  \n`text` |  Plain text representation of this resource  \n`data` |  Raw binary data of the media content  \n`mimetype` |  The MIME type indicating the format/type of the media content  \n`path` |  Local filesystem path where the media content can be accessed  \nURL where the media content can be accessed remotely  \nSource code in `llama_index/core/schema.py`\n```\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n```\n| ```\nclass MediaResource(BaseModel):\n\"\"\"\n    A container class for media content.\n\n    This class represents a generic media resource that can be stored and accessed\n    in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports\n    storing vector embeddings for the media content.\n\n    Attributes:\n        embeddings: Multi-vector dict representation of this resource for embedding-based search/retrieval\n        text: Plain text representation of this resource\n        data: Raw binary data of the media content\n        mimetype: The MIME type indicating the format/type of the media content\n        path: Local filesystem path where the media content can be accessed\n        url: URL where the media content can be accessed remotely\n\n    \"\"\"\n\n    embeddings: dict[EmbeddingKind, list[float]] | None = Field(\n        default=None, description=\"Vector representation of this resource.\"\n    )\n    data: bytes | None = Field(\n        default=None,\n        exclude=True,\n        description=\"base64 binary representation of this resource.\",\n    )\n    text: str | None = Field(\n        default=None, description=\"Text representation of this resource.\"\n    )\n    path: Path | None = Field(\n        default=None, description=\"Filesystem path of this resource.\"\n    )\n    url: AnyUrl | None = Field(default=None, description=\"URL to reach this resource.\")\n    mimetype: str | None = Field(\n        default=None, description=\"MIME type of this resource.\"\n    )\n\n    model_config = {\n        # This ensures validation runs even for None values\n        \"validate_default\": True\n    }\n\n    @field_validator(\"data\", mode=\"after\")\n    @classmethod\n    def validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n        If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n        In case the model was built passing binary data but without a mimetype,\n        we try to guess it using the filetype library. To avoid resource-intense\n        operations, we won't load the path or the URL to guess the mimetype.\n        \"\"\"\n        if v is None:\n            return v\n\n        try:\n            # Check if data is already base64 encoded.\n            # b64decode() can succeed on random binary data, so we\n            # pass verify=True to make sure it's not a false positive\n            decoded = base64.b64decode(v, validate=True)\n        except BinasciiError:\n            # b64decode failed, return encoded\n            return base64.b64encode(v)\n\n        # Good as is, return unchanged\n        return v\n\n    @field_validator(\"mimetype\", mode=\"after\")\n    @classmethod\n    def validate_mimetype(cls, v: str | None, info: ValidationInfo) -> str | None:\n        if v is not None:\n            return v\n\n        # Since this field validator runs after the one for `data`\n        # then the contents of `data` should be encoded already\n        b64_data = info.data.get(\"data\")\n        if b64_data:  # encoded bytes\n            decoded_data = base64.b64decode(b64_data)\n            if guess := filetype.guess(decoded_data):\n                return guess.mime\n\n        # guess from path\n        rpath: str | None = info.data[\"path\"]\n        if rpath:\n            extension = Path(rpath).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                return ftype.mime\n\n        return v\n\n    @field_serializer(\"path\")  # type: ignore\n    def serialize_path(\n        self, path: Optional[Path], _info: ValidationInfo\n    ) -> Optional[str]:\n        if path is None:\n            return path\n        return str(path)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash to uniquely identify the media resource.\n\n        The hash is generated based on the available content (data, path, text or url).\n        Returns an empty string if no content is available.\n        \"\"\"\n        bits: list[str] = []\n        if self.text is not None:\n            bits.append(self.text)\n        if self.data is not None:\n            # Hash the binary data if available\n            bits.append(str(sha256(self.data).hexdigest()))\n        if self.path is not None:\n            # Hash the file path if provided\n            bits.append(str(sha256(str(self.path).encode(\"utf-8\")).hexdigest()))\n        if self.url is not None:\n            # Use the URL string as basis for hash\n            bits.append(str(sha256(str(self.url).encode(\"utf-8\")).hexdigest()))\n\n        doc_identity = \"\".join(bits)\n        if not doc_identity:\n            return \"\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash to uniquely identify the media resource.\nThe hash is generated based on the available content (data, path, text or url). Returns an empty string if no content is available.\n###  validate_data `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.validate_data \"Permanent link\")\n```\nvalidate_data(v: bytes | None, info: ValidationInfo) -> bytes | None\n\n```\n\nIf binary data was passed, store the resource as base64 and guess the mimetype when possible.\nIn case the model was built passing binary data but without a mimetype, we try to guess it using the filetype library. To avoid resource-intense operations, we won't load the path or the URL to guess the mimetype.\nSource code in `llama_index/core/schema.py`\n```\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n```\n| ```\n@field_validator(\"data\", mode=\"after\")\n@classmethod\ndef validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n    If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n    In case the model was built passing binary data but without a mimetype,\n    we try to guess it using the filetype library. To avoid resource-intense\n    operations, we won't load the path or the URL to guess the mimetype.\n    \"\"\"\n    if v is None:\n        return v\n\n    try:\n        # Check if data is already base64 encoded.\n        # b64decode() can succeed on random binary data, so we\n        # pass verify=True to make sure it's not a false positive\n        decoded = base64.b64decode(v, validate=True)\n    except BinasciiError:\n        # b64decode failed, return encoded\n        return base64.b64encode(v)\n\n    # Good as is, return unchanged\n    return v\n\n```\n  \n---|---  \n##  Node [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Text content of the node. |  `None`  \n`image_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Image content of the node. |  `None`  \n`audio_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Audio content of the node. |  `None`  \n`video_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Video content of the node. |  `None`  \n`text_template` |  Template for how text_resource is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n```\n| ```\nclass Node(BaseNode):\n    text_resource: MediaResource | None = Field(\n        default=None, description=\"Text content of the node.\"\n    )\n    image_resource: MediaResource | None = Field(\n        default=None, description=\"Image content of the node.\"\n    )\n    audio_resource: MediaResource | None = Field(\n        default=None, description=\"Audio content of the node.\"\n    )\n    video_resource: MediaResource | None = Field(\n        default=None, description=\"Video content of the node.\"\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text_resource is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Node\"\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.MULTIMODAL\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n        Get the text content for the node if available.\n\n        Provided for backward compatibility, use self.text_resource directly instead.\n        \"\"\"\n        if self.text_resource:\n            metadata_str = self.get_metadata_str(metadata_mode)\n            if metadata_mode == MetadataMode.NONE or not metadata_str:\n                return self.text_resource.text or \"\"\n\n            return self.text_template.format(\n                content=self.text_resource.text or \"\",\n                metadata_str=metadata_str,\n            ).strip()\n        return \"\"\n\n    def set_content(self, value: str) -> None:\n\"\"\"\n        Set the text content of the node.\n\n        Provided for backward compatibility, set self.text_resource instead.\n        \"\"\"\n        self.text_resource = MediaResource(text=value)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash representing the state of the node.\n\n        The hash is generated based on the available resources (audio, image, text or video) and its metadata.\n        \"\"\"\n        doc_identities = []\n        metadata_str = self.get_metadata_str(mode=MetadataMode.ALL)\n        if metadata_str:\n            doc_identities.append(metadata_str)\n        if self.audio_resource is not None:\n            doc_identities.append(self.audio_resource.hash)\n        if self.image_resource is not None:\n            doc_identities.append(self.image_resource.hash)\n        if self.text_resource is not None:\n            doc_identities.append(self.text_resource.hash)\n        if self.video_resource is not None:\n            doc_identities.append(self.video_resource.hash)\n\n        doc_identity = \"-\".join(doc_identities)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash representing the state of the node.\nThe hash is generated based on the available resources (audio, image, text or video) and its metadata.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n637\n638\n639\n640\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.MULTIMODAL\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet the text content for the node if available.\nProvided for backward compatibility, use self.text_resource directly instead.\nSource code in `llama_index/core/schema.py`\n```\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n    Get the text content for the node if available.\n\n    Provided for backward compatibility, use self.text_resource directly instead.\n    \"\"\"\n    if self.text_resource:\n        metadata_str = self.get_metadata_str(metadata_mode)\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text_resource.text or \"\"\n\n        return self.text_template.format(\n            content=self.text_resource.text or \"\",\n            metadata_str=metadata_str,\n        ).strip()\n    return \"\"\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the text content of the node.\nProvided for backward compatibility, set self.text_resource instead.\nSource code in `llama_index/core/schema.py`\n```\n659\n660\n661\n662\n663\n664\n665\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"\n    Set the text content of the node.\n\n    Provided for backward compatibility, set self.text_resource instead.\n    \"\"\"\n    self.text_resource = MediaResource(text=value)\n\n```\n  \n---|---  \n##  TextNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode \"Permanent link\")\nBases: \nProvided for backward compatibility.\nNote: we keep the field with the typo \"seperator\" to maintain backward compatibility for serialized objects.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text` |  Text content of the node.  \n`mimetype` |  MIME type of the node content. |  `'text/plain'`  \n`start_char_idx` |  `int | None` |  Start char index of the node. |  `None`  \n`end_char_idx` |  `int | None` |  End char index of the node. |  `None`  \n`metadata_seperator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \n`text_template` |  Template for how text is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n```\n| ```\nclass TextNode(BaseNode):\n\"\"\"\n    Provided for backward compatibility.\n\n    Note: we keep the field with the typo \"seperator\" to maintain backward compatibility for\n    serialized objects.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make TextNode forward-compatible with Node by supporting 'text_resource' in the constructor.\"\"\"\n        if \"text_resource\" in kwargs:\n            tr = kwargs.pop(\"text_resource\")\n            if isinstance(tr, MediaResource):\n                kwargs[\"text\"] = tr.text\n            else:\n                kwargs[\"text\"] = tr[\"text\"]\n        super().__init__(*args, **kwargs)\n\n    text: str = Field(default=\"\", description=\"Text content of the node.\")\n    mimetype: str = Field(\n        default=\"text/plain\", description=\"MIME type of the node content.\"\n    )\n    start_char_idx: Optional[int] = Field(\n        default=None, description=\"Start char index of the node.\"\n    )\n    end_char_idx: Optional[int] = Field(\n        default=None, description=\"End char index of the node.\"\n    )\n    metadata_seperator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TextNode\"\n\n    @property\n    def hash(self) -> str:\n        doc_identity = str(self.text) + str(self.metadata)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.TEXT\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n        metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text\n\n        return self.text_template.format(\n            content=self.text, metadata_str=metadata_str\n        ).strip()\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_seperator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    def set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n        self.text = value\n\n    def get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n        return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n    def get_text(self) -> str:\n        return self.get_content(metadata_mode=MetadataMode.NONE)\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'node_info' is deprecated, use 'get_node_info' instead.\",\n    )\n    def node_info(self) -> Dict[str, Any]:\n\"\"\"Deprecated: Get node info.\"\"\"\n        return self.get_node_info()\n\n```\n  \n---|---  \n###  node_info `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.node_info \"Permanent link\")\n```\nnode_info: [, ]\n\n```\n\nDeprecated: Get node info.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n740\n741\n742\n743\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.TEXT\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n745\n746\n747\n748\n749\n750\n751\n752\n753\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n    metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n    if metadata_mode == MetadataMode.NONE or not metadata_str:\n        return self.text\n\n    return self.text_template.format(\n        content=self.text, metadata_str=metadata_str\n    ).strip()\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_seperator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n778\n779\n780\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n    self.text = value\n\n```\n  \n---|---  \n###  get_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_node_info \"Permanent link\")\n```\nget_node_info() -> [, ]\n\n```\n\nGet node info.\nSource code in `llama_index/core/schema.py`\n```\n782\n783\n784\n```\n| ```\ndef get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n    return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n```\n  \n---|---  \n##  ImageNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode \"Permanent link\")\nBases: \nNode with image.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`image` |  `str | None` |  `None`  \n`image_path` |  `str | None` |  `None`  \n`image_url` |  `str | None` |  `None`  \n`image_mimetype` |  `str | None` |  `None`  \n`text_embedding` |  `List[float] | None` |  Text embedding of image node, if text field is filled out |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n```\n| ```\nclass ImageNode(TextNode):\n\"\"\"Node with image.\"\"\"\n\n    # TODO: store reference instead of actual image\n    # base64 encoded image str\n    image: Optional[str] = None\n    image_path: Optional[str] = None\n    image_url: Optional[str] = None\n    image_mimetype: Optional[str] = None\n    text_embedding: Optional[List[float]] = Field(\n        default=None,\n        description=\"Text embedding of image node, if text field is filled out\",\n    )\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make ImageNode forward-compatible with Node by supporting 'image_resource' in the constructor.\"\"\"\n        if \"image_resource\" in kwargs:\n            ir = kwargs.pop(\"image_resource\")\n            if isinstance(ir, MediaResource):\n                kwargs[\"image_path\"] = ir.path.as_posix() if ir.path else None\n                kwargs[\"image_url\"] = ir.url\n                kwargs[\"image_mimetype\"] = ir.mimetype\n            else:\n                kwargs[\"image_path\"] = ir.get(\"path\", None)\n                kwargs[\"image_url\"] = ir.get(\"url\", None)\n                kwargs[\"image_mimetype\"] = ir.get(\"mimetype\", None)\n\n        mimetype = kwargs.get(\"image_mimetype\")\n        if not mimetype and kwargs.get(\"image_path\") is not None:\n            # guess mimetype from image_path\n            extension = Path(kwargs[\"image_path\"]).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                kwargs[\"image_mimetype\"] = ftype.mime\n\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.IMAGE\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageNode\"\n\n    def resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n        if self.image is not None:\n            import base64\n\n            return BytesIO(base64.b64decode(self.image))\n        elif self.image_path is not None:\n            return self.image_path\n        elif self.image_url is not None:\n            # load image from URL\n            import requests\n\n            response = requests.get(self.image_url, timeout=(60, 60))\n            return BytesIO(response.content)\n        else:\n            raise ValueError(\"No image found in node.\")\n\n    @property\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n        # doc identity depends on if image, image_path, or image_url is set\n        image_str = self.image or \"None\"\n        image_path_str = self.image_path or \"None\"\n        image_url_str = self.image_url or \"None\"\n        image_text = self.text or \"None\"\n        doc_identity = f\"{image_str}-{image_path_str}-{image_url_str}-{image_text}\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.resolve_image \"Permanent link\")\n```\nresolve_image() -> ImageType\n\n```\n\nResolve an image such that PIL can read it.\nSource code in `llama_index/core/schema.py`\n```\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n```\n| ```\ndef resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n    if self.image is not None:\n        import base64\n\n        return BytesIO(base64.b64decode(self.image))\n    elif self.image_path is not None:\n        return self.image_path\n    elif self.image_url is not None:\n        # load image from URL\n        import requests\n\n        response = requests.get(self.image_url, timeout=(60, 60))\n        return BytesIO(response.content)\n    else:\n        raise ValueError(\"No image found in node.\")\n\n```\n  \n---|---  \n##  IndexNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode \"Permanent link\")\nBases: \nNode with reference to any object.\nThis can include other indices, query engines, retrievers.\nThis can also include other nodes (though this is overlapping with `relationships` on the Node class).\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`index_id` |  _required_  \n`obj` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n```\n| ```\nclass IndexNode(TextNode):\n\"\"\"\n    Node with reference to any object.\n\n    This can include other indices, query engines, retrievers.\n\n    This can also include other nodes (though this is overlapping with `relationships`\n    on the Node class).\n\n    \"\"\"\n\n    index_id: str\n    obj: Any = None\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        from llama_index.core.storage.docstore.utils import doc_to_json\n\n        data = super().dict(**kwargs)\n\n        try:\n            if self.obj is None:\n                data[\"obj\"] = None\n            elif isinstance(self.obj, BaseNode):\n                data[\"obj\"] = doc_to_json(self.obj)\n            elif isinstance(self.obj, BaseModel):\n                data[\"obj\"] = self.obj.model_dump()\n            else:\n                data[\"obj\"] = json.dumps(self.obj)\n        except Exception:\n            raise ValueError(\"IndexNode obj is not serializable: \" + str(self.obj))\n\n        return data\n\n    @classmethod\n    def from_text_node(\n        cls,\n        node: TextNode,\n        index_id: str,\n    ) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n        # copy all attributes from text node, add index id\n        return cls(\n            **node.dict(),\n            index_id=index_id,\n        )\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        output = super().from_dict(data, **kwargs)\n\n        obj = data.get(\"obj\")\n        parsed_obj = None\n\n        if isinstance(obj, str):\n            parsed_obj = TextNode(text=obj)\n        elif isinstance(obj, dict):\n            from llama_index.core.storage.docstore.utils import json_to_doc\n\n            # check if its a node, else assume stringable\n            try:\n                parsed_obj = json_to_doc(obj)  # type: ignore[assignment]\n            except Exception:\n                parsed_obj = TextNode(text=str(obj))\n\n        output.obj = parsed_obj\n\n        return output\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.INDEX\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"IndexNode\"\n\n```\n  \n---|---  \n###  from_text_node `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode.from_text_node \"Permanent link\")\n```\nfrom_text_node(node: , index_id: ) -> \n\n```\n\nCreate index node from text node.\nSource code in `llama_index/core/schema.py`\n```\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n```\n| ```\n@classmethod\ndef from_text_node(\n    cls,\n    node: TextNode,\n    index_id: str,\n) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n    # copy all attributes from text node, add index id\n    return cls(\n        **node.dict(),\n        index_id=index_id,\n    )\n\n```\n  \n---|---  \n##  NodeWithScore [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node` |  |  _required_  \n`score` |  `float | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n```\n| ```\nclass NodeWithScore(BaseComponent):\n    node: SerializeAsAny[BaseNode]\n    score: Optional[float] = None\n\n    def __str__(self) -> str:\n        score_str = \"None\" if self.score is None else f\"{self.score: 0.3f}\"\n        return f\"{self.node}\\nScore: {score_str}\\n\"\n\n    def get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n        if self.score is None:\n            if raise_error:\n                raise ValueError(\"Score not set.\")\n            else:\n                return 0.0\n        else:\n            return self.score\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"NodeWithScore\"\n\n    ##### pass through methods to BaseNode #####\n    @property\n    def node_id(self) -> str:\n        return self.node.node_id\n\n    @property\n    def id_(self) -> str:\n        return self.node.id_\n\n    @property\n    def text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.text\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        return self.node.metadata\n\n    @property\n    def embedding(self) -> Optional[List[float]]:\n        return self.node.embedding\n\n    def get_text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.get_text()\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n        return self.node.get_content(metadata_mode=metadata_mode)\n\n    def get_embedding(self) -> List[float]:\n        return self.node.get_embedding()\n\n```\n  \n---|---  \n###  get_score [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore.get_score \"Permanent link\")\n```\nget_score(raise_error:  = False) -> float\n\n```\n\nGet score.\nSource code in `llama_index/core/schema.py`\n```\n958\n959\n960\n961\n962\n963\n964\n965\n966\n```\n| ```\ndef get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n    if self.score is None:\n        if raise_error:\n            raise ValueError(\"Score not set.\")\n        else:\n            return 0.0\n    else:\n        return self.score\n\n```\n  \n---|---  \n##  Document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document \"Permanent link\")\nBases: \nGeneric interface for a data document.\nThis document connects to data sources.\nSource code in `llama_index/core/schema.py`\n```\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\nclass Document(Node):\n\"\"\"\n    Generic interface for a data document.\n\n    This document connects to data sources.\n    \"\"\"\n\n    def __init__(self, **data: Any) -> None:\n\"\"\"\n        Keeps backward compatibility with old 'Document' versions.\n\n        If 'text' was passed, store it in 'text_resource'.\n        If 'doc_id' was passed, store it in 'id_'.\n        If 'extra_info' was passed, store it in 'metadata'.\n        \"\"\"\n        if \"doc_id\" in data:\n            value = data.pop(\"doc_id\")\n            if \"id_\" in data:\n                msg = \"'doc_id' is deprecated and 'id_' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"id_\"] = value\n\n        if \"extra_info\" in data:\n            value = data.pop(\"extra_info\")\n            if \"metadata\" in data:\n                msg = \"'extra_info' is deprecated and 'metadata' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"metadata\"] = value\n\n        if data.get(\"text\"):\n            text = data.pop(\"text\")\n            if \"text_resource\" in data:\n                text_resource = (\n                    data[\"text_resource\"]\n                    if isinstance(data[\"text_resource\"], MediaResource)\n                    else MediaResource.model_validate(data[\"text_resource\"])\n                )\n                if (text_resource.text or \"\").strip() != text.strip():\n                    msg = (\n                        \"'text' is deprecated and 'text_resource' will be used instead\"\n                    )\n                    logging.warning(msg)\n            else:\n                data[\"text_resource\"] = MediaResource(text=text)\n\n        super().__init__(**data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n        data = super().custom_model_dump(handler, info)\n        exclude_set = set(info.exclude or [])\n        if \"text\" not in exclude_set:\n            data[\"text\"] = self.text\n        return data\n\n    @property\n    def text(self) -> str:\n\"\"\"Provided for backward compatibility, it returns the content of text_resource.\"\"\"\n        return self.get_content()\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n        return ObjectType.DOCUMENT\n\n    @property\n    def doc_id(self) -> str:\n\"\"\"Get document ID.\"\"\"\n        return self.id_\n\n    @doc_id.setter\n    def doc_id(self, id_: str) -> None:\n        self.id_ = id_\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Doc ID: {self.doc_id}\\n{source_text_wrapped}\"\n\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'get_doc_id' is deprecated, access the 'id_' property instead.\",\n    )\n    def get_doc_id(self) -> str:  # pragma: nocover\n        return self.id_\n\n    def to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n        from llama_index.core.bridge.langchain import (\n            Document as LCDocument,  # type: ignore\n        )\n\n        metadata = self.metadata or {}\n        return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n    @classmethod\n    def from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n        if doc.id:\n            return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n        return cls(text=doc.page_content, metadata=doc.metadata)\n\n    def to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n        from haystack import Document as HaystackDocument  # type: ignore\n\n        return HaystackDocument(\n            content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n        )\n\n    @classmethod\n    def from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n        return cls(\n            text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n        )\n\n    def to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n        return {\n            \"doc_id\": self.id_,\n            \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n        }\n\n    @classmethod\n    def from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n        return cls(\n            text=doc[\"data\"][\"content\"],\n            metadata=doc[\"data\"][\"meta_data\"],\n            id_=doc[\"doc_id\"],\n        )\n\n    def to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n        import numpy as np\n        from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n        return MemoryRecord(\n            id=self.id_,\n            text=self.text,\n            additional_metadata=self.get_metadata_str(),\n            embedding=np.array(self.embedding) if self.embedding else None,\n        )\n\n    @classmethod\n    def from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n        return cls(\n            text=doc._text,\n            metadata={\"additional_metadata\": doc._additional_metadata},\n            embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n            id_=doc._id,\n        )\n\n    def to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n        # write document to temp file\n        import tempfile\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(self.text.encode(\"utf-8\"))\n            f.flush()\n            client.embed(f.name)\n\n    @classmethod\n    def example(cls) -> Document:\n        return Document(\n            text=SAMPLE_TEXT,\n            metadata={\"filename\": \"README.md\", \"category\": \"codebase\"},\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Document\"\n\n    def to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n        from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n        return CloudDocument(\n            text=self.text,\n            metadata=self.metadata,\n            excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n            id=self.id_,\n        )\n\n    @classmethod\n    def from_cloud_document(\n        cls,\n        doc: CloudDocument,\n    ) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n        return Document(\n            text=doc.text,\n            metadata=doc.metadata,\n            excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n            id_=doc.id,\n        )\n\n```\n  \n---|---  \n###  text `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.text \"Permanent link\")\n```\ntext: \n\n```\n\nProvided for backward compatibility, it returns the content of text_resource.\n###  doc_id `property` `writable` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.doc_id \"Permanent link\")\n```\ndoc_id: \n\n```\n\nGet document ID.\n###  custom_model_dump [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.custom_model_dump \"Permanent link\")\n```\ncustom_model_dump(handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> [, ]\n\n```\n\nFor full backward compatibility with the text field, we customize the model serializer.\nSource code in `llama_index/core/schema.py`\n```\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n```\n| ```\n@model_serializer(mode=\"wrap\")\ndef custom_model_dump(\n    self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n    data = super().custom_model_dump(handler, info)\n    exclude_set = set(info.exclude or [])\n    if \"text\" not in exclude_set:\n        data[\"text\"] = self.text\n    return data\n\n```\n  \n---|---  \n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Document type.\nSource code in `llama_index/core/schema.py`\n```\n1077\n1078\n1079\n1080\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n    return ObjectType.DOCUMENT\n\n```\n  \n---|---  \n###  to_langchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_langchain_format \"Permanent link\")\n```\nto_langchain_format() -> Document\n\n```\n\nConvert struct to LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n```\n| ```\ndef to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n    from llama_index.core.bridge.langchain import (\n        Document as LCDocument,  # type: ignore\n    )\n\n    metadata = self.metadata or {}\n    return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n```\n  \n---|---  \n###  from_langchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_langchain_format \"Permanent link\")\n```\nfrom_langchain_format(doc: Document) -> \n\n```\n\nConvert struct from LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1116\n1117\n1118\n1119\n1120\n1121\n```\n| ```\n@classmethod\ndef from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n    if doc.id:\n        return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n    return cls(text=doc.page_content, metadata=doc.metadata)\n\n```\n  \n---|---  \n###  to_haystack_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_haystack_format \"Permanent link\")\n```\nto_haystack_format() -> Document\n\n```\n\nConvert struct to Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n```\n| ```\ndef to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n    from haystack import Document as HaystackDocument  # type: ignore\n\n    return HaystackDocument(\n        content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n    )\n\n```\n  \n---|---  \n###  from_haystack_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_haystack_format \"Permanent link\")\n```\nfrom_haystack_format(doc: Document) -> \n\n```\n\nConvert struct from Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1131\n1132\n1133\n1134\n1135\n1136\n```\n| ```\n@classmethod\ndef from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n    return cls(\n        text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n    )\n\n```\n  \n---|---  \n###  to_embedchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_embedchain_format \"Permanent link\")\n```\nto_embedchain_format() -> [, ]\n\n```\n\nConvert struct to EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1138\n1139\n1140\n1141\n1142\n1143\n```\n| ```\ndef to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n    return {\n        \"doc_id\": self.id_,\n        \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n    }\n\n```\n  \n---|---  \n###  from_embedchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_embedchain_format \"Permanent link\")\n```\nfrom_embedchain_format(doc: [, ]) -> \n\n```\n\nConvert struct from EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n```\n| ```\n@classmethod\ndef from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n    return cls(\n        text=doc[\"data\"][\"content\"],\n        metadata=doc[\"data\"][\"meta_data\"],\n        id_=doc[\"doc_id\"],\n    )\n\n```\n  \n---|---  \n###  to_semantic_kernel_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_semantic_kernel_format \"Permanent link\")\n```\nto_semantic_kernel_format() -> MemoryRecord\n\n```\n\nConvert struct to Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n```\n| ```\ndef to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n    import numpy as np\n    from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n    return MemoryRecord(\n        id=self.id_,\n        text=self.text,\n        additional_metadata=self.get_metadata_str(),\n        embedding=np.array(self.embedding) if self.embedding else None,\n    )\n\n```\n  \n---|---  \n###  from_semantic_kernel_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_semantic_kernel_format \"Permanent link\")\n```\nfrom_semantic_kernel_format(doc: MemoryRecord) -> \n\n```\n\nConvert struct from Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n```\n| ```\n@classmethod\ndef from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n    return cls(\n        text=doc._text,\n        metadata={\"additional_metadata\": doc._additional_metadata},\n        embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n        id_=doc._id,\n    )\n\n```\n  \n---|---  \n###  to_vectorflow [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_vectorflow \"Permanent link\")\n```\nto_vectorflow(client: ) -> None\n\n```\n\nSend a document to vectorflow, since they don't have a document object.\nSource code in `llama_index/core/schema.py`\n```\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n```\n| ```\ndef to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n    # write document to temp file\n    import tempfile\n\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(self.text.encode(\"utf-8\"))\n        f.flush()\n        client.embed(f.name)\n\n```\n  \n---|---  \n###  to_cloud_document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_cloud_document \"Permanent link\")\n```\nto_cloud_document() -> CloudDocument\n\n```\n\nConvert to LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n```\n| ```\ndef to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n    from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n    return CloudDocument(\n        text=self.text,\n        metadata=self.metadata,\n        excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n        id=self.id_,\n    )\n\n```\n  \n---|---  \n###  from_cloud_document `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_cloud_document \"Permanent link\")\n```\nfrom_cloud_document(doc: CloudDocument) -> \n\n```\n\nConvert from LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\n@classmethod\ndef from_cloud_document(\n    cls,\n    doc: CloudDocument,\n) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n    return Document(\n        text=doc.text,\n        metadata=doc.metadata,\n        excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n        id_=doc.id,\n    )\n\n```\n  \n---|---  \n##  ImageDocument [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument \"Permanent link\")\nBases: \nBackward compatible wrapper around Document containing an image.\nSource code in `llama_index/core/schema.py`\n```\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\nclass ImageDocument(Document):\n\"\"\"Backward compatible wrapper around Document containing an image.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        image = kwargs.pop(\"image\", None)\n        image_path = kwargs.pop(\"image_path\", None)\n        image_url = kwargs.pop(\"image_url\", None)\n        image_mimetype = kwargs.pop(\"image_mimetype\", None)\n        text_embedding = kwargs.pop(\"text_embedding\", None)\n\n        if image:\n            kwargs[\"image_resource\"] = MediaResource(\n                data=image, mimetype=image_mimetype\n            )\n        elif image_path:\n            if not is_image_pil(image_path):\n                raise ValueError(\"The specified file path is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                path=image_path, mimetype=image_mimetype\n            )\n        elif image_url:\n            if not is_image_url_pil(image_url):\n                raise ValueError(\"The specified URL is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                url=image_url, mimetype=image_mimetype\n            )\n\n        super().__init__(**kwargs)\n\n    @property\n    def image(self) -> str | None:\n        if self.image_resource and self.image_resource.data:\n            return self.image_resource.data.decode(\"utf-8\")\n        return None\n\n    @image.setter\n    def image(self, image: str) -> None:\n        self.image_resource = MediaResource(data=image.encode(\"utf-8\"))\n\n    @property\n    def image_path(self) -> str | None:\n        if self.image_resource and self.image_resource.path:\n            return str(self.image_resource.path)\n        return None\n\n    @image_path.setter\n    def image_path(self, image_path: str) -> None:\n        self.image_resource = MediaResource(path=Path(image_path))\n\n    @property\n    def image_url(self) -> str | None:\n        if self.image_resource and self.image_resource.url:\n            return str(self.image_resource.url)\n        return None\n\n    @image_url.setter\n    def image_url(self, image_url: str) -> None:\n        self.image_resource = MediaResource(url=AnyUrl(url=image_url))\n\n    @property\n    def image_mimetype(self) -> str | None:\n        if self.image_resource:\n            return self.image_resource.mimetype\n        return None\n\n    @image_mimetype.setter\n    def image_mimetype(self, image_mimetype: str) -> None:\n        if self.image_resource:\n            self.image_resource.mimetype = image_mimetype\n\n    @property\n    def text_embedding(self) -> list[float] | None:\n        if self.text_resource and self.text_resource.embeddings:\n            return self.text_resource.embeddings.get(\"dense\")\n        return None\n\n    @text_embedding.setter\n    def text_embedding(self, embeddings: list[float]) -> None:\n        if self.text_resource:\n            if self.text_resource.embeddings is None:\n                self.text_resource.embeddings = {}\n            self.text_resource.embeddings[\"dense\"] = embeddings\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageDocument\"\n\n    def resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n        Resolve an image such that PIL can read it.\n\n        Args:\n            as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n        \"\"\"\n        if self.image_resource is None:\n            return BytesIO()\n\n        if self.image_resource.data is not None:\n            if as_base64:\n                return BytesIO(self.image_resource.data)\n            return BytesIO(base64.b64decode(self.image_resource.data))\n        elif self.image_resource.path is not None:\n            img_bytes = self.image_resource.path.read_bytes()\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        elif self.image_resource.url is not None:\n            # load image from URL\n            response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n            img_bytes = response.content\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        else:\n            raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument.resolve_image \"Permanent link\")\n```\nresolve_image(as_base64:  = False) -> BytesIO\n\n```\n\nResolve an image such that PIL can read it.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`as_base64` |  `bool` |  whether the resolved image should be returned as base64-encoded bytes |  `False`  \nSource code in `llama_index/core/schema.py`\n```\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\ndef resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n    Resolve an image such that PIL can read it.\n\n    Args:\n        as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n    \"\"\"\n    if self.image_resource is None:\n        return BytesIO()\n\n    if self.image_resource.data is not None:\n        if as_base64:\n            return BytesIO(self.image_resource.data)\n        return BytesIO(base64.b64decode(self.image_resource.data))\n    elif self.image_resource.path is not None:\n        img_bytes = self.image_resource.path.read_bytes()\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    elif self.image_resource.url is not None:\n        # load image from URL\n        response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n        img_bytes = response.content\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    else:\n        raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n##  QueryBundle `dataclass` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle \"Permanent link\")\nBases: `DataClassJsonMixin`\nQuery bundle.\nThis dataclass contains the original query string and associated transformations.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`query_str` |  the original user-specified query string. This is currently used by all non embedding-based queries. |  _required_  \n`custom_embedding_strs` |  `list[str]` |  list of strings used for embedding the query. This is currently used by all embedding-based queries. |  `None`  \n`embedding` |  `list[float]` |  the stored embedding for the query. |  `None`  \n`image_path` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n```\n| ```\n@dataclass\nclass QueryBundle(DataClassJsonMixin):\n\"\"\"\n    Query bundle.\n\n    This dataclass contains the original query string and associated transformations.\n\n    Args:\n        query_str (str): the original user-specified query string.\n            This is currently used by all non embedding-based queries.\n        custom_embedding_strs (list[str]): list of strings used for embedding the query.\n            This is currently used by all embedding-based queries.\n        embedding (list[float]): the stored embedding for the query.\n\n    \"\"\"\n\n    query_str: str\n    # using single image path as query input\n    image_path: Optional[str] = None\n    custom_embedding_strs: Optional[List[str]] = None\n    embedding: Optional[List[float]] = None\n\n    @property\n    def embedding_strs(self) -> List[str]:\n\"\"\"Use custom embedding strs if specified, otherwise use query str.\"\"\"\n        if self.custom_embedding_strs is None:\n            if len(self.query_str) == 0:\n                return []\n            return [self.query_str]\n        else:\n            return self.custom_embedding_strs\n\n    @property\n    def embedding_image(self) -> List[ImageType]:\n\"\"\"Use image path for image retrieval.\"\"\"\n        if self.image_path is None:\n            return []\n        return [self.image_path]\n\n    def __str__(self) -> str:\n\"\"\"Convert to string representation.\"\"\"\n        return self.query_str\n\n```\n  \n---|---  \n###  embedding_strs `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_strs \"Permanent link\")\n```\nembedding_strs: []\n\n```\n\nUse custom embedding strs if specified, otherwise use query str.\n###  embedding_image `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_image \"Permanent link\")\n```\nembedding_image: [ImageType]\n\n```\n\nUse image path for image retrieval.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "# Index\nBase schema for data structures.\n##  BaseComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent \"Permanent link\")\nBases: `BaseModel`\nBase component object to capture class names.\nSource code in `llama_index/core/schema.py`\n```\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n```\n| ```\nclass BaseComponent(BaseModel):\n\"\"\"Base component object to capture class names.\"\"\"\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)\n        json_schema = handler.resolve_ref_schema(json_schema)\n\n        # inject class name to help with serde\n        if \"properties\" in json_schema:\n            json_schema[\"properties\"][\"class_name\"] = {\n                \"title\": \"Class Name\",\n                \"type\": \"string\",\n                \"default\": cls.class_name(),\n            }\n        return json_schema\n\n    @classmethod\n    def class_name(cls) -> str:\n\"\"\"\n        Get the class name, used as a unique ID in serialization.\n\n        This provides a key that makes serialization robust against actual class\n        name changes.\n        \"\"\"\n        return \"base_component\"\n\n    def json(self, **kwargs: Any) -> str:\n        return self.to_json(**kwargs)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n        data = handler(self)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        return self.model_dump(**kwargs)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = super().__getstate__()\n\n        # remove attributes that are not pickleable -- kind of dangerous\n        keys_to_remove = []\n        for key, val in state[\"__dict__\"].items():\n            try:\n                pickle.dumps(val)\n            except Exception:\n                keys_to_remove.append(key)\n\n        for key in keys_to_remove:\n            logging.warning(f\"Removing unpickleable attribute {key}\")\n            del state[\"__dict__\"][key]\n\n        # remove private attributes if they aren't pickleable -- kind of dangerous\n        keys_to_remove = []\n        private_attrs = state.get(\"__pydantic_private__\", None)\n        if private_attrs:\n            for key, val in state[\"__pydantic_private__\"].items():\n                try:\n                    pickle.dumps(val)\n                except Exception:\n                    keys_to_remove.append(key)\n\n            for key in keys_to_remove:\n                logging.warning(f\"Removing unpickleable private attribute {key}\")\n                del state[\"__pydantic_private__\"][key]\n\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        # Use the __dict__ and __init__ method to set state\n        # so that all variables initialize\n        try:\n            self.__init__(**state[\"__dict__\"])  # type: ignore\n        except Exception:\n            # Fall back to the default __setstate__ method\n            # This may not work if the class had unpickleable attributes\n            super().__setstate__(state)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = self.dict(**kwargs)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def to_json(self, **kwargs: Any) -> str:\n        data = self.to_dict(**kwargs)\n        return json.dumps(data)\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        # In SimpleKVStore we rely on shallow coping. Hence, the data will be modified in the store directly.\n        # And it is the same when the user is passing a dictionary to create a component. We can't modify the passed down dictionary.\n        data = dict(data)\n        if isinstance(kwargs, dict):\n            data.update(kwargs)\n        data.pop(\"class_name\", None)\n        return cls(**data)\n\n    @classmethod\n    def from_json(cls, data_str: str, **kwargs: Any) -> Self:  # type: ignore\n        data = json.loads(data_str)\n        return cls.from_dict(data, **kwargs)\n\n```\n  \n---|---  \n###  class_name `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent.class_name \"Permanent link\")\n```\nclass_name() -> \n\n```\n\nGet the class name, used as a unique ID in serialization.\nThis provides a key that makes serialization robust against actual class name changes.\nSource code in `llama_index/core/schema.py`\n```\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n```\n| ```\n@classmethod\ndef class_name(cls) -> str:\n\"\"\"\n    Get the class name, used as a unique ID in serialization.\n\n    This provides a key that makes serialization robust against actual class\n    name changes.\n    \"\"\"\n    return \"base_component\"\n\n```\n  \n---|---  \n##  TransformComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent \"Permanent link\")\nBases: , `DispatcherSpanMixin`\nBase class for transform components.\nSource code in `llama_index/core/schema.py`\n```\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n```\n| ```\nclass TransformComponent(BaseComponent, DispatcherSpanMixin):\n\"\"\"Base class for transform components.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:\n\"\"\"Transform nodes.\"\"\"\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n        return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n###  acall `async` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent.acall \"Permanent link\")\n```\nacall(nodes: Sequence[], **kwargs: ) -> Sequence[]\n\n```\n\nAsync transform nodes.\nSource code in `llama_index/core/schema.py`\n```\n199\n200\n201\n202\n203\n```\n| ```\nasync def acall(\n    self, nodes: Sequence[BaseNode], **kwargs: Any\n) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n    return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n##  NodeRelationship [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeRelationship \"Permanent link\")\nBases: `str`, `Enum`\nNode relationships used in `BaseNode` class.\nAttributes:\nName | Type | Description  \n---|---|---  \n`SOURCE` |  The node is the source document.  \n`PREVIOUS` |  The node is the previous node in the document.  \nThe node is the next node in the document.  \n`PARENT` |  The node is the parent node in the document.  \n`CHILD` |  The node is a child node in the document.  \nSource code in `llama_index/core/schema.py`\n```\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n```\n| ```\nclass NodeRelationship(str, Enum):\n\"\"\"\n    Node relationships used in `BaseNode` class.\n\n    Attributes:\n        SOURCE: The node is the source document.\n        PREVIOUS: The node is the previous node in the document.\n        NEXT: The node is the next node in the document.\n        PARENT: The node is the parent node in the document.\n        CHILD: The node is a child node in the document.\n\n    \"\"\"\n\n    SOURCE = auto()\n    PREVIOUS = auto()\n    NEXT = auto()\n    PARENT = auto()\n    CHILD = auto()\n\n```\n  \n---|---  \n##  RelatedNodeInfo [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.RelatedNodeInfo \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node_id` |  _required_  \n`node_type` |  `Annotated[ObjectType, PlainSerializer] | str | None` |  `None`  \n`hash` |  `str | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n248\n249\n250\n251\n252\n253\n254\n255\n256\n```\n| ```\nclass RelatedNodeInfo(BaseComponent):\n    node_id: str\n    node_type: Annotated[ObjectType, EnumNameSerializer] | str | None = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    hash: Optional[str] = None\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"RelatedNodeInfo\"\n\n```\n  \n---|---  \n##  BaseNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode \"Permanent link\")\nBases: \nBase node Object.\nGeneric abstract interface for retrievable nodes\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`id_` |  Unique ID of the node. |  `'f64e9cec-5009-4429-bf4d-fa73983e3f01'`  \n`embedding` |  `List[float] | None` |  Embedding of the node. |  `None`  \n`excluded_embed_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the embed model. |  `<dynamic>`  \n`excluded_llm_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the LLM. |  `<dynamic>`  \n`metadata_template` |  Template for how metadata is formatted, with {key} and {value} placeholders. |  `'{key}: {value}'`  \n`metadata_separator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \nSource code in `llama_index/core/schema.py`\n```\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\nclass BaseNode(BaseComponent):\n\"\"\"\n    Base node Object.\n\n    Generic abstract interface for retrievable nodes\n\n    \"\"\"\n\n    # hash is computed on local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\"\n    )\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"Embedding of the node.\"\n    )\n\n\"\"\"\"\n    metadata fields\n    - injected as part of the text shown to LLMs as context\n    - injected as part of the text for generating embeddings\n    - used by vector DBs for metadata filtering\n\n    \"\"\"\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"A flat dictionary of metadata fields\",\n        alias=\"extra_info\",\n    )\n    excluded_embed_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the embed model.\",\n    )\n    excluded_llm_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the LLM.\",\n    )\n    relationships: Dict[\n        Annotated[NodeRelationship, EnumNameSerializer],\n        RelatedNodeType,\n    ] = Field(\n        default_factory=dict,\n        description=\"A mapping of relationships to other node information.\",\n    )\n    metadata_template: str = Field(\n        default=DEFAULT_METADATA_TMPL,\n        description=(\n            \"Template for how metadata is formatted, with {key} and \"\n            \"{value} placeholders.\"\n        ),\n    )\n    metadata_separator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n        alias=\"metadata_seperator\",\n    )\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n    @abstractmethod\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_separator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    @abstractmethod\n    def set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n    @property\n    @abstractmethod\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n\n    @property\n    def node_id(self) -> str:\n        return self.id_\n\n    @node_id.setter\n    def node_id(self, value: str) -> None:\n        self.id_ = value\n\n    @property\n    def source_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"\n        Source object node.\n\n        Extracted from the relationships field.\n\n        \"\"\"\n        if NodeRelationship.SOURCE not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.SOURCE]\n        if isinstance(relation, list):\n            raise ValueError(\"Source object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def prev_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Prev node.\"\"\"\n        if NodeRelationship.PREVIOUS not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PREVIOUS]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Previous object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def next_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Next node.\"\"\"\n        if NodeRelationship.NEXT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.NEXT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Next object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def parent_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Parent node.\"\"\"\n        if NodeRelationship.PARENT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PARENT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Parent object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def child_nodes(self) -> Optional[List[RelatedNodeInfo]]:\n\"\"\"Child nodes.\"\"\"\n        if NodeRelationship.CHILD not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.CHILD]\n        if not isinstance(relation, list):\n            raise ValueError(\"Child objects must be a list of RelatedNodeInfo objects.\")\n        return relation\n\n    @property\n    def ref_doc_id(self) -> Optional[str]:  # pragma: no cover\n\"\"\"Deprecated: Get ref doc id.\"\"\"\n        source_node = self.source_node\n        if source_node is None:\n            return None\n        return source_node.node_id\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self) -> dict[str, Any]:  # pragma: no coverde\n        return self.metadata\n\n    @extra_info.setter\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self, extra_info: dict[str, Any]) -> None:  # pragma: no coverde\n        self.metadata = extra_info\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Node ID: {self.node_id}\\n{source_text_wrapped}\"\n\n    def get_embedding(self) -> List[float]:\n\"\"\"\n        Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    def as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n        return RelatedNodeInfo(\n            node_id=self.node_id,\n            node_type=self.get_type(),\n            metadata=self.metadata,\n            hash=self.hash,\n        )\n\n```\n  \n---|---  \n###  embedding `class-attribute` `instance-attribute` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.embedding \"Permanent link\")\n```\nembedding: Optional[[float]] = (default=None, description='Embedding of the node.')\n\n```\n\n\" metadata fields - injected as part of the text shown to LLMs as context - injected as part of the text for generating embeddings - used by vector DBs for metadata filtering\n###  hash `abstractmethod` `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  source_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.source_node \"Permanent link\")\n```\nsource_node: Optional[]\n\n```\n\nSource object node.\nExtracted from the relationships field.\n###  prev_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.prev_node \"Permanent link\")\n```\nprev_node: Optional[]\n\n```\n\nPrev node.\n###  next_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.next_node \"Permanent link\")\n```\nnext_node: Optional[]\n\n```\n\nNext node.\n###  parent_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.parent_node \"Permanent link\")\n```\nparent_node: Optional[]\n\n```\n\nParent node.\n###  child_nodes `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.child_nodes \"Permanent link\")\n```\nchild_nodes: Optional[[]]\n\n```\n\nChild nodes.\n###  ref_doc_id `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.ref_doc_id \"Permanent link\")\n```\nref_doc_id: Optional[]\n\n```\n\nDeprecated: Get ref doc id.\n###  get_type `abstractmethod` `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n321\n322\n323\n324\n```\n| ```\n@classmethod\n@abstractmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n```\n  \n---|---  \n###  get_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n326\n327\n328\n```\n| ```\n@abstractmethod\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_separator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n353\n354\n355\n```\n| ```\n@abstractmethod\ndef set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n```\n  \n---|---  \n###  get_embedding [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_embedding \"Permanent link\")\n```\nget_embedding() -> [float]\n\n```\n\nGet embedding.\nErrors if embedding is None.\nSource code in `llama_index/core/schema.py`\n```\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n```\n| ```\ndef get_embedding(self) -> List[float]:\n\"\"\"\n    Get embedding.\n\n    Errors if embedding is None.\n\n    \"\"\"\n    if self.embedding is None:\n        raise ValueError(\"embedding not set.\")\n    return self.embedding\n\n```\n  \n---|---  \n###  as_related_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.as_related_node_info \"Permanent link\")\n```\nas_related_node_info() -> \n\n```\n\nGet node as RelatedNodeInfo.\nSource code in `llama_index/core/schema.py`\n```\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\ndef as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n    return RelatedNodeInfo(\n        node_id=self.node_id,\n        node_type=self.get_type(),\n        metadata=self.metadata,\n        hash=self.hash,\n    )\n\n```\n  \n---|---  \n##  MediaResource [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"Permanent link\")\nBases: `BaseModel`\nA container class for media content.\nThis class represents a generic media resource that can be stored and accessed in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports storing vector embeddings for the media content.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`embeddings` |  `dict[Literal['sparse', 'dense'], list[float]] | None` |  Vector representation of this resource. |  `None`  \n`data` |  `bytes | None` |  base64 binary representation of this resource. |  `None`  \n`text` |  `str | None` |  Text representation of this resource. |  `None`  \n`path` |  `Path | None` |  Filesystem path of this resource. |  `None`  \n`url` |  `AnyUrl | None` |  URL to reach this resource. |  `None`  \n`mimetype` |  `str | None` |  MIME type of this resource. |  `None`  \nAttributes:\nName | Type | Description  \n---|---|---  \n`embeddings` |  Multi-vector dict representation of this resource for embedding-based search/retrieval  \n`text` |  Plain text representation of this resource  \n`data` |  Raw binary data of the media content  \n`mimetype` |  The MIME type indicating the format/type of the media content  \n`path` |  Local filesystem path where the media content can be accessed  \nURL where the media content can be accessed remotely  \nSource code in `llama_index/core/schema.py`\n```\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n```\n| ```\nclass MediaResource(BaseModel):\n\"\"\"\n    A container class for media content.\n\n    This class represents a generic media resource that can be stored and accessed\n    in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports\n    storing vector embeddings for the media content.\n\n    Attributes:\n        embeddings: Multi-vector dict representation of this resource for embedding-based search/retrieval\n        text: Plain text representation of this resource\n        data: Raw binary data of the media content\n        mimetype: The MIME type indicating the format/type of the media content\n        path: Local filesystem path where the media content can be accessed\n        url: URL where the media content can be accessed remotely\n\n    \"\"\"\n\n    embeddings: dict[EmbeddingKind, list[float]] | None = Field(\n        default=None, description=\"Vector representation of this resource.\"\n    )\n    data: bytes | None = Field(\n        default=None,\n        exclude=True,\n        description=\"base64 binary representation of this resource.\",\n    )\n    text: str | None = Field(\n        default=None, description=\"Text representation of this resource.\"\n    )\n    path: Path | None = Field(\n        default=None, description=\"Filesystem path of this resource.\"\n    )\n    url: AnyUrl | None = Field(default=None, description=\"URL to reach this resource.\")\n    mimetype: str | None = Field(\n        default=None, description=\"MIME type of this resource.\"\n    )\n\n    model_config = {\n        # This ensures validation runs even for None values\n        \"validate_default\": True\n    }\n\n    @field_validator(\"data\", mode=\"after\")\n    @classmethod\n    def validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n        If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n        In case the model was built passing binary data but without a mimetype,\n        we try to guess it using the filetype library. To avoid resource-intense\n        operations, we won't load the path or the URL to guess the mimetype.\n        \"\"\"\n        if v is None:\n            return v\n\n        try:\n            # Check if data is already base64 encoded.\n            # b64decode() can succeed on random binary data, so we\n            # pass verify=True to make sure it's not a false positive\n            decoded = base64.b64decode(v, validate=True)\n        except BinasciiError:\n            # b64decode failed, return encoded\n            return base64.b64encode(v)\n\n        # Good as is, return unchanged\n        return v\n\n    @field_validator(\"mimetype\", mode=\"after\")\n    @classmethod\n    def validate_mimetype(cls, v: str | None, info: ValidationInfo) -> str | None:\n        if v is not None:\n            return v\n\n        # Since this field validator runs after the one for `data`\n        # then the contents of `data` should be encoded already\n        b64_data = info.data.get(\"data\")\n        if b64_data:  # encoded bytes\n            decoded_data = base64.b64decode(b64_data)\n            if guess := filetype.guess(decoded_data):\n                return guess.mime\n\n        # guess from path\n        rpath: str | None = info.data[\"path\"]\n        if rpath:\n            extension = Path(rpath).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                return ftype.mime\n\n        return v\n\n    @field_serializer(\"path\")  # type: ignore\n    def serialize_path(\n        self, path: Optional[Path], _info: ValidationInfo\n    ) -> Optional[str]:\n        if path is None:\n            return path\n        return str(path)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash to uniquely identify the media resource.\n\n        The hash is generated based on the available content (data, path, text or url).\n        Returns an empty string if no content is available.\n        \"\"\"\n        bits: list[str] = []\n        if self.text is not None:\n            bits.append(self.text)\n        if self.data is not None:\n            # Hash the binary data if available\n            bits.append(str(sha256(self.data).hexdigest()))\n        if self.path is not None:\n            # Hash the file path if provided\n            bits.append(str(sha256(str(self.path).encode(\"utf-8\")).hexdigest()))\n        if self.url is not None:\n            # Use the URL string as basis for hash\n            bits.append(str(sha256(str(self.url).encode(\"utf-8\")).hexdigest()))\n\n        doc_identity = \"\".join(bits)\n        if not doc_identity:\n            return \"\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash to uniquely identify the media resource.\nThe hash is generated based on the available content (data, path, text or url). Returns an empty string if no content is available.\n###  validate_data `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.validate_data \"Permanent link\")\n```\nvalidate_data(v: bytes | None, info: ValidationInfo) -> bytes | None\n\n```\n\nIf binary data was passed, store the resource as base64 and guess the mimetype when possible.\nIn case the model was built passing binary data but without a mimetype, we try to guess it using the filetype library. To avoid resource-intense operations, we won't load the path or the URL to guess the mimetype.\nSource code in `llama_index/core/schema.py`\n```\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n```\n| ```\n@field_validator(\"data\", mode=\"after\")\n@classmethod\ndef validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n    If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n    In case the model was built passing binary data but without a mimetype,\n    we try to guess it using the filetype library. To avoid resource-intense\n    operations, we won't load the path or the URL to guess the mimetype.\n    \"\"\"\n    if v is None:\n        return v\n\n    try:\n        # Check if data is already base64 encoded.\n        # b64decode() can succeed on random binary data, so we\n        # pass verify=True to make sure it's not a false positive\n        decoded = base64.b64decode(v, validate=True)\n    except BinasciiError:\n        # b64decode failed, return encoded\n        return base64.b64encode(v)\n\n    # Good as is, return unchanged\n    return v\n\n```\n  \n---|---  \n##  Node [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Text content of the node. |  `None`  \n`image_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Image content of the node. |  `None`  \n`audio_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Audio content of the node. |  `None`  \n`video_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Video content of the node. |  `None`  \n`text_template` |  Template for how text_resource is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n```\n| ```\nclass Node(BaseNode):\n    text_resource: MediaResource | None = Field(\n        default=None, description=\"Text content of the node.\"\n    )\n    image_resource: MediaResource | None = Field(\n        default=None, description=\"Image content of the node.\"\n    )\n    audio_resource: MediaResource | None = Field(\n        default=None, description=\"Audio content of the node.\"\n    )\n    video_resource: MediaResource | None = Field(\n        default=None, description=\"Video content of the node.\"\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text_resource is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Node\"\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.MULTIMODAL\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n        Get the text content for the node if available.\n\n        Provided for backward compatibility, use self.text_resource directly instead.\n        \"\"\"\n        if self.text_resource:\n            metadata_str = self.get_metadata_str(metadata_mode)\n            if metadata_mode == MetadataMode.NONE or not metadata_str:\n                return self.text_resource.text or \"\"\n\n            return self.text_template.format(\n                content=self.text_resource.text or \"\",\n                metadata_str=metadata_str,\n            ).strip()\n        return \"\"\n\n    def set_content(self, value: str) -> None:\n\"\"\"\n        Set the text content of the node.\n\n        Provided for backward compatibility, set self.text_resource instead.\n        \"\"\"\n        self.text_resource = MediaResource(text=value)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash representing the state of the node.\n\n        The hash is generated based on the available resources (audio, image, text or video) and its metadata.\n        \"\"\"\n        doc_identities = []\n        metadata_str = self.get_metadata_str(mode=MetadataMode.ALL)\n        if metadata_str:\n            doc_identities.append(metadata_str)\n        if self.audio_resource is not None:\n            doc_identities.append(self.audio_resource.hash)\n        if self.image_resource is not None:\n            doc_identities.append(self.image_resource.hash)\n        if self.text_resource is not None:\n            doc_identities.append(self.text_resource.hash)\n        if self.video_resource is not None:\n            doc_identities.append(self.video_resource.hash)\n\n        doc_identity = \"-\".join(doc_identities)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash representing the state of the node.\nThe hash is generated based on the available resources (audio, image, text or video) and its metadata.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n637\n638\n639\n640\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.MULTIMODAL\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet the text content for the node if available.\nProvided for backward compatibility, use self.text_resource directly instead.\nSource code in `llama_index/core/schema.py`\n```\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n    Get the text content for the node if available.\n\n    Provided for backward compatibility, use self.text_resource directly instead.\n    \"\"\"\n    if self.text_resource:\n        metadata_str = self.get_metadata_str(metadata_mode)\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text_resource.text or \"\"\n\n        return self.text_template.format(\n            content=self.text_resource.text or \"\",\n            metadata_str=metadata_str,\n        ).strip()\n    return \"\"\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the text content of the node.\nProvided for backward compatibility, set self.text_resource instead.\nSource code in `llama_index/core/schema.py`\n```\n659\n660\n661\n662\n663\n664\n665\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"\n    Set the text content of the node.\n\n    Provided for backward compatibility, set self.text_resource instead.\n    \"\"\"\n    self.text_resource = MediaResource(text=value)\n\n```\n  \n---|---  \n##  TextNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode \"Permanent link\")\nBases: \nProvided for backward compatibility.\nNote: we keep the field with the typo \"seperator\" to maintain backward compatibility for serialized objects.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text` |  Text content of the node.  \n`mimetype` |  MIME type of the node content. |  `'text/plain'`  \n`start_char_idx` |  `int | None` |  Start char index of the node. |  `None`  \n`end_char_idx` |  `int | None` |  End char index of the node. |  `None`  \n`metadata_seperator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \n`text_template` |  Template for how text is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n```\n| ```\nclass TextNode(BaseNode):\n\"\"\"\n    Provided for backward compatibility.\n\n    Note: we keep the field with the typo \"seperator\" to maintain backward compatibility for\n    serialized objects.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make TextNode forward-compatible with Node by supporting 'text_resource' in the constructor.\"\"\"\n        if \"text_resource\" in kwargs:\n            tr = kwargs.pop(\"text_resource\")\n            if isinstance(tr, MediaResource):\n                kwargs[\"text\"] = tr.text\n            else:\n                kwargs[\"text\"] = tr[\"text\"]\n        super().__init__(*args, **kwargs)\n\n    text: str = Field(default=\"\", description=\"Text content of the node.\")\n    mimetype: str = Field(\n        default=\"text/plain\", description=\"MIME type of the node content.\"\n    )\n    start_char_idx: Optional[int] = Field(\n        default=None, description=\"Start char index of the node.\"\n    )\n    end_char_idx: Optional[int] = Field(\n        default=None, description=\"End char index of the node.\"\n    )\n    metadata_seperator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TextNode\"\n\n    @property\n    def hash(self) -> str:\n        doc_identity = str(self.text) + str(self.metadata)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.TEXT\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n        metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text\n\n        return self.text_template.format(\n            content=self.text, metadata_str=metadata_str\n        ).strip()\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_seperator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    def set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n        self.text = value\n\n    def get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n        return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n    def get_text(self) -> str:\n        return self.get_content(metadata_mode=MetadataMode.NONE)\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'node_info' is deprecated, use 'get_node_info' instead.\",\n    )\n    def node_info(self) -> Dict[str, Any]:\n\"\"\"Deprecated: Get node info.\"\"\"\n        return self.get_node_info()\n\n```\n  \n---|---  \n###  node_info `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.node_info \"Permanent link\")\n```\nnode_info: [, ]\n\n```\n\nDeprecated: Get node info.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n740\n741\n742\n743\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.TEXT\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n745\n746\n747\n748\n749\n750\n751\n752\n753\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n    metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n    if metadata_mode == MetadataMode.NONE or not metadata_str:\n        return self.text\n\n    return self.text_template.format(\n        content=self.text, metadata_str=metadata_str\n    ).strip()\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_seperator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n778\n779\n780\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n    self.text = value\n\n```\n  \n---|---  \n###  get_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_node_info \"Permanent link\")\n```\nget_node_info() -> [, ]\n\n```\n\nGet node info.\nSource code in `llama_index/core/schema.py`\n```\n782\n783\n784\n```\n| ```\ndef get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n    return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n```\n  \n---|---  \n##  ImageNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode \"Permanent link\")\nBases: \nNode with image.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`image` |  `str | None` |  `None`  \n`image_path` |  `str | None` |  `None`  \n`image_url` |  `str | None` |  `None`  \n`image_mimetype` |  `str | None` |  `None`  \n`text_embedding` |  `List[float] | None` |  Text embedding of image node, if text field is filled out |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n```\n| ```\nclass ImageNode(TextNode):\n\"\"\"Node with image.\"\"\"\n\n    # TODO: store reference instead of actual image\n    # base64 encoded image str\n    image: Optional[str] = None\n    image_path: Optional[str] = None\n    image_url: Optional[str] = None\n    image_mimetype: Optional[str] = None\n    text_embedding: Optional[List[float]] = Field(\n        default=None,\n        description=\"Text embedding of image node, if text field is filled out\",\n    )\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make ImageNode forward-compatible with Node by supporting 'image_resource' in the constructor.\"\"\"\n        if \"image_resource\" in kwargs:\n            ir = kwargs.pop(\"image_resource\")\n            if isinstance(ir, MediaResource):\n                kwargs[\"image_path\"] = ir.path.as_posix() if ir.path else None\n                kwargs[\"image_url\"] = ir.url\n                kwargs[\"image_mimetype\"] = ir.mimetype\n            else:\n                kwargs[\"image_path\"] = ir.get(\"path\", None)\n                kwargs[\"image_url\"] = ir.get(\"url\", None)\n                kwargs[\"image_mimetype\"] = ir.get(\"mimetype\", None)\n\n        mimetype = kwargs.get(\"image_mimetype\")\n        if not mimetype and kwargs.get(\"image_path\") is not None:\n            # guess mimetype from image_path\n            extension = Path(kwargs[\"image_path\"]).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                kwargs[\"image_mimetype\"] = ftype.mime\n\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.IMAGE\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageNode\"\n\n    def resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n        if self.image is not None:\n            import base64\n\n            return BytesIO(base64.b64decode(self.image))\n        elif self.image_path is not None:\n            return self.image_path\n        elif self.image_url is not None:\n            # load image from URL\n            import requests\n\n            response = requests.get(self.image_url, timeout=(60, 60))\n            return BytesIO(response.content)\n        else:\n            raise ValueError(\"No image found in node.\")\n\n    @property\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n        # doc identity depends on if image, image_path, or image_url is set\n        image_str = self.image or \"None\"\n        image_path_str = self.image_path or \"None\"\n        image_url_str = self.image_url or \"None\"\n        image_text = self.text or \"None\"\n        doc_identity = f\"{image_str}-{image_path_str}-{image_url_str}-{image_text}\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.resolve_image \"Permanent link\")\n```\nresolve_image() -> ImageType\n\n```\n\nResolve an image such that PIL can read it.\nSource code in `llama_index/core/schema.py`\n```\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n```\n| ```\ndef resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n    if self.image is not None:\n        import base64\n\n        return BytesIO(base64.b64decode(self.image))\n    elif self.image_path is not None:\n        return self.image_path\n    elif self.image_url is not None:\n        # load image from URL\n        import requests\n\n        response = requests.get(self.image_url, timeout=(60, 60))\n        return BytesIO(response.content)\n    else:\n        raise ValueError(\"No image found in node.\")\n\n```\n  \n---|---  \n##  IndexNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode \"Permanent link\")\nBases: \nNode with reference to any object.\nThis can include other indices, query engines, retrievers.\nThis can also include other nodes (though this is overlapping with `relationships` on the Node class).\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`index_id` |  _required_  \n`obj` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n```\n| ```\nclass IndexNode(TextNode):\n\"\"\"\n    Node with reference to any object.\n\n    This can include other indices, query engines, retrievers.\n\n    This can also include other nodes (though this is overlapping with `relationships`\n    on the Node class).\n\n    \"\"\"\n\n    index_id: str\n    obj: Any = None\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        from llama_index.core.storage.docstore.utils import doc_to_json\n\n        data = super().dict(**kwargs)\n\n        try:\n            if self.obj is None:\n                data[\"obj\"] = None\n            elif isinstance(self.obj, BaseNode):\n                data[\"obj\"] = doc_to_json(self.obj)\n            elif isinstance(self.obj, BaseModel):\n                data[\"obj\"] = self.obj.model_dump()\n            else:\n                data[\"obj\"] = json.dumps(self.obj)\n        except Exception:\n            raise ValueError(\"IndexNode obj is not serializable: \" + str(self.obj))\n\n        return data\n\n    @classmethod\n    def from_text_node(\n        cls,\n        node: TextNode,\n        index_id: str,\n    ) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n        # copy all attributes from text node, add index id\n        return cls(\n            **node.dict(),\n            index_id=index_id,\n        )\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        output = super().from_dict(data, **kwargs)\n\n        obj = data.get(\"obj\")\n        parsed_obj = None\n\n        if isinstance(obj, str):\n            parsed_obj = TextNode(text=obj)\n        elif isinstance(obj, dict):\n            from llama_index.core.storage.docstore.utils import json_to_doc\n\n            # check if its a node, else assume stringable\n            try:\n                parsed_obj = json_to_doc(obj)  # type: ignore[assignment]\n            except Exception:\n                parsed_obj = TextNode(text=str(obj))\n\n        output.obj = parsed_obj\n\n        return output\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.INDEX\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"IndexNode\"\n\n```\n  \n---|---  \n###  from_text_node `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode.from_text_node \"Permanent link\")\n```\nfrom_text_node(node: , index_id: ) -> \n\n```\n\nCreate index node from text node.\nSource code in `llama_index/core/schema.py`\n```\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n```\n| ```\n@classmethod\ndef from_text_node(\n    cls,\n    node: TextNode,\n    index_id: str,\n) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n    # copy all attributes from text node, add index id\n    return cls(\n        **node.dict(),\n        index_id=index_id,\n    )\n\n```\n  \n---|---  \n##  NodeWithScore [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node` |  |  _required_  \n`score` |  `float | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n```\n| ```\nclass NodeWithScore(BaseComponent):\n    node: SerializeAsAny[BaseNode]\n    score: Optional[float] = None\n\n    def __str__(self) -> str:\n        score_str = \"None\" if self.score is None else f\"{self.score: 0.3f}\"\n        return f\"{self.node}\\nScore: {score_str}\\n\"\n\n    def get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n        if self.score is None:\n            if raise_error:\n                raise ValueError(\"Score not set.\")\n            else:\n                return 0.0\n        else:\n            return self.score\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"NodeWithScore\"\n\n    ##### pass through methods to BaseNode #####\n    @property\n    def node_id(self) -> str:\n        return self.node.node_id\n\n    @property\n    def id_(self) -> str:\n        return self.node.id_\n\n    @property\n    def text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.text\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        return self.node.metadata\n\n    @property\n    def embedding(self) -> Optional[List[float]]:\n        return self.node.embedding\n\n    def get_text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.get_text()\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n        return self.node.get_content(metadata_mode=metadata_mode)\n\n    def get_embedding(self) -> List[float]:\n        return self.node.get_embedding()\n\n```\n  \n---|---  \n###  get_score [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore.get_score \"Permanent link\")\n```\nget_score(raise_error:  = False) -> float\n\n```\n\nGet score.\nSource code in `llama_index/core/schema.py`\n```\n958\n959\n960\n961\n962\n963\n964\n965\n966\n```\n| ```\ndef get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n    if self.score is None:\n        if raise_error:\n            raise ValueError(\"Score not set.\")\n        else:\n            return 0.0\n    else:\n        return self.score\n\n```\n  \n---|---  \n##  Document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document \"Permanent link\")\nBases: \nGeneric interface for a data document.\nThis document connects to data sources.\nSource code in `llama_index/core/schema.py`\n```\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\nclass Document(Node):\n\"\"\"\n    Generic interface for a data document.\n\n    This document connects to data sources.\n    \"\"\"\n\n    def __init__(self, **data: Any) -> None:\n\"\"\"\n        Keeps backward compatibility with old 'Document' versions.\n\n        If 'text' was passed, store it in 'text_resource'.\n        If 'doc_id' was passed, store it in 'id_'.\n        If 'extra_info' was passed, store it in 'metadata'.\n        \"\"\"\n        if \"doc_id\" in data:\n            value = data.pop(\"doc_id\")\n            if \"id_\" in data:\n                msg = \"'doc_id' is deprecated and 'id_' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"id_\"] = value\n\n        if \"extra_info\" in data:\n            value = data.pop(\"extra_info\")\n            if \"metadata\" in data:\n                msg = \"'extra_info' is deprecated and 'metadata' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"metadata\"] = value\n\n        if data.get(\"text\"):\n            text = data.pop(\"text\")\n            if \"text_resource\" in data:\n                text_resource = (\n                    data[\"text_resource\"]\n                    if isinstance(data[\"text_resource\"], MediaResource)\n                    else MediaResource.model_validate(data[\"text_resource\"])\n                )\n                if (text_resource.text or \"\").strip() != text.strip():\n                    msg = (\n                        \"'text' is deprecated and 'text_resource' will be used instead\"\n                    )\n                    logging.warning(msg)\n            else:\n                data[\"text_resource\"] = MediaResource(text=text)\n\n        super().__init__(**data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n        data = super().custom_model_dump(handler, info)\n        exclude_set = set(info.exclude or [])\n        if \"text\" not in exclude_set:\n            data[\"text\"] = self.text\n        return data\n\n    @property\n    def text(self) -> str:\n\"\"\"Provided for backward compatibility, it returns the content of text_resource.\"\"\"\n        return self.get_content()\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n        return ObjectType.DOCUMENT\n\n    @property\n    def doc_id(self) -> str:\n\"\"\"Get document ID.\"\"\"\n        return self.id_\n\n    @doc_id.setter\n    def doc_id(self, id_: str) -> None:\n        self.id_ = id_\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Doc ID: {self.doc_id}\\n{source_text_wrapped}\"\n\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'get_doc_id' is deprecated, access the 'id_' property instead.\",\n    )\n    def get_doc_id(self) -> str:  # pragma: nocover\n        return self.id_\n\n    def to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n        from llama_index.core.bridge.langchain import (\n            Document as LCDocument,  # type: ignore\n        )\n\n        metadata = self.metadata or {}\n        return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n    @classmethod\n    def from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n        if doc.id:\n            return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n        return cls(text=doc.page_content, metadata=doc.metadata)\n\n    def to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n        from haystack import Document as HaystackDocument  # type: ignore\n\n        return HaystackDocument(\n            content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n        )\n\n    @classmethod\n    def from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n        return cls(\n            text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n        )\n\n    def to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n        return {\n            \"doc_id\": self.id_,\n            \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n        }\n\n    @classmethod\n    def from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n        return cls(\n            text=doc[\"data\"][\"content\"],\n            metadata=doc[\"data\"][\"meta_data\"],\n            id_=doc[\"doc_id\"],\n        )\n\n    def to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n        import numpy as np\n        from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n        return MemoryRecord(\n            id=self.id_,\n            text=self.text,\n            additional_metadata=self.get_metadata_str(),\n            embedding=np.array(self.embedding) if self.embedding else None,\n        )\n\n    @classmethod\n    def from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n        return cls(\n            text=doc._text,\n            metadata={\"additional_metadata\": doc._additional_metadata},\n            embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n            id_=doc._id,\n        )\n\n    def to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n        # write document to temp file\n        import tempfile\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(self.text.encode(\"utf-8\"))\n            f.flush()\n            client.embed(f.name)\n\n    @classmethod\n    def example(cls) -> Document:\n        return Document(\n            text=SAMPLE_TEXT,\n            metadata={\"filename\": \"README.md\", \"category\": \"codebase\"},\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Document\"\n\n    def to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n        from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n        return CloudDocument(\n            text=self.text,\n            metadata=self.metadata,\n            excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n            id=self.id_,\n        )\n\n    @classmethod\n    def from_cloud_document(\n        cls,\n        doc: CloudDocument,\n    ) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n        return Document(\n            text=doc.text,\n            metadata=doc.metadata,\n            excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n            id_=doc.id,\n        )\n\n```\n  \n---|---  \n###  text `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.text \"Permanent link\")\n```\ntext: \n\n```\n\nProvided for backward compatibility, it returns the content of text_resource.\n###  doc_id `property` `writable` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.doc_id \"Permanent link\")\n```\ndoc_id: \n\n```\n\nGet document ID.\n###  custom_model_dump [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.custom_model_dump \"Permanent link\")\n```\ncustom_model_dump(handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> [, ]\n\n```\n\nFor full backward compatibility with the text field, we customize the model serializer.\nSource code in `llama_index/core/schema.py`\n```\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n```\n| ```\n@model_serializer(mode=\"wrap\")\ndef custom_model_dump(\n    self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n    data = super().custom_model_dump(handler, info)\n    exclude_set = set(info.exclude or [])\n    if \"text\" not in exclude_set:\n        data[\"text\"] = self.text\n    return data\n\n```\n  \n---|---  \n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Document type.\nSource code in `llama_index/core/schema.py`\n```\n1077\n1078\n1079\n1080\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n    return ObjectType.DOCUMENT\n\n```\n  \n---|---  \n###  to_langchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_langchain_format \"Permanent link\")\n```\nto_langchain_format() -> Document\n\n```\n\nConvert struct to LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n```\n| ```\ndef to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n    from llama_index.core.bridge.langchain import (\n        Document as LCDocument,  # type: ignore\n    )\n\n    metadata = self.metadata or {}\n    return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n```\n  \n---|---  \n###  from_langchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_langchain_format \"Permanent link\")\n```\nfrom_langchain_format(doc: Document) -> \n\n```\n\nConvert struct from LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1116\n1117\n1118\n1119\n1120\n1121\n```\n| ```\n@classmethod\ndef from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n    if doc.id:\n        return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n    return cls(text=doc.page_content, metadata=doc.metadata)\n\n```\n  \n---|---  \n###  to_haystack_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_haystack_format \"Permanent link\")\n```\nto_haystack_format() -> Document\n\n```\n\nConvert struct to Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n```\n| ```\ndef to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n    from haystack import Document as HaystackDocument  # type: ignore\n\n    return HaystackDocument(\n        content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n    )\n\n```\n  \n---|---  \n###  from_haystack_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_haystack_format \"Permanent link\")\n```\nfrom_haystack_format(doc: Document) -> \n\n```\n\nConvert struct from Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1131\n1132\n1133\n1134\n1135\n1136\n```\n| ```\n@classmethod\ndef from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n    return cls(\n        text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n    )\n\n```\n  \n---|---  \n###  to_embedchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_embedchain_format \"Permanent link\")\n```\nto_embedchain_format() -> [, ]\n\n```\n\nConvert struct to EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1138\n1139\n1140\n1141\n1142\n1143\n```\n| ```\ndef to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n    return {\n        \"doc_id\": self.id_,\n        \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n    }\n\n```\n  \n---|---  \n###  from_embedchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_embedchain_format \"Permanent link\")\n```\nfrom_embedchain_format(doc: [, ]) -> \n\n```\n\nConvert struct from EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n```\n| ```\n@classmethod\ndef from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n    return cls(\n        text=doc[\"data\"][\"content\"],\n        metadata=doc[\"data\"][\"meta_data\"],\n        id_=doc[\"doc_id\"],\n    )\n\n```\n  \n---|---  \n###  to_semantic_kernel_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_semantic_kernel_format \"Permanent link\")\n```\nto_semantic_kernel_format() -> MemoryRecord\n\n```\n\nConvert struct to Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n```\n| ```\ndef to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n    import numpy as np\n    from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n    return MemoryRecord(\n        id=self.id_,\n        text=self.text,\n        additional_metadata=self.get_metadata_str(),\n        embedding=np.array(self.embedding) if self.embedding else None,\n    )\n\n```\n  \n---|---  \n###  from_semantic_kernel_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_semantic_kernel_format \"Permanent link\")\n```\nfrom_semantic_kernel_format(doc: MemoryRecord) -> \n\n```\n\nConvert struct from Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n```\n| ```\n@classmethod\ndef from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n    return cls(\n        text=doc._text,\n        metadata={\"additional_metadata\": doc._additional_metadata},\n        embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n        id_=doc._id,\n    )\n\n```\n  \n---|---  \n###  to_vectorflow [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_vectorflow \"Permanent link\")\n```\nto_vectorflow(client: ) -> None\n\n```\n\nSend a document to vectorflow, since they don't have a document object.\nSource code in `llama_index/core/schema.py`\n```\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n```\n| ```\ndef to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n    # write document to temp file\n    import tempfile\n\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(self.text.encode(\"utf-8\"))\n        f.flush()\n        client.embed(f.name)\n\n```\n  \n---|---  \n###  to_cloud_document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_cloud_document \"Permanent link\")\n```\nto_cloud_document() -> CloudDocument\n\n```\n\nConvert to LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n```\n| ```\ndef to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n    from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n    return CloudDocument(\n        text=self.text,\n        metadata=self.metadata,\n        excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n        id=self.id_,\n    )\n\n```\n  \n---|---  \n###  from_cloud_document `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_cloud_document \"Permanent link\")\n```\nfrom_cloud_document(doc: CloudDocument) -> \n\n```\n\nConvert from LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\n@classmethod\ndef from_cloud_document(\n    cls,\n    doc: CloudDocument,\n) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n    return Document(\n        text=doc.text,\n        metadata=doc.metadata,\n        excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n        id_=doc.id,\n    )\n\n```\n  \n---|---  \n##  ImageDocument [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument \"Permanent link\")\nBases: \nBackward compatible wrapper around Document containing an image.\nSource code in `llama_index/core/schema.py`\n```\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\nclass ImageDocument(Document):\n\"\"\"Backward compatible wrapper around Document containing an image.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        image = kwargs.pop(\"image\", None)\n        image_path = kwargs.pop(\"image_path\", None)\n        image_url = kwargs.pop(\"image_url\", None)\n        image_mimetype = kwargs.pop(\"image_mimetype\", None)\n        text_embedding = kwargs.pop(\"text_embedding\", None)\n\n        if image:\n            kwargs[\"image_resource\"] = MediaResource(\n                data=image, mimetype=image_mimetype\n            )\n        elif image_path:\n            if not is_image_pil(image_path):\n                raise ValueError(\"The specified file path is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                path=image_path, mimetype=image_mimetype\n            )\n        elif image_url:\n            if not is_image_url_pil(image_url):\n                raise ValueError(\"The specified URL is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                url=image_url, mimetype=image_mimetype\n            )\n\n        super().__init__(**kwargs)\n\n    @property\n    def image(self) -> str | None:\n        if self.image_resource and self.image_resource.data:\n            return self.image_resource.data.decode(\"utf-8\")\n        return None\n\n    @image.setter\n    def image(self, image: str) -> None:\n        self.image_resource = MediaResource(data=image.encode(\"utf-8\"))\n\n    @property\n    def image_path(self) -> str | None:\n        if self.image_resource and self.image_resource.path:\n            return str(self.image_resource.path)\n        return None\n\n    @image_path.setter\n    def image_path(self, image_path: str) -> None:\n        self.image_resource = MediaResource(path=Path(image_path))\n\n    @property\n    def image_url(self) -> str | None:\n        if self.image_resource and self.image_resource.url:\n            return str(self.image_resource.url)\n        return None\n\n    @image_url.setter\n    def image_url(self, image_url: str) -> None:\n        self.image_resource = MediaResource(url=AnyUrl(url=image_url))\n\n    @property\n    def image_mimetype(self) -> str | None:\n        if self.image_resource:\n            return self.image_resource.mimetype\n        return None\n\n    @image_mimetype.setter\n    def image_mimetype(self, image_mimetype: str) -> None:\n        if self.image_resource:\n            self.image_resource.mimetype = image_mimetype\n\n    @property\n    def text_embedding(self) -> list[float] | None:\n        if self.text_resource and self.text_resource.embeddings:\n            return self.text_resource.embeddings.get(\"dense\")\n        return None\n\n    @text_embedding.setter\n    def text_embedding(self, embeddings: list[float]) -> None:\n        if self.text_resource:\n            if self.text_resource.embeddings is None:\n                self.text_resource.embeddings = {}\n            self.text_resource.embeddings[\"dense\"] = embeddings\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageDocument\"\n\n    def resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n        Resolve an image such that PIL can read it.\n\n        Args:\n            as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n        \"\"\"\n        if self.image_resource is None:\n            return BytesIO()\n\n        if self.image_resource.data is not None:\n            if as_base64:\n                return BytesIO(self.image_resource.data)\n            return BytesIO(base64.b64decode(self.image_resource.data))\n        elif self.image_resource.path is not None:\n            img_bytes = self.image_resource.path.read_bytes()\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        elif self.image_resource.url is not None:\n            # load image from URL\n            response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n            img_bytes = response.content\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        else:\n            raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument.resolve_image \"Permanent link\")\n```\nresolve_image(as_base64:  = False) -> BytesIO\n\n```\n\nResolve an image such that PIL can read it.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`as_base64` |  `bool` |  whether the resolved image should be returned as base64-encoded bytes |  `False`  \nSource code in `llama_index/core/schema.py`\n```\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\ndef resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n    Resolve an image such that PIL can read it.\n\n    Args:\n        as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n    \"\"\"\n    if self.image_resource is None:\n        return BytesIO()\n\n    if self.image_resource.data is not None:\n        if as_base64:\n            return BytesIO(self.image_resource.data)\n        return BytesIO(base64.b64decode(self.image_resource.data))\n    elif self.image_resource.path is not None:\n        img_bytes = self.image_resource.path.read_bytes()\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    elif self.image_resource.url is not None:\n        # load image from URL\n        response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n        img_bytes = response.content\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    else:\n        raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n##  QueryBundle `dataclass` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle \"Permanent link\")\nBases: `DataClassJsonMixin`\nQuery bundle.\nThis dataclass contains the original query string and associated transformations.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`query_str` |  the original user-specified query string. This is currently used by all non embedding-based queries. |  _required_  \n`custom_embedding_strs` |  `list[str]` |  list of strings used for embedding the query. This is currently used by all embedding-based queries. |  `None`  \n`embedding` |  `list[float]` |  the stored embedding for the query. |  `None`  \n`image_path` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n```\n| ```\n@dataclass\nclass QueryBundle(DataClassJsonMixin):\n\"\"\"\n    Query bundle.\n\n    This dataclass contains the original query string and associated transformations.\n\n    Args:\n        query_str (str): the original user-specified query string.\n            This is currently used by all non embedding-based queries.\n        custom_embedding_strs (list[str]): list of strings used for embedding the query.\n            This is currently used by all embedding-based queries.\n        embedding (list[float]): the stored embedding for the query.\n\n    \"\"\"\n\n    query_str: str\n    # using single image path as query input\n    image_path: Optional[str] = None\n    custom_embedding_strs: Optional[List[str]] = None\n    embedding: Optional[List[float]] = None\n\n    @property\n    def embedding_strs(self) -> List[str]:\n\"\"\"Use custom embedding strs if specified, otherwise use query str.\"\"\"\n        if self.custom_embedding_strs is None:\n            if len(self.query_str) == 0:\n                return []\n            return [self.query_str]\n        else:\n            return self.custom_embedding_strs\n\n    @property\n    def embedding_image(self) -> List[ImageType]:\n\"\"\"Use image path for image retrieval.\"\"\"\n        if self.image_path is None:\n            return []\n        return [self.image_path]\n\n    def __str__(self) -> str:\n\"\"\"Convert to string representation.\"\"\"\n        return self.query_str\n\n```\n  \n---|---  \n###  embedding_strs `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_strs \"Permanent link\")\n```\nembedding_strs: []\n\n```\n\nUse custom embedding strs if specified, otherwise use query str.\n###  embedding_image `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_image \"Permanent link\")\n```\nembedding_image: [ImageType]\n\n```\n\nUse image path for image retrieval.\n"}, "__type__": "4"}, "aaf853f8-6515-49be-b87b-0bbea59462d2": {"__data__": {"id_": "aaf853f8-6515-49be-b87b-0bbea59462d2", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework_getting_started_starter_example_local.md", "file_name": "python_framework_getting_started_starter_example_local.md", "file_type": "text/markdown", "file_size": 8669, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#_top)\n# Starter Tutorial (Using Local LLMs)\nThis tutorial will show you how to get started building agents with LlamaIndex. We\u2019ll start with a basic example and then show how to add RAG (Retrieval-Augmented Generation) capabilities.\nWe will use [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) as our embedding model and `llama3.1 8B` served through `Ollama`.\n## Setup\n[Section titled \u201cSetup\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#setup)\nOllama is a tool to help you get set up with LLMs locally with minimal setup.\nFollow the [README](https://github.com/jmorganca/ollama) to learn how to install it.\nTo download the Llama3 model just do `ollama pull llama3.1`.\n**NOTE** : You will need a machine with at least ~32GB of RAM.\nAs explained in our [installation guide](https://developers.llamaindex.ai/python/framework/getting_started/installation), `llama-index` is actually a collection of packages. To run Ollama and Huggingface, we will need to install those integrations:\nTerminal window```\n\n\npipinstallllama-index-llms-ollamallama-index-embeddings-huggingface\n\n\n```\n\nThe package names spell out the imports, which is very helpful for remembering how to import them or install them!\n```\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n```\n\nMore integrations are all listed on <https://llamahub.ai>.\n## Basic Agent Example\n[Section titled \u201cBasic Agent Example\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#basic-agent-example)\nLet\u2019s start with a simple example using an agent that can perform basic multiplication by calling a tool. Create a file called `starter.py`:\n```\n\n\nimport asyncio\n\n\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\n\n# Define a simple calculator tool\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n# Create an agent workflow with our calculator tool\n\n\n\nagent =FunctionAgent(\n\n\n\n\ntools=[multiply],\n\n\n\n\nllm=Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\nsystem_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n\n\n\n\n\n\n\nasyncdefmain():\n\n\n\n\n# Run the agent\n\n\n\n\nresponse =await agent.run(\"What is 1234 * 4567?\")\n\n\n\n\nprint(str(response))\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThis will output something like: `The answer to 1234 * 4567 is 5635678.`\nWhat happened is:\n  * The agent was given a question: `What is 1234 * 4567?`\n  * Under the hood, this question, plus the schema of the tools (name, docstring, and arguments) were passed to the LLM\n  * The agent selected the `multiply` tool and wrote the arguments to the tool\n  * The agent received the result from the tool and interpolated it into the final response\n\n\n## Adding Chat History\n[Section titled \u201cAdding Chat History\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-chat-history)\nThe `AgentWorkflow` is also able to remember previous messages. This is contained inside the `Context` of the `AgentWorkflow`.\nIf the `Context` is passed in, the agent will use it to continue the conversation.\n```\n\n\nfrom llama_index.core.workflow import Context\n\n\n\n\n# create context\n\n\n\nctx =Context(agent)\n\n\n\n\n# run agent with context\n\n\n\nresponse =await agent.run(\"My name is Logan\",ctx=ctx)\n\n\n\n\nresponse =await agent.run(\"What is my name?\",ctx=ctx)\n\n\n```\n\n## Adding RAG Capabilities\n[Section titled \u201cAdding RAG Capabilities\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-rag-capabilities)\nNow let\u2019s enhance our agent by adding the ability to search through documents. First, let\u2019s get some example data using our terminal:\nTerminal window```\n\n\nmkdirdata\n\n\n\n\nwgethttps://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt-Odata/paul_graham_essay.txt\n\n\n```\n\nYour directory structure should look like this now:\n```\n\u251c\u2500\u2500 starter.py \u2514\u2500\u2500 data \u00a0\u00a0 \u2514\u2500\u2500 paul_graham_essay.txt\n```\n\nNow we can create a tool for searching through documents using LlamaIndex. By default, our `VectorStoreIndex` will use a `text-embedding-ada-002` embeddings from OpenAI to embed and retrieve the text.\nOur modified `starter.py` should look like this:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n\n\n\n\nfrom llama_index.core.agent.workflow import AgentWorkflow\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\nimport asyncio\n\n\n\n\nimport os\n\n\n\n\n# Settings control global defaults\n\n\n\nSettings.embed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n\n\n\nSettings.llm =Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\n# Create a RAG tool using LlamaIndex\n\n\n\ndocuments =SimpleDirectoryReader(\"data\").load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n\nasyncdefsearch_documents(query: str) -> str:\n\n\n\n\n\"\"\"Useful for answering natural language questions about an personal essay written by Paul Graham.\"\"\"\n\n\n\n\nresponse =await query_engine.aquery(query)\n\n\n\n\nreturnstr(response)\n\n\n\n\n\n# Create an enhanced workflow with both tools\n\n\n\nagent = AgentWorkflow.from_tools_or_functions(\n\n\n\n\n[multiply, search_documents],\n\n\n\n\nllm=Settings.llm,\n\n\n\n\nsystem_prompt=\"\"\"You are a helpful assistant that can perform calculations\n\n\n\n\nand search through documents to answer questions.\"\"\",\n\n\n\n\n\n\n# Now we can ask questions about the documents or do calculations\n\n\n\nasyncdefmain():\n\n\n\n\nresponse =await agent.run(\n\n\n\n\n\"What did the author do in college? Also, what's 7 * 8?\"\n\n\n\n\n\nprint(response)\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThe agent can now seamlessly switch between using the calculator and searching through documents to answer questions.\n## Storing the RAG Index\n[Section titled \u201cStoring the RAG Index\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#storing-the-rag-index)\nTo avoid reprocessing documents every time, you can persist the index to disk:\n```\n\n# Save the index\n\n\n\nindex.storage_context.persist(\"storage\")\n\n\n\n\n# Later, load the index\n\n\n\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n\n\n\n\nindex =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n```\n\n```\n\n\nindex = VectorStoreIndex.from_vector_store(\n\n\n\n\nvector_store,\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n```\n\n## What\u2019s Next?\n[Section titled \u201cWhat\u2019s Next?\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#whats-next)\nThis is just the beginning of what you can do with LlamaIndex agents! You can:\n  * Add more tools to your agent\n  * Use different LLMs\n  * Customize the agent\u2019s behavior using system prompts\n  * Add streaming capabilities\n  * Implement human-in-the-loop workflows\n  * Use multiple agents to collaborate on tasks\n\n\nSome helpful next links:\n  * See more advanced agent examples in our [Agent documentation](https://developers.llamaindex.ai/python/framework/understanding/agent)\n  * Learn more about [high-level concepts](https://developers.llamaindex.ai/python/framework/getting_started/concepts)\n  * Explore how to [customize things](https://developers.llamaindex.ai/python/framework/getting_started/faq)\n  * Check out the [component guides](https://developers.llamaindex.ai/python/framework/module_guides)\n\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#_top)\n# Starter Tutorial (Using Local LLMs)\nThis tutorial will show you how to get started building agents with LlamaIndex. We\u2019ll start with a basic example and then show how to add RAG (Retrieval-Augmented Generation) capabilities.\nWe will use [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) as our embedding model and `llama3.1 8B` served through `Ollama`.\n## Setup\n[Section titled \u201cSetup\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#setup)\nOllama is a tool to help you get set up with LLMs locally with minimal setup.\nFollow the [README](https://github.com/jmorganca/ollama) to learn how to install it.\nTo download the Llama3 model just do `ollama pull llama3.1`.\n**NOTE** : You will need a machine with at least ~32GB of RAM.\nAs explained in our [installation guide](https://developers.llamaindex.ai/python/framework/getting_started/installation), `llama-index` is actually a collection of packages. To run Ollama and Huggingface, we will need to install those integrations:\nTerminal window```\n\n\npipinstallllama-index-llms-ollamallama-index-embeddings-huggingface\n\n\n```\n\nThe package names spell out the imports, which is very helpful for remembering how to import them or install them!\n```\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n```\n\nMore integrations are all listed on <https://llamahub.ai>.\n## Basic Agent Example\n[Section titled \u201cBasic Agent Example\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#basic-agent-example)\nLet\u2019s start with a simple example using an agent that can perform basic multiplication by calling a tool. Create a file called `starter.py`:\n```\n\n\nimport asyncio\n\n\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\n\n# Define a simple calculator tool\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n# Create an agent workflow with our calculator tool\n\n\n\nagent =FunctionAgent(\n\n\n\n\ntools=[multiply],\n\n\n\n\nllm=Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\nsystem_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n\n\n\n\n\n\n\nasyncdefmain():\n\n\n\n\n# Run the agent\n\n\n\n\nresponse =await agent.run(\"What is 1234 * 4567?\")\n\n\n\n\nprint(str(response))\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThis will output something like: `The answer to 1234 * 4567 is 5635678.`\nWhat happened is:\n  * The agent was given a question: `What is 1234 * 4567?`\n  * Under the hood, this question, plus the schema of the tools (name, docstring, and arguments) were passed to the LLM\n  * The agent selected the `multiply` tool and wrote the arguments to the tool\n  * The agent received the result from the tool and interpolated it into the final response\n\n\n## Adding Chat History\n[Section titled \u201cAdding Chat History\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-chat-history)\nThe `AgentWorkflow` is also able to remember previous messages. This is contained inside the `Context` of the `AgentWorkflow`.\nIf the `Context` is passed in, the agent will use it to continue the conversation.\n```\n\n\nfrom llama_index.core.workflow import Context\n\n\n\n\n# create context\n\n\n\nctx =Context(agent)\n\n\n\n\n# run agent with context\n\n\n\nresponse =await agent.run(\"My name is Logan\",ctx=ctx)\n\n\n\n\nresponse =await agent.run(\"What is my name?\",ctx=ctx)\n\n\n```\n\n## Adding RAG Capabilities\n[Section titled \u201cAdding RAG Capabilities\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-rag-capabilities)\nNow let\u2019s enhance our agent by adding the ability to search through documents. First, let\u2019s get some example data using our terminal:\nTerminal window```\n\n\nmkdirdata\n\n\n\n\nwgethttps://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt-Odata/paul_graham_essay.txt\n\n\n```\n\nYour directory structure should look like this now:\n```\n\u251c\u2500\u2500 starter.py \u2514\u2500\u2500 data \u00a0\u00a0 \u2514\u2500\u2500 paul_graham_essay.txt\n```\n\nNow we can create a tool for searching through documents using LlamaIndex. By default, our `VectorStoreIndex` will use a `text-embedding-ada-002` embeddings from OpenAI to embed and retrieve the text.\nOur modified `starter.py` should look like this:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n\n\n\n\nfrom llama_index.core.agent.workflow import AgentWorkflow\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\nimport asyncio\n\n\n\n\nimport os\n\n\n\n\n# Settings control global defaults\n\n\n\nSettings.embed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n\n\n\nSettings.llm =Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\n# Create a RAG tool using LlamaIndex\n\n\n\ndocuments =SimpleDirectoryReader(\"data\").load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n\nasyncdefsearch_documents(query: str) -> str:\n\n\n\n\n\"\"\"Useful for answering natural language questions about an personal essay written by Paul Graham.\"\"\"\n\n\n\n\nresponse =await query_engine.aquery(query)\n\n\n\n\nreturnstr(response)\n\n\n\n\n\n# Create an enhanced workflow with both tools\n\n\n\nagent = AgentWorkflow.from_tools_or_functions(\n\n\n\n\n[multiply, search_documents],\n\n\n\n\nllm=Settings.llm,\n\n\n\n\nsystem_prompt=\"\"\"You are a helpful assistant that can perform calculations\n\n\n\n\nand search through documents to answer questions.\"\"\",\n\n\n\n\n\n\n# Now we can ask questions about the documents or do calculations\n\n\n\nasyncdefmain():\n\n\n\n\nresponse =await agent.run(\n\n\n\n\n\"What did the author do in college? Also, what's 7 * 8?\"\n\n\n\n\n\nprint(response)\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThe agent can now seamlessly switch between using the calculator and searching through documents to answer questions.\n## Storing the RAG Index\n[Section titled \u201cStoring the RAG Index\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#storing-the-rag-index)\nTo avoid reprocessing documents every time, you can persist the index to disk:\n```\n\n# Save the index\n\n\n\nindex.storage_context.persist(\"storage\")\n\n\n\n\n# Later, load the index\n\n\n\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n\n\n\n\nindex =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n```\n\n```\n\n\nindex = VectorStoreIndex.from_vector_store(\n\n\n\n\nvector_store,\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n```\n\n## What\u2019s Next?\n[Section titled \u201cWhat\u2019s Next?\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#whats-next)\nThis is just the beginning of what you can do with LlamaIndex agents! You can:\n  * Add more tools to your agent\n  * Use different LLMs\n  * Customize the agent\u2019s behavior using system prompts\n  * Add streaming capabilities\n  * Implement human-in-the-loop workflows\n  * Use multiple agents to collaborate on tasks\n\n\nSome helpful next links:\n  * See more advanced agent examples in our [Agent documentation](https://developers.llamaindex.ai/python/framework/understanding/agent)\n  * Learn more about [high-level concepts](https://developers.llamaindex.ai/python/framework/getting_started/concepts)\n  * Explore how to [customize things](https://developers.llamaindex.ai/python/framework/getting_started/faq)\n  * Check out the [component guides](https://developers.llamaindex.ai/python/framework/module_guides)\n\n\n"}, "__type__": "4"}, "be401141-ec1c-4ee1-9dc4-52fe6940a365": {"__data__": {"id_": "be401141-ec1c-4ee1-9dc4-52fe6940a365", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework_module_guides_loading_connector_usage_pattern.md", "file_name": "python_framework_module_guides_loading_connector_usage_pattern.md", "file_type": "text/markdown", "file_size": 998, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#_top)\n# Usage Pattern\n## Get Started\n[Section titled \u201cGet Started\u201d](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#get-started)\nEach data loader contains a \u201cUsage\u201d section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which downloads the loader file into a module that you can use within your application.\nExample usage:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, download_loader\n\n\n\n\n\nfrom llama_index.readers.google import GoogleDocsReader\n\n\n\n\n\ngdoc_ids =[\"1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec\"]\n\n\n\n\nloader =GoogleDocsReader()\n\n\n\n\ndocuments = loader.load_data(document_ids=gdoc_ids)\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nquery_engine.query(\"Where did the author go to school?\")\n\n\n```\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#_top)\n# Usage Pattern\n## Get Started\n[Section titled \u201cGet Started\u201d](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#get-started)\nEach data loader contains a \u201cUsage\u201d section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which downloads the loader file into a module that you can use within your application.\nExample usage:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, download_loader\n\n\n\n\n\nfrom llama_index.readers.google import GoogleDocsReader\n\n\n\n\n\ngdoc_ids =[\"1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec\"]\n\n\n\n\nloader =GoogleDocsReader()\n\n\n\n\ndocuments = loader.load_data(document_ids=gdoc_ids)\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nquery_engine.query(\"Where did the author go to school?\")\n\n\n```\n\n"}, "__type__": "4"}, "8d82b3d0-f4d8-4d67-a1fe-aff488212800": {"__data__": {"id_": "8d82b3d0-f4d8-4d67-a1fe-aff488212800", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework_understanding_putting_it_all_together_chatbots_building_a_chatbot.md", "file_name": "python_framework_understanding_putting_it_all_together_chatbots_building_a_chatbot.md", "file_type": "text/markdown", "file_size": 15079, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#_top)\n# How to Build a Chatbot\nLlamaIndex serves as a bridge between your data and Large Language Models (LLMs), providing a toolkit that enables you to establish a query interface around your data for a variety of tasks, such as question-answering and summarization.\nIn this tutorial, we\u2019ll walk you through building a context-augmented chatbot using a [Data Agent](https://gpt-index.readthedocs.io/en/stable/core_modules/agent_modules/agents/root.html). This agent, powered by LLMs, is capable of intelligently executing tasks over your data. The end result is a chatbot agent equipped with a robust set of data interface tools provided by LlamaIndex to answer queries about your data.\n**Note** : This tutorial builds upon initial work on creating a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).\n### Context\n[Section titled \u201cContext\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#context)\nIn this guide, we\u2019ll build a \u201c10-K Chatbot\u201d that uses raw UBER 10-K HTML filings from Dropbox. Users can interact with the chatbot to ask questions related to the 10-K filings.\n### Preparation\n[Section titled \u201cPreparation\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#preparation)\n```\n\n\nimport os\n\n\n\n\nimport openai\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] =\"sk-...\"\n\n\n\n\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n\n\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n### Ingest Data\n[Section titled \u201cIngest Data\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#ingest-data)\nLet\u2019s first download the raw 10-k files, from 2019-2022.\n```\n\n# NOTE: the code examples assume you're operating within a Jupyter notebook.\n\n\n# download files\n\n\n!mkdir data\n\n\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n\n\n!unzip data/UBER.zip -d data\n\n```\n\nTo parse the HTML files into formatted text, we use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library. Thanks to [LlamaHub](https://llamahub.ai/), we can directly integrate with Unstructured, allowing conversion of any text into a Document format that LlamaIndex can ingest.\nFirst we install the necessary packages:\n```\n\n!pip install llama-hub unstructured\n\n```\n\nThen we can use the `UnstructuredReader` to parse the HTML files into a list of `Document` objects.\n```\n\n\nfrom llama_index.readers.file import UnstructuredReader\n\n\n\n\nfrom pathlib import Path\n\n\n\n\n\nyears =[2022, 2021, 2020, 2019]\n\n\n\n\n\nloader =UnstructuredReader()\n\n\n\n\ndoc_set = {}\n\n\n\n\nall_docs =[]\n\n\n\n\nfor year in years:\n\n\n\n\nyear_docs = loader.load_data(\n\n\n\n\nfile=Path(f\"./data/UBER/UBER_{year}.html\"),split_documents=False\n\n\n\n\n\n# insert year metadata into each year\n\n\n\n\nforin year_docs:\n\n\n\n\nd.metadata = {\"year\": year}\n\n\n\n\ndoc_set[year] = year_docs\n\n\n\n\nall_docs.extend(year_docs)\n\n\n```\n\n### Setting up Vector Indices for each year\n[Section titled \u201cSetting up Vector Indices for each year\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-vector-indices-for-each-year)\nWe first setup a vector index for each year. Each vector index allows us to ask questions about the 10-K filing of a given year.\nWe build each index and save it to disk.\n```\n\n# initialize simple vector indices\n\n\n\nfrom llama_index.core import VectorStoreIndex, StorageContext\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\nSettings.chunk_size =512\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults()\n\n\n\n\ncur_index = VectorStoreIndex.from_documents(\n\n\n\n\ndoc_set[year],\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n\n\nstorage_context.persist(persist_dir=f\"./storage/{year}\")\n\n\n```\n\nTo load an index from disk, do the following\n```\n\n# Load indices from disk\n\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=f\"./storage/{year}\"\n\n\n\n\n\ncur_index =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n```\n\n### Setting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\n[Section titled \u201cSetting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-a-sub-question-query-engine-to-synthesize-answers-across-10-k-filings)\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.\nTo address this, we can use a [Sub Question Query Engine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/sub_question_query_engine.html). It decomposes a query into subqueries, each answered by an individual vector index, and synthesizes the results to answer the overall query.\nLlamaIndex provides some wrappers around indices (and query engines) so that they can be used by query engines and agents. First we define a `QueryEngineTool` for each vector index. Each tool has a name and a description; these are what the LLM agent sees to decide which tool to choose.\n```\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\n\nindividual_query_engine_tools =[\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=index_set[year].as_query_engine(),\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"vector_index_{year}\",\n\n\n\n\ndescription=f\"useful for when you want to answer queries about the {year} SEC 10-K for Uber\",\n\n\n\n\n\n\nfor year in years\n\n\n\n```\n\nNow we can create the Sub Question Query Engine, which will allow us to synthesize answers across the 10-K filings. We pass in the `individual_query_engine_tools` we defined above, as well as an `llm` that will be used to run the subqueries.\n```\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\n\n\n\n\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n\n\n\n\nquery_engine_tools=individual_query_engine_tools,\n\n\n\n\nllm=OpenAI(model=\"gpt-3.5-turbo\"),\n\n\n\n```\n\n### Setting up the Chatbot Agent\n[Section titled \u201cSetting up the Chatbot Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-agent)\nWe use a LlamaIndex Data Agent to setup the outer chatbot agent, which has access to a set of Tools. Specifically, we will use a `FunctionAgent`, that takes advantage of OpenAI API function calling. We want to use the separate Tools we defined previously for each index (corresponding to a given year), as well as a tool for the sub question query engine we defined above.\nFirst we define a `QueryEngineTool` for the sub question query engine:\n```\n\n\nquery_engine_tool =QueryEngineTool(\n\n\n\n\nquery_engine=query_engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=\"sub_question_query_engine\",\n\n\n\n\ndescription=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber\",\n\n\n\n\n```\n\nThen, we combine the Tools we defined above into a single list of tools for the agent:\n```\n\n\ntools = individual_query_engine_tools +[query_engine_tool]\n\n\n```\n\nFinally, we call `FunctionAgent()` to create the agent, passing in the list of tools we defined above.\n```\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n```\n\n### Testing the Agent\n[Section titled \u201cTesting the Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#testing-the-agent)\nWe can now test the agent with various queries.\nIf we test it with a simple \u201chello\u201d query, the agent does not use any Tools.\n```\n\n\nresponse =await agent.run(\"hi, i am bob\")\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHello Bob! How can I assist you today?\n\n```\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use the relevant vector index Tool.\n```\n\n\nresponse =await agent.run(\n\n\n\n\n\"What were some of the biggest risk factors in 2020 for Uber?\"\n\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nSome of the biggest risk factors for Uber in 2020 were:\n\n\n\n1. The adverse impact of the COVID-19 pandemic and actions taken to mitigate it on the business.\n\n\n2. The potential reclassification of drivers as employees, workers, or quasi-employees instead of independent contractors.\n\n\n3. Intense competition in the mobility, delivery, and logistics industries, with low-cost alternatives and well-capitalized competitors.\n\n\n4. The need to lower fares or service fees and offer driver incentives and consumer discounts to remain competitive.\n\n\n5. Significant losses incurred and the uncertainty of achieving profitability.\n\n\n6. The risk of not attracting or maintaining a critical mass of platform users.\n\n\n7. Operational, compliance, and cultural challenges related to the workplace culture and forward-leaning approach.\n\n\n8. The potential negative impact of international investments and the challenges of conducting business in foreign countries.\n\n\n9. Risks associated with operational and compliance challenges, localization, laws and regulations, competition, social acceptance, technological compatibility, improper business practices, liability uncertainty, managing international operations, currency fluctuations, cash transactions, tax consequences, and payment fraud.\n\n\n\nThese risk factors highlight the challenges and uncertainties that Uber faced in 2020.\n\n```\n\nFinally, if we test it with a query to compare/contrast risk factors across years, the agent will use the Sub Question Query Engine Tool.\n```\n\n\ncross_query_str =\"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n\n\n\n\n\nresponse =await agent.run(cross_query_str)\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHere is a comparison of the risk factors described in the Uber 10-K reports across years:\n\n\n\n2022 Risk Factors:\n\n\n- Potential adverse effect if drivers were classified as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and expectation of increased operating expenses.\n\n\n- Impact of future pandemics or disease outbreaks on the business and financial results.\n\n\n- Potential harm to the business due to economic conditions and their effect on discretionary consumer spending.\n\n\n\n2021 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic and actions to mitigate it on the business.\n\n\n- Potential reclassification of drivers as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees and offer incentives to remain competitive.\n\n\n- History of significant losses and uncertainty of achieving profitability.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n\n2020 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic on the business.\n\n\n- Potential reclassification of drivers as employees.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and potential future expenses.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n- Operational and cultural challenges faced by the company.\n\n\n\n2019 Risk Factors:\n\n\n- Competition with local companies.\n\n\n- Differing levels of social acceptance.\n\n\n- Technological compatibility issues.\n\n\n- Exposure to improper business practices.\n\n\n- Legal uncertainty.\n\n\n- Difficulties in managing international operations.\n\n\n- Fluctuations in currency exchange rates.\n\n\n- Regulations governing local currencies.\n\n\n- Tax consequences.\n\n\n- Financial accounting burdens.\n\n\n- Difficulties in implementing financial systems.\n\n\n- Import and export restrictions.\n\n\n- Political and economic instability.\n\n\n- Public health concerns.\n\n\n- Reduced protection for intellectual property rights.\n\n\n- Limited influence over minority-owned affiliates.\n\n\n- Regulatory complexities.\n\n\n\nThese comparisons highlight both common and unique risk factors that Uber faced in different years.\n\n```\n\n### Setting up the Chatbot Loop\n[Section titled \u201cSetting up the Chatbot Loop\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-loop)\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to chat with our SEC-augmented chatbot!\n```\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n\n\n\nwhileTrue:\n\n\n\n\ntext_input =input(\"User: \")\n\n\n\n\nif text_input ==\"exit\":\n\n\n\n\nbreak\n\n\n\n\nresponse =await agent.run(text_input)\n\n\n\n\nprint(f\"Agent: {response}\")\n\n\n```\n\nHere\u2019s an example of the loop in action:\n```\n\nUser:  What were some of the legal proceedings against Uber in 2022?\n\n\nAgent: In 2022, Uber faced several legal proceedings. Some of the notable ones include:\n\n\n\n1. Petition against Proposition 22: A petition was filed in California alleging that Proposition 22, which classifies app-based drivers as independent contractors, is unconstitutional.\n\n\n\n2. Lawsuit by Massachusetts Attorney General: The Massachusetts Attorney General filed a lawsuit against Uber, claiming that drivers should be classified as employees and entitled to protections under wage and labor laws.\n\n\n\n3. Allegations by New York Attorney General: The New York Attorney General made allegations against Uber regarding the misclassification of drivers and related employment violations.\n\n\n\n4. Swiss social security rulings: Swiss social security rulings classified Uber drivers as employees, which could have implications for Uber's operations in Switzerland.\n\n\n\n5. Class action lawsuits in Australia: Uber faced class action lawsuits in Australia, with allegations that the company conspired to harm participants in the taxi, hire-car, and limousine industries.\n\n\n\nIt's important to note that the outcomes of these legal proceedings are uncertain and may vary.\n\n\n\nUser:\n\n```\n\n### Notebook\n[Section titled \u201cNotebook\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#notebook)\nTake a look at our [corresponding notebook](https://developers.llamaindex.ai/python/examples/agent/chatbot_sec).\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#_top)\n# How to Build a Chatbot\nLlamaIndex serves as a bridge between your data and Large Language Models (LLMs), providing a toolkit that enables you to establish a query interface around your data for a variety of tasks, such as question-answering and summarization.\nIn this tutorial, we\u2019ll walk you through building a context-augmented chatbot using a [Data Agent](https://gpt-index.readthedocs.io/en/stable/core_modules/agent_modules/agents/root.html). This agent, powered by LLMs, is capable of intelligently executing tasks over your data. The end result is a chatbot agent equipped with a robust set of data interface tools provided by LlamaIndex to answer queries about your data.\n**Note** : This tutorial builds upon initial work on creating a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).\n### Context\n[Section titled \u201cContext\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#context)\nIn this guide, we\u2019ll build a \u201c10-K Chatbot\u201d that uses raw UBER 10-K HTML filings from Dropbox. Users can interact with the chatbot to ask questions related to the 10-K filings.\n### Preparation\n[Section titled \u201cPreparation\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#preparation)\n```\n\n\nimport os\n\n\n\n\nimport openai\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] =\"sk-...\"\n\n\n\n\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n\n\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n### Ingest Data\n[Section titled \u201cIngest Data\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#ingest-data)\nLet\u2019s first download the raw 10-k files, from 2019-2022.\n```\n\n# NOTE: the code examples assume you're operating within a Jupyter notebook.\n\n\n# download files\n\n\n!mkdir data\n\n\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n\n\n!unzip data/UBER.zip -d data\n\n```\n\nTo parse the HTML files into formatted text, we use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library. Thanks to [LlamaHub](https://llamahub.ai/), we can directly integrate with Unstructured, allowing conversion of any text into a Document format that LlamaIndex can ingest.\nFirst we install the necessary packages:\n```\n\n!pip install llama-hub unstructured\n\n```\n\nThen we can use the `UnstructuredReader` to parse the HTML files into a list of `Document` objects.\n```\n\n\nfrom llama_index.readers.file import UnstructuredReader\n\n\n\n\nfrom pathlib import Path\n\n\n\n\n\nyears =[2022, 2021, 2020, 2019]\n\n\n\n\n\nloader =UnstructuredReader()\n\n\n\n\ndoc_set = {}\n\n\n\n\nall_docs =[]\n\n\n\n\nfor year in years:\n\n\n\n\nyear_docs = loader.load_data(\n\n\n\n\nfile=Path(f\"./data/UBER/UBER_{year}.html\"),split_documents=False\n\n\n\n\n\n# insert year metadata into each year\n\n\n\n\nforin year_docs:\n\n\n\n\nd.metadata = {\"year\": year}\n\n\n\n\ndoc_set[year] = year_docs\n\n\n\n\nall_docs.extend(year_docs)\n\n\n```\n\n### Setting up Vector Indices for each year\n[Section titled \u201cSetting up Vector Indices for each year\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-vector-indices-for-each-year)\nWe first setup a vector index for each year. Each vector index allows us to ask questions about the 10-K filing of a given year.\nWe build each index and save it to disk.\n```\n\n# initialize simple vector indices\n\n\n\nfrom llama_index.core import VectorStoreIndex, StorageContext\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\nSettings.chunk_size =512\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults()\n\n\n\n\ncur_index = VectorStoreIndex.from_documents(\n\n\n\n\ndoc_set[year],\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n\n\nstorage_context.persist(persist_dir=f\"./storage/{year}\")\n\n\n```\n\nTo load an index from disk, do the following\n```\n\n# Load indices from disk\n\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=f\"./storage/{year}\"\n\n\n\n\n\ncur_index =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n```\n\n### Setting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\n[Section titled \u201cSetting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-a-sub-question-query-engine-to-synthesize-answers-across-10-k-filings)\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.\nTo address this, we can use a [Sub Question Query Engine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/sub_question_query_engine.html). It decomposes a query into subqueries, each answered by an individual vector index, and synthesizes the results to answer the overall query.\nLlamaIndex provides some wrappers around indices (and query engines) so that they can be used by query engines and agents. First we define a `QueryEngineTool` for each vector index. Each tool has a name and a description; these are what the LLM agent sees to decide which tool to choose.\n```\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\n\nindividual_query_engine_tools =[\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=index_set[year].as_query_engine(),\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"vector_index_{year}\",\n\n\n\n\ndescription=f\"useful for when you want to answer queries about the {year} SEC 10-K for Uber\",\n\n\n\n\n\n\nfor year in years\n\n\n\n```\n\nNow we can create the Sub Question Query Engine, which will allow us to synthesize answers across the 10-K filings. We pass in the `individual_query_engine_tools` we defined above, as well as an `llm` that will be used to run the subqueries.\n```\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\n\n\n\n\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n\n\n\n\nquery_engine_tools=individual_query_engine_tools,\n\n\n\n\nllm=OpenAI(model=\"gpt-3.5-turbo\"),\n\n\n\n```\n\n### Setting up the Chatbot Agent\n[Section titled \u201cSetting up the Chatbot Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-agent)\nWe use a LlamaIndex Data Agent to setup the outer chatbot agent, which has access to a set of Tools. Specifically, we will use a `FunctionAgent`, that takes advantage of OpenAI API function calling. We want to use the separate Tools we defined previously for each index (corresponding to a given year), as well as a tool for the sub question query engine we defined above.\nFirst we define a `QueryEngineTool` for the sub question query engine:\n```\n\n\nquery_engine_tool =QueryEngineTool(\n\n\n\n\nquery_engine=query_engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=\"sub_question_query_engine\",\n\n\n\n\ndescription=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber\",\n\n\n\n\n```\n\nThen, we combine the Tools we defined above into a single list of tools for the agent:\n```\n\n\ntools = individual_query_engine_tools +[query_engine_tool]\n\n\n```\n\nFinally, we call `FunctionAgent()` to create the agent, passing in the list of tools we defined above.\n```\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n```\n\n### Testing the Agent\n[Section titled \u201cTesting the Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#testing-the-agent)\nWe can now test the agent with various queries.\nIf we test it with a simple \u201chello\u201d query, the agent does not use any Tools.\n```\n\n\nresponse =await agent.run(\"hi, i am bob\")\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHello Bob! How can I assist you today?\n\n```\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use the relevant vector index Tool.\n```\n\n\nresponse =await agent.run(\n\n\n\n\n\"What were some of the biggest risk factors in 2020 for Uber?\"\n\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nSome of the biggest risk factors for Uber in 2020 were:\n\n\n\n1. The adverse impact of the COVID-19 pandemic and actions taken to mitigate it on the business.\n\n\n2. The potential reclassification of drivers as employees, workers, or quasi-employees instead of independent contractors.\n\n\n3. Intense competition in the mobility, delivery, and logistics industries, with low-cost alternatives and well-capitalized competitors.\n\n\n4. The need to lower fares or service fees and offer driver incentives and consumer discounts to remain competitive.\n\n\n5. Significant losses incurred and the uncertainty of achieving profitability.\n\n\n6. The risk of not attracting or maintaining a critical mass of platform users.\n\n\n7. Operational, compliance, and cultural challenges related to the workplace culture and forward-leaning approach.\n\n\n8. The potential negative impact of international investments and the challenges of conducting business in foreign countries.\n\n\n9. Risks associated with operational and compliance challenges, localization, laws and regulations, competition, social acceptance, technological compatibility, improper business practices, liability uncertainty, managing international operations, currency fluctuations, cash transactions, tax consequences, and payment fraud.\n\n\n\nThese risk factors highlight the challenges and uncertainties that Uber faced in 2020.\n\n```\n\nFinally, if we test it with a query to compare/contrast risk factors across years, the agent will use the Sub Question Query Engine Tool.\n```\n\n\ncross_query_str =\"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n\n\n\n\n\nresponse =await agent.run(cross_query_str)\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHere is a comparison of the risk factors described in the Uber 10-K reports across years:\n\n\n\n2022 Risk Factors:\n\n\n- Potential adverse effect if drivers were classified as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and expectation of increased operating expenses.\n\n\n- Impact of future pandemics or disease outbreaks on the business and financial results.\n\n\n- Potential harm to the business due to economic conditions and their effect on discretionary consumer spending.\n\n\n\n2021 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic and actions to mitigate it on the business.\n\n\n- Potential reclassification of drivers as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees and offer incentives to remain competitive.\n\n\n- History of significant losses and uncertainty of achieving profitability.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n\n2020 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic on the business.\n\n\n- Potential reclassification of drivers as employees.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and potential future expenses.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n- Operational and cultural challenges faced by the company.\n\n\n\n2019 Risk Factors:\n\n\n- Competition with local companies.\n\n\n- Differing levels of social acceptance.\n\n\n- Technological compatibility issues.\n\n\n- Exposure to improper business practices.\n\n\n- Legal uncertainty.\n\n\n- Difficulties in managing international operations.\n\n\n- Fluctuations in currency exchange rates.\n\n\n- Regulations governing local currencies.\n\n\n- Tax consequences.\n\n\n- Financial accounting burdens.\n\n\n- Difficulties in implementing financial systems.\n\n\n- Import and export restrictions.\n\n\n- Political and economic instability.\n\n\n- Public health concerns.\n\n\n- Reduced protection for intellectual property rights.\n\n\n- Limited influence over minority-owned affiliates.\n\n\n- Regulatory complexities.\n\n\n\nThese comparisons highlight both common and unique risk factors that Uber faced in different years.\n\n```\n\n### Setting up the Chatbot Loop\n[Section titled \u201cSetting up the Chatbot Loop\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-loop)\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to chat with our SEC-augmented chatbot!\n```\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n\n\n\nwhileTrue:\n\n\n\n\ntext_input =input(\"User: \")\n\n\n\n\nif text_input ==\"exit\":\n\n\n\n\nbreak\n\n\n\n\nresponse =await agent.run(text_input)\n\n\n\n\nprint(f\"Agent: {response}\")\n\n\n```\n\nHere\u2019s an example of the loop in action:\n```\n\nUser:  What were some of the legal proceedings against Uber in 2022?\n\n\nAgent: In 2022, Uber faced several legal proceedings. Some of the notable ones include:\n\n\n\n1. Petition against Proposition 22: A petition was filed in California alleging that Proposition 22, which classifies app-based drivers as independent contractors, is unconstitutional.\n\n\n\n2. Lawsuit by Massachusetts Attorney General: The Massachusetts Attorney General filed a lawsuit against Uber, claiming that drivers should be classified as employees and entitled to protections under wage and labor laws.\n\n\n\n3. Allegations by New York Attorney General: The New York Attorney General made allegations against Uber regarding the misclassification of drivers and related employment violations.\n\n\n\n4. Swiss social security rulings: Swiss social security rulings classified Uber drivers as employees, which could have implications for Uber's operations in Switzerland.\n\n\n\n5. Class action lawsuits in Australia: Uber faced class action lawsuits in Australia, with allegations that the company conspired to harm participants in the taxi, hire-car, and limousine industries.\n\n\n\nIt's important to note that the outcomes of these legal proceedings are uncertain and may vary.\n\n\n\nUser:\n\n```\n\n### Notebook\n[Section titled \u201cNotebook\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#notebook)\nTake a look at our [corresponding notebook](https://developers.llamaindex.ai/python/examples/agent/chatbot_sec).\n"}, "__type__": "4"}, "dbcf148c-f2cc-418a-96ed-ad469eb99b95": {"__data__": {"id_": "dbcf148c-f2cc-418a-96ed-ad469eb99b95", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_cloud_llamacloud_integrations_data_sinks_pinecone.md", "file_name": "python_cloud_llamacloud_integrations_data_sinks_pinecone.md", "file_type": "text/markdown", "file_size": 2076, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#_top)\n# Pinecone\nConfigure your own Pinecone instance as data sink.\n## Configure via UI\n[Section titled \u201cConfigure via UI\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-ui)\n## Configure via API / Client\n[Section titled \u201cConfigure via API / Client\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-api--client)\n  * [ TypeScript Client ](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#tab-panel-84)\n\n\n```\n\n\nfrom llama_cloud.types import CloudPineconeVectorStore\n\n\n\n\n\nds = {\n\n\n\n\n'name': '<your-name>',\n\n\n\n\n'sink_type': 'PINECONE',\n\n\n\n\n'component': CloudPineconeVectorStore(\n\n\n\n\napi_key='<api_key>',\n\n\n\n\nindex_name='<index_name>',\n\n\n\n\nname_space='<name_space>',# optional\n\n\n\n\ninsert_kwargs='<insert_kwargs>'# optional\n\n\n\n\n\n\ndata_sink = client.data_sinks.create_data_sink(request=ds)\n\n\n```\n\n```\n\n\nconst ds = {\n\n\n\n\n'name': 'pinecone',\n\n\n\n\n'sinkType': 'PINECONE',\n\n\n\n\n'component': {\n\n\n\n\n'api_key': '<api_key>',\n\n\n\n\n'index_name': '<index_name>',\n\n\n\n\n'name_space': '<name_space>'// optional\n\n\n\n\n'insert_kwargs': '<insert_kwargs>'// optional\n\n\n\n\n\n\n\ndata_sink=awaitclient.dataSinks.createDataSink({\n\n\n\n\nprojectId: projectId,\n\n\n\n\nbody: ds\n\n\n\n```\n\n## Filter Syntax\n[Section titled \u201cFilter Syntax\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#filter-syntax)\nWhen using Pinecone as a data sink, you can apply filters using the following syntax:\nFilter Operator | Pinecone Equivalent | Description  \n---|---|---  \n`$eq` | Equals  \n`$ne` | Not equal  \n`$gt` | Greater than  \n`$lt` | Less than  \n`$gte` | Greater than or equal  \n`$lte` | Less than or equal  \n`$in` | Value is in a list  \n`nin` | `$nin` | Value is not in a list  \nThese filters can be applied to metadata fields when querying your Pinecone index to refine search results based on specific criteria.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#_top)\n# Pinecone\nConfigure your own Pinecone instance as data sink.\n## Configure via UI\n[Section titled \u201cConfigure via UI\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-ui)\n## Configure via API / Client\n[Section titled \u201cConfigure via API / Client\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#configure-via-api--client)\n  * [ TypeScript Client ](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#tab-panel-84)\n\n\n```\n\n\nfrom llama_cloud.types import CloudPineconeVectorStore\n\n\n\n\n\nds = {\n\n\n\n\n'name': '<your-name>',\n\n\n\n\n'sink_type': 'PINECONE',\n\n\n\n\n'component': CloudPineconeVectorStore(\n\n\n\n\napi_key='<api_key>',\n\n\n\n\nindex_name='<index_name>',\n\n\n\n\nname_space='<name_space>',# optional\n\n\n\n\ninsert_kwargs='<insert_kwargs>'# optional\n\n\n\n\n\n\ndata_sink = client.data_sinks.create_data_sink(request=ds)\n\n\n```\n\n```\n\n\nconst ds = {\n\n\n\n\n'name': 'pinecone',\n\n\n\n\n'sinkType': 'PINECONE',\n\n\n\n\n'component': {\n\n\n\n\n'api_key': '<api_key>',\n\n\n\n\n'index_name': '<index_name>',\n\n\n\n\n'name_space': '<name_space>'// optional\n\n\n\n\n'insert_kwargs': '<insert_kwargs>'// optional\n\n\n\n\n\n\n\ndata_sink=awaitclient.dataSinks.createDataSink({\n\n\n\n\nprojectId: projectId,\n\n\n\n\nbody: ds\n\n\n\n```\n\n## Filter Syntax\n[Section titled \u201cFilter Syntax\u201d](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/pinecone/#filter-syntax)\nWhen using Pinecone as a data sink, you can apply filters using the following syntax:\nFilter Operator | Pinecone Equivalent | Description  \n---|---|---  \n`$eq` | Equals  \n`$ne` | Not equal  \n`$gt` | Greater than  \n`$lt` | Less than  \n`$gte` | Greater than or equal  \n`$lte` | Less than or equal  \n`$in` | Value is in a list  \n`nin` | `$nin` | Value is not in a list  \nThese filters can be applied to metadata fields when querying your Pinecone index to refine search results based on specific criteria.\n"}, "__type__": "4"}, "e5a55779-a21a-4bd4-b054-f87d55317877": {"__data__": {"id_": "e5a55779-a21a-4bd4-b054-f87d55317877", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_cloud_self_hosting_configuration_file-storage.md", "file_name": "python_cloud_self_hosting_configuration_file-storage.md", "file_type": "text/markdown", "file_size": 6251, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#_top)\n# File Storage\n##  Self-Hosting Documentation Access \nThis section requires a password to access. Interested in self-hosting? [Contact sales](https://www.llamaindex.ai/contact) to learn more. \nSelf-Hosting Documentation Access Granted  Logout \nFile storage is an integral part of LlamaCloud. Without it, many key features would not be possible. This page walks through how to configure file storage for your deployment \u2014 which buckets you need to create and for non-AWS deployments, how to configure the S3 Proxy to interact with them.\n## Requirements\n[Section titled \u201cRequirements\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#requirements)\n  * A valid blob storage service. We recommend the following: \n    * [Google Cloud Storage](https://cloud.google.com/storage)\n  * Because LlamaCloud heavily relies on file storage, you will need to create the following buckets: \n    * `llama-platform-parsed-documents`\n    * `llama-platform-etl`\n    * `llama-platform-external-components`\n    * `llama-platform-file-parsing`\n    * `llama-platform-raw-files`\n    * `llama-cloud-parse-output`\n    * `llama-platform-file-screenshots`\n    * `llama-platform-extract-output` (for `LlamaExtract`)\n\n\n## Connecting to AWS S3\n[Section titled \u201cConnecting to AWS S3\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-aws-s3)\nBelow are two ways to configure a connection to AWS S3:\n### (Recommended) IAM Role for Service Accounts\n[Section titled \u201c(Recommended) IAM Role for Service Accounts\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#recommended-iam-role-for-service-accounts)\nWe recommend that users create a new IAM Role and Policy for LlamaCloud. You can then attach the role ARN as a service account annotation.\n```\n\n// Example IAM Policy\n\n\n\n\n\"Version\": \"2012-10-17\",\n\n\n\n\n\"Statement\": [\n\n\n\n\n\n\"Effect\": \"Allow\",\n\n\n\n\n\"Action\": [\"s3:*\"], // this is not secure\n\n\n\n\n\"Resource\": [\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents\",\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents/*\",\n\n\n\n\n\n\n\n```\n\nAfter creating something similar to the above policy, update the `backend`, `jobsService`, `jobsWorker`, and `llamaParse` service accounts with the EKS annotation.\n```\n\n# Example for the backend service account. Repeat for each of the services listed above.\n\n\n\nbackend:\n\n\n\n\nserviceAccountAnnotations:\n\n\n\n\neks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/<role-name>\n\n\n```\n\nFor more information, feel free to refer to the [official AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) about this topic.\n### AWS Credentials\n[Section titled \u201cAWS Credentials\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#aws-credentials)\nCreate a user with a policy attached for the aforementioned s3 buckets. Afterwards, you can configure the platform to use the aws credentials of that user by setting the following values in your `values.yaml` file:\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"aws\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nJCLOUDS_PROVIDER: \"aws-s3\"\n\n\n\n\nJCLOUDS_IDENTITY: <AWS-ACCESS-KEY>\n\n\n\n\nJCLOUDS_CREDENTIAL: <AWS-SECRET-KEY>\n\n\n\n\nJCLOUDS_REGION: <AWS-REGION># e.g. \"us-east-1\"\n\n\n\n\nJCLOUDS_ENDPOINT: \"https://s3.<AWS-REGION>.amazonaws.com\"\n\n\n```\n\n## Overriding Default Bucket Names\n[Section titled \u201cOverriding Default Bucket Names\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#overriding-default-bucket-names)\nWe allow users to override the default bucket names in the `values.yaml` file.\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nparsedDocuments: \"<your-bucket-name>\"\n\n\n\n\nparsedEtl: \"<your-bucket-name>\"\n\n\n\n\nparsedExternalComponents: \"<your-bucket-name>\"\n\n\n\n\nparsedFileParsing: \"<your-bucket-name>\"\n\n\n\n\nparsedRawFile: \"<your-bucket-name>\"\n\n\n\n\nparseOutput: \"<your-bucket-name>\"\n\n\n\n\nparsedFileScreenshot: \"<your-bucket-name>\"\n\n\n\n\nextractOutput: \"<your-bucket-name>\"\n\n\n\n\nparseFileUpload: \"<your-bucket-name>\"\n\n\n\n\nparseFileOutput: \"<your-bucket-name>\"\n\n\n```\n\n## Connecting to Azure Blob Storage or Other Providers with S3Proxy\n[Section titled \u201cConnecting to Azure Blob Storage or Other Providers with S3Proxy\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-azure-blob-storage-or-other-providers-with-s3proxy)\nLlamaCloud was first developed on AWS, which means that we started by natively supporting S3. However, to make a self-hosted solution possible, we need a way for the platform to interact with other providers.\nWe leverage the open-source project [S3Proxy](https://github.com/gaul/s3proxy) to translate the S3 API requests into requests to other storage providers. A containerized deployment of S3Proxy is supported out of the box in our helm charts.\nS3Proxy should always be set to `enabled: true`, even when deploying LlamaCloud on AWS. This causes S3Proxy to be deployed as a sidecar on several of the LlamaCloud pods.\nThe following is an example for how to connect your LlamaCloud deployment to Azure Blob Storage. For more examples of connecting to different providers, please refer to the project\u2019s [Examples](https://github.com/gaul/s3proxy/wiki/Storage-backend-examples) page.\n  * [ Azure Blob Storage with S3 Proxy ](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#tab-panel-56)\n\n\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"azure\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nS3PROXY_ENDPOINT: \"http://0.0.0.0:80\"\n\n\n\n\nS3PROXY_AUTHORIZATION: \"none\"\n\n\n\n\nS3PROXY_IGNORE_UNKNOWN_HEADERS: \"true\"\n\n\n\n\nS3PROXY_CORS_ALLOW_ORIGINS: \"*\"\n\n\n\n\nJCLOUDS_PROVIDER: \"azureblob\"\n\n\n\n\nJCLOUDS_REGION: \"eastus\"# Change to your region\n\n\n\n\nJCLOUDS_AZUREBLOB_AUTH: \"azureKey\"\n\n\n\n\nJCLOUDS_IDENTITY: \"fill-out\"# Change to your storage account name\n\n\n\n\nJCLOUDS_CREDENTIAL: \"fill-out\"# Change to your storage account key\n\n\n\n\nJCLOUDS_ENDPOINT: \"fill-out\"# Change to your storage account endpoint\n\n\n```\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#_top)\n# File Storage\n##  Self-Hosting Documentation Access \nThis section requires a password to access. Interested in self-hosting? [Contact sales](https://www.llamaindex.ai/contact) to learn more. \nSelf-Hosting Documentation Access Granted  Logout \nFile storage is an integral part of LlamaCloud. Without it, many key features would not be possible. This page walks through how to configure file storage for your deployment \u2014 which buckets you need to create and for non-AWS deployments, how to configure the S3 Proxy to interact with them.\n## Requirements\n[Section titled \u201cRequirements\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#requirements)\n  * A valid blob storage service. We recommend the following: \n    * [Google Cloud Storage](https://cloud.google.com/storage)\n  * Because LlamaCloud heavily relies on file storage, you will need to create the following buckets: \n    * `llama-platform-parsed-documents`\n    * `llama-platform-etl`\n    * `llama-platform-external-components`\n    * `llama-platform-file-parsing`\n    * `llama-platform-raw-files`\n    * `llama-cloud-parse-output`\n    * `llama-platform-file-screenshots`\n    * `llama-platform-extract-output` (for `LlamaExtract`)\n\n\n## Connecting to AWS S3\n[Section titled \u201cConnecting to AWS S3\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-aws-s3)\nBelow are two ways to configure a connection to AWS S3:\n### (Recommended) IAM Role for Service Accounts\n[Section titled \u201c(Recommended) IAM Role for Service Accounts\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#recommended-iam-role-for-service-accounts)\nWe recommend that users create a new IAM Role and Policy for LlamaCloud. You can then attach the role ARN as a service account annotation.\n```\n\n// Example IAM Policy\n\n\n\n\n\"Version\": \"2012-10-17\",\n\n\n\n\n\"Statement\": [\n\n\n\n\n\n\"Effect\": \"Allow\",\n\n\n\n\n\"Action\": [\"s3:*\"], // this is not secure\n\n\n\n\n\"Resource\": [\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents\",\n\n\n\n\n\"arn:aws:s3:::llama-platform-parsed-documents/*\",\n\n\n\n\n\n\n\n```\n\nAfter creating something similar to the above policy, update the `backend`, `jobsService`, `jobsWorker`, and `llamaParse` service accounts with the EKS annotation.\n```\n\n# Example for the backend service account. Repeat for each of the services listed above.\n\n\n\nbackend:\n\n\n\n\nserviceAccountAnnotations:\n\n\n\n\neks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/<role-name>\n\n\n```\n\nFor more information, feel free to refer to the [official AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) about this topic.\n### AWS Credentials\n[Section titled \u201cAWS Credentials\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#aws-credentials)\nCreate a user with a policy attached for the aforementioned s3 buckets. Afterwards, you can configure the platform to use the aws credentials of that user by setting the following values in your `values.yaml` file:\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"aws\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nJCLOUDS_PROVIDER: \"aws-s3\"\n\n\n\n\nJCLOUDS_IDENTITY: <AWS-ACCESS-KEY>\n\n\n\n\nJCLOUDS_CREDENTIAL: <AWS-SECRET-KEY>\n\n\n\n\nJCLOUDS_REGION: <AWS-REGION># e.g. \"us-east-1\"\n\n\n\n\nJCLOUDS_ENDPOINT: \"https://s3.<AWS-REGION>.amazonaws.com\"\n\n\n```\n\n## Overriding Default Bucket Names\n[Section titled \u201cOverriding Default Bucket Names\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#overriding-default-bucket-names)\nWe allow users to override the default bucket names in the `values.yaml` file.\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nparsedDocuments: \"<your-bucket-name>\"\n\n\n\n\nparsedEtl: \"<your-bucket-name>\"\n\n\n\n\nparsedExternalComponents: \"<your-bucket-name>\"\n\n\n\n\nparsedFileParsing: \"<your-bucket-name>\"\n\n\n\n\nparsedRawFile: \"<your-bucket-name>\"\n\n\n\n\nparseOutput: \"<your-bucket-name>\"\n\n\n\n\nparsedFileScreenshot: \"<your-bucket-name>\"\n\n\n\n\nextractOutput: \"<your-bucket-name>\"\n\n\n\n\nparseFileUpload: \"<your-bucket-name>\"\n\n\n\n\nparseFileOutput: \"<your-bucket-name>\"\n\n\n```\n\n## Connecting to Azure Blob Storage or Other Providers with S3Proxy\n[Section titled \u201cConnecting to Azure Blob Storage or Other Providers with S3Proxy\u201d](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#connecting-to-azure-blob-storage-or-other-providers-with-s3proxy)\nLlamaCloud was first developed on AWS, which means that we started by natively supporting S3. However, to make a self-hosted solution possible, we need a way for the platform to interact with other providers.\nWe leverage the open-source project [S3Proxy](https://github.com/gaul/s3proxy) to translate the S3 API requests into requests to other storage providers. A containerized deployment of S3Proxy is supported out of the box in our helm charts.\nS3Proxy should always be set to `enabled: true`, even when deploying LlamaCloud on AWS. This causes S3Proxy to be deployed as a sidecar on several of the LlamaCloud pods.\nThe following is an example for how to connect your LlamaCloud deployment to Azure Blob Storage. For more examples of connecting to different providers, please refer to the project\u2019s [Examples](https://github.com/gaul/s3proxy/wiki/Storage-backend-examples) page.\n  * [ Azure Blob Storage with S3 Proxy ](https://developers.llamaindex.ai/python/cloud/self_hosting/configuration/file-storage/#tab-panel-56)\n\n\n```\n\n\nconfig:\n\n\n\n\nstorageBuckets:\n\n\n\n\nprovider: \"azure\"\n\n\n\n\ns3proxy:\n\n\n\n\nenabled: true\n\n\n\n\ncontainerPort: 8080\n\n\n\n\nconfig:\n\n\n\n\nS3PROXY_ENDPOINT: \"http://0.0.0.0:80\"\n\n\n\n\nS3PROXY_AUTHORIZATION: \"none\"\n\n\n\n\nS3PROXY_IGNORE_UNKNOWN_HEADERS: \"true\"\n\n\n\n\nS3PROXY_CORS_ALLOW_ORIGINS: \"*\"\n\n\n\n\nJCLOUDS_PROVIDER: \"azureblob\"\n\n\n\n\nJCLOUDS_REGION: \"eastus\"# Change to your region\n\n\n\n\nJCLOUDS_AZUREBLOB_AUTH: \"azureKey\"\n\n\n\n\nJCLOUDS_IDENTITY: \"fill-out\"# Change to your storage account name\n\n\n\n\nJCLOUDS_CREDENTIAL: \"fill-out\"# Change to your storage account key\n\n\n\n\nJCLOUDS_ENDPOINT: \"fill-out\"# Change to your storage account endpoint\n\n\n```\n\n"}, "__type__": "4"}, "236f1170-1385-476f-b976-06c9d21bf00d": {"__data__": {"id_": "236f1170-1385-476f-b976-06c9d21bf00d", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_docstore_tablestoredocstoredemo.md", "file_name": "python_examples_docstore_tablestoredocstoredemo.md", "file_type": "text/markdown", "file_size": 9752, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#_top)\n# Tablestore Demo \nThis guide shows you how to directly use our `DocumentStore` abstraction backed by Tablestore. By putting nodes in the docstore, this allows you to define multiple indices over the same underlying docstore, instead of duplicating data across indices.\n```\n\n\n%pip install llama-index-storage-docstore-tablestore\n\n\n\n\n%pip install llama-index-storage-index-store-tablestore\n\n\n\n\n%pip install llama-index-vector-stores-tablestore\n\n\n\n\n\n%pip install llama-index-llms-dashscope\n\n\n\n\n%pip install llama-index-embeddings-dashscope\n\n\n\n\n\n%pip install llama-index\n\n\n\n\n%pip install matplotlib\n\n\n```\n\n```\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n```\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n```\n\n\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\n\n\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex\n\n\n\n\nfrom llama_index.core import SummaryIndex\n\n\n\n\nfrom llama_index.core.response.notebook_utils import display_response\n\n\n\n\nfrom llama_index.core import Settings\n\n\n```\n\n#### Config Tablestore\n[Section titled \u201cConfig Tablestore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-tablestore)\nNext, we use tablestore\u2019s docsstore to perform a demo.\n```\n\n\nimport getpass\n\n\n\n\nimport os\n\n\n\n\n\nos.environ[\"tablestore_end_point\"] = getpass.getpass(\"tablestore end_point:\")\n\n\n\n\nos.environ[\"tablestore_instance_name\"] = getpass.getpass(\n\n\n\n\n\"tablestore instance_name:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_id\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_id:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_secret\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_secret:\"\n\n\n\n```\n\n#### Config DashScope LLM\n[Section titled \u201cConfig DashScope LLM\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-dashscope-llm)\nNext, we use dashscope\u2019s llm to perform a demo.\n```\n\n\nimport os\n\n\n\n\nimport getpass\n\n\n\n\n\nos.environ[\"DASHSCOPE_API_KEY\"] = getpass.getpass(\"DashScope api key:\")\n\n\n```\n\n#### Download Data\n[Section titled \u201cDownload Data\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#download-data)\n```\n\n\n!mkdir -p 'data/paul_graham/'\n\n\n\n\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt'-O 'data/paul_graham/paul_graham_essay.txt'\n\n\n```\n\n#### Load Documents\n[Section titled \u201cLoad Documents\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#load-documents)\n```\n\n\nreader =SimpleDirectoryReader(\"./data/paul_graham/\")\n\n\n\n\ndocuments = reader.load_data()\n\n\n```\n\n#### Parse into Nodes\n[Section titled \u201cParse into Nodes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#parse-into-nodes)\n```\n\n\nfrom llama_index.core.node_parser import SentenceSplitter\n\n\n\n\n\nnodes =SentenceSplitter().get_nodes_from_documents(documents)\n\n\n```\n\n#### Init Store/Embedding/LLM/StorageContext\n[Section titled \u201cInit Store/Embedding/LLM/StorageContext\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#init-storeembeddingllmstoragecontext)\n```\n\n\nfrom llama_index.storage.docstore.tablestore import TablestoreDocumentStore\n\n\n\n\nfrom llama_index.storage.index_store.tablestore import TablestoreIndexStore\n\n\n\n\nfrom llama_index.vector_stores.tablestore import TablestoreVectorStore\n\n\n\n\nfrom llama_index.embeddings.dashscope import (\n\n\n\n\nDashScopeEmbedding,\n\n\n\n\nDashScopeTextEmbeddingModels,\n\n\n\n\nDashScopeTextEmbeddingType,\n\n\n\n\n\nfrom llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n\n\n\n\n\nembedder =DashScopeEmbedding(\n\n\n\n\nmodel_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,# default demiension is 1024\n\n\n\n\ntext_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,\n\n\n\n\n\n\ndashscope_llm =DashScope(\n\n\n\n\nmodel_name=DashScopeGenerationModels.QWEN_MAX,\n\n\n\n\napi_key=os.environ[\"DASHSCOPE_API_KEY\"],\n\n\n\n\n\nSettings.llm = dashscope_llm\n\n\n\n\n\ndocstore = TablestoreDocumentStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nindex_store = TablestoreIndexStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nvector_store =TablestoreVectorStore(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\nvector_dimension=1024,# embedder dimension is 1024\n\n\n\n\n\nvector_store.create_table_if_not_exist()\n\n\n\n\nvector_store.create_search_index_if_not_exist()\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n```\n\n#### Add to docStore\n[Section titled \u201cAdd to docStore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#add-to-docstore)\n```\n\n\nstorage_context.docstore.add_documents(nodes)\n\n\n```\n\n#### Define & Add Multiple Indexes\n[Section titled \u201cDefine & Add Multiple Indexes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#define--add-multiple-indexes)\nEach index uses the same underlying Node.\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/list.html\n\n\n\nsummary_index =SummaryIndex(nodes,storage_context=storage_context)\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/vector_store.html\n\n\n\nvector_index =VectorStoreIndex(\n\n\n\n\nnodes,\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/table.html\n\n\n\nkeyword_table_index =SimpleKeywordTableIndex(\n\n\n\n\nnodes=nodes,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nllm=dashscope_llm,\n\n\n\n```\n\n```\n\n\n# NOTE: the docstore still has the same nodes\n\n\n\n\nlen(storage_context.docstore.docs)\n\n\n```\n\n#### Test out saving and loading\n[Section titled \u201cTest out saving and loading\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-saving-and-loading)\n```\n\n\n# NOTE: docstore and index_store is persisted in Tablestore by default\n\n\n\n\n# NOTE: here only need to persist simple vector store to disk\n\n\n\n\nstorage_context.persist()\n\n\n```\n\n```\n\n# note down index IDs\n\n\n\nlist_id = summary_index.index_id\n\n\n\n\nvector_id = vector_index.index_id\n\n\n\n\nkeyword_id = keyword_table_index.index_id\n\n\n\n\nprint(list_id, vector_id, keyword_id)\n\n\n```\n\n```\n\nc05fec2a-ac87-4761-beeb-0901f9e6530e d0b021ed-3427-46ad-927d-12d72752dbc4 2e9bfc3a-5e69-408a-9430-7b0c8baf3d77\n\n```\n\n```\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n# re-create storage context\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n\n\n\nsummary_index =load_index_from_storage(\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=list_id,\n\n\n\n\n\nkeyword_table_index =load_index_from_storage(\n\n\n\n\nllm=dashscope_llm,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=keyword_id,\n\n\n\n\n# You need to add \"vector_store=xxx\" to StorageContext to load vector index from Tablestore\n\n\n\nvector_index =load_index_from_storage(\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=vector_id,\n\n\n\n```\n\n#### Test out some Queries\n[Section titled \u201cTest out some Queries\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-some-queries)\n```\n\n\nSettings.llm = dashscope_llm\n\n\n\n\nSettings.chunk_size =1024\n\n\n```\n\n```\n\n\nquery_engine = summary_index.as_query_engine()\n\n\n\n\nlist_response = query_engine.query(\"What is a summary of this document?\")\n\n\n```\n\n```\n\n\ndisplay_response(list_response)\n\n\n```\n\n```\n\n\nquery_engine = vector_index.as_query_engine()\n\n\n\n\nvector_response = query_engine.query(\"What did the author do growing up?\")\n\n\n```\n\n```\n\n\ndisplay_response(vector_response)\n\n\n```\n\n**`Final Response:`**Growing up, the author was involved in writing and programming outside of school. Initially, they wrote short stories, which they now consider to be not very good, as they lacked much plot and focused more on characters\u2019 emotions. In terms of programming, the author started with an IBM 1401 at their junior high school, where they attempted to write basic programs in Fortran using punch cards. Later, after getting a TRS-80 microcomputer, the author delved deeper into programming, creating simple games, a program to predict the flight height of model rockets, and even a word processor that their father used for writing.\n```\n\n\nquery_engine = keyword_table_index.as_query_engine()\n\n\n\n\nkeyword_response = query_engine.query(\n\n\n\n\n\"What did the author do after his time at YC?\"\n\n\n\n```\n\n```\n\n\ndisplay_response(keyword_response)\n\n\n```\n\n**`Final Response:`**After his time at YC, the author decided to take up painting, dedicating himself to it to see how good he could become. He spent most of 2014 focused on this. However, by November, he lost interest and stopped. Following this, he returned to writing essays and even ventured into topics beyond startups. In March 2015, he also began working on Lisp again.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#_top)\n# Tablestore Demo \nThis guide shows you how to directly use our `DocumentStore` abstraction backed by Tablestore. By putting nodes in the docstore, this allows you to define multiple indices over the same underlying docstore, instead of duplicating data across indices.\n```\n\n\n%pip install llama-index-storage-docstore-tablestore\n\n\n\n\n%pip install llama-index-storage-index-store-tablestore\n\n\n\n\n%pip install llama-index-vector-stores-tablestore\n\n\n\n\n\n%pip install llama-index-llms-dashscope\n\n\n\n\n%pip install llama-index-embeddings-dashscope\n\n\n\n\n\n%pip install llama-index\n\n\n\n\n%pip install matplotlib\n\n\n```\n\n```\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n```\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n```\n\n\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\n\n\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex\n\n\n\n\nfrom llama_index.core import SummaryIndex\n\n\n\n\nfrom llama_index.core.response.notebook_utils import display_response\n\n\n\n\nfrom llama_index.core import Settings\n\n\n```\n\n#### Config Tablestore\n[Section titled \u201cConfig Tablestore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-tablestore)\nNext, we use tablestore\u2019s docsstore to perform a demo.\n```\n\n\nimport getpass\n\n\n\n\nimport os\n\n\n\n\n\nos.environ[\"tablestore_end_point\"] = getpass.getpass(\"tablestore end_point:\")\n\n\n\n\nos.environ[\"tablestore_instance_name\"] = getpass.getpass(\n\n\n\n\n\"tablestore instance_name:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_id\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_id:\"\n\n\n\n\n\nos.environ[\"tablestore_access_key_secret\"] = getpass.getpass(\n\n\n\n\n\"tablestore access_key_secret:\"\n\n\n\n```\n\n#### Config DashScope LLM\n[Section titled \u201cConfig DashScope LLM\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#config-dashscope-llm)\nNext, we use dashscope\u2019s llm to perform a demo.\n```\n\n\nimport os\n\n\n\n\nimport getpass\n\n\n\n\n\nos.environ[\"DASHSCOPE_API_KEY\"] = getpass.getpass(\"DashScope api key:\")\n\n\n```\n\n#### Download Data\n[Section titled \u201cDownload Data\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#download-data)\n```\n\n\n!mkdir -p 'data/paul_graham/'\n\n\n\n\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt'-O 'data/paul_graham/paul_graham_essay.txt'\n\n\n```\n\n#### Load Documents\n[Section titled \u201cLoad Documents\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#load-documents)\n```\n\n\nreader =SimpleDirectoryReader(\"./data/paul_graham/\")\n\n\n\n\ndocuments = reader.load_data()\n\n\n```\n\n#### Parse into Nodes\n[Section titled \u201cParse into Nodes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#parse-into-nodes)\n```\n\n\nfrom llama_index.core.node_parser import SentenceSplitter\n\n\n\n\n\nnodes =SentenceSplitter().get_nodes_from_documents(documents)\n\n\n```\n\n#### Init Store/Embedding/LLM/StorageContext\n[Section titled \u201cInit Store/Embedding/LLM/StorageContext\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#init-storeembeddingllmstoragecontext)\n```\n\n\nfrom llama_index.storage.docstore.tablestore import TablestoreDocumentStore\n\n\n\n\nfrom llama_index.storage.index_store.tablestore import TablestoreIndexStore\n\n\n\n\nfrom llama_index.vector_stores.tablestore import TablestoreVectorStore\n\n\n\n\nfrom llama_index.embeddings.dashscope import (\n\n\n\n\nDashScopeEmbedding,\n\n\n\n\nDashScopeTextEmbeddingModels,\n\n\n\n\nDashScopeTextEmbeddingType,\n\n\n\n\n\nfrom llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n\n\n\n\n\nembedder =DashScopeEmbedding(\n\n\n\n\nmodel_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,# default demiension is 1024\n\n\n\n\ntext_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,\n\n\n\n\n\n\ndashscope_llm =DashScope(\n\n\n\n\nmodel_name=DashScopeGenerationModels.QWEN_MAX,\n\n\n\n\napi_key=os.environ[\"DASHSCOPE_API_KEY\"],\n\n\n\n\n\nSettings.llm = dashscope_llm\n\n\n\n\n\ndocstore = TablestoreDocumentStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nindex_store = TablestoreIndexStore.from_config(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\n\n\nvector_store =TablestoreVectorStore(\n\n\n\n\nendpoint=os.getenv(\"tablestore_end_point\"),\n\n\n\n\ninstance_name=os.getenv(\"tablestore_instance_name\"),\n\n\n\n\naccess_key_id=os.getenv(\"tablestore_access_key_id\"),\n\n\n\n\naccess_key_secret=os.getenv(\"tablestore_access_key_secret\"),\n\n\n\n\nvector_dimension=1024,# embedder dimension is 1024\n\n\n\n\n\nvector_store.create_table_if_not_exist()\n\n\n\n\nvector_store.create_search_index_if_not_exist()\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n```\n\n#### Add to docStore\n[Section titled \u201cAdd to docStore\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#add-to-docstore)\n```\n\n\nstorage_context.docstore.add_documents(nodes)\n\n\n```\n\n#### Define & Add Multiple Indexes\n[Section titled \u201cDefine & Add Multiple Indexes\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#define--add-multiple-indexes)\nEach index uses the same underlying Node.\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/list.html\n\n\n\nsummary_index =SummaryIndex(nodes,storage_context=storage_context)\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/vector_store.html\n\n\n\nvector_index =VectorStoreIndex(\n\n\n\n\nnodes,\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n```\n\n```\n\n# https://gpt-index.readthedocs.io/en/latest/api_reference/indices/table.html\n\n\n\nkeyword_table_index =SimpleKeywordTableIndex(\n\n\n\n\nnodes=nodes,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nllm=dashscope_llm,\n\n\n\n```\n\n```\n\n\n# NOTE: the docstore still has the same nodes\n\n\n\n\nlen(storage_context.docstore.docs)\n\n\n```\n\n#### Test out saving and loading\n[Section titled \u201cTest out saving and loading\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-saving-and-loading)\n```\n\n\n# NOTE: docstore and index_store is persisted in Tablestore by default\n\n\n\n\n# NOTE: here only need to persist simple vector store to disk\n\n\n\n\nstorage_context.persist()\n\n\n```\n\n```\n\n# note down index IDs\n\n\n\nlist_id = summary_index.index_id\n\n\n\n\nvector_id = vector_index.index_id\n\n\n\n\nkeyword_id = keyword_table_index.index_id\n\n\n\n\nprint(list_id, vector_id, keyword_id)\n\n\n```\n\n```\n\nc05fec2a-ac87-4761-beeb-0901f9e6530e d0b021ed-3427-46ad-927d-12d72752dbc4 2e9bfc3a-5e69-408a-9430-7b0c8baf3d77\n\n```\n\n```\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n# re-create storage context\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\ndocstore=docstore,index_store=index_store,vector_store=vector_store\n\n\n\n\n\n\nsummary_index =load_index_from_storage(\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=list_id,\n\n\n\n\n\nkeyword_table_index =load_index_from_storage(\n\n\n\n\nllm=dashscope_llm,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=keyword_id,\n\n\n\n\n# You need to add \"vector_store=xxx\" to StorageContext to load vector index from Tablestore\n\n\n\nvector_index =load_index_from_storage(\n\n\n\n\ninsert_batch_size=20,\n\n\n\n\nembed_model=embedder,\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\nindex_id=vector_id,\n\n\n\n```\n\n#### Test out some Queries\n[Section titled \u201cTest out some Queries\u201d](https://developers.llamaindex.ai/python/examples/docstore/tablestoredocstoredemo/#test-out-some-queries)\n```\n\n\nSettings.llm = dashscope_llm\n\n\n\n\nSettings.chunk_size =1024\n\n\n```\n\n```\n\n\nquery_engine = summary_index.as_query_engine()\n\n\n\n\nlist_response = query_engine.query(\"What is a summary of this document?\")\n\n\n```\n\n```\n\n\ndisplay_response(list_response)\n\n\n```\n\n```\n\n\nquery_engine = vector_index.as_query_engine()\n\n\n\n\nvector_response = query_engine.query(\"What did the author do growing up?\")\n\n\n```\n\n```\n\n\ndisplay_response(vector_response)\n\n\n```\n\n**`Final Response:`**Growing up, the author was involved in writing and programming outside of school. Initially, they wrote short stories, which they now consider to be not very good, as they lacked much plot and focused more on characters\u2019 emotions. In terms of programming, the author started with an IBM 1401 at their junior high school, where they attempted to write basic programs in Fortran using punch cards. Later, after getting a TRS-80 microcomputer, the author delved deeper into programming, creating simple games, a program to predict the flight height of model rockets, and even a word processor that their father used for writing.\n```\n\n\nquery_engine = keyword_table_index.as_query_engine()\n\n\n\n\nkeyword_response = query_engine.query(\n\n\n\n\n\"What did the author do after his time at YC?\"\n\n\n\n```\n\n```\n\n\ndisplay_response(keyword_response)\n\n\n```\n\n**`Final Response:`**After his time at YC, the author decided to take up painting, dedicating himself to it to see how good he could become. He spent most of 2014 focused on this. However, by November, he lost interest and stopped. Following this, he returned to writing essays and even ventured into topics beyond startups. In March 2015, he also began working on Lisp again.\n"}, "__type__": "4"}, "f11a5cfa-9966-4efa-a233-6837ee306c6a": {"__data__": {"id_": "f11a5cfa-9966-4efa-a233-6837ee306c6a", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_embeddings_huggingface.md", "file_name": "python_examples_embeddings_huggingface.md", "file_type": "text/markdown", "file_size": 6622, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#_top)\n# Local Embeddings with HuggingFace \nLlamaIndex has support for HuggingFace embedding models, including Sentence Transformer models like BGE, Mixedbread, Nomic, Jina, E5, etc. We can use these models to create embeddings for our documents and queries for retrieval.\nFurthermore, we provide utilities to create and use ONNX and OpenVINO models using the [Optimum library](https://huggingface.co/docs/optimum) from HuggingFace.\n## HuggingFaceEmbedding\n[Section titled \u201cHuggingFaceEmbedding\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#huggingfaceembedding)\nThe base `HuggingFaceEmbedding` class is a generic wrapper around any HuggingFace model for embeddings. All [embedding models](https://huggingface.co/models?library=sentence-transformers) on Hugging Face should work. You can refer to the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more recommendations.\nThis class depends on the sentence-transformers package, which you can install with `pip install sentence-transformers`.\nNOTE: if you were previously using a `HuggingFaceEmbeddings` from LangChain, this should give equivalent results.\nIf you\u2019re opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.\n```\n\n\n%pip install llama-index-embeddings-huggingface\n\n\n```\n\n```\n\n\n!pip install llama-index\n\n\n```\n\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads https://huggingface.co/BAAI/bge-small-en-v1.5\n\n\n\nembed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\n\n```\n\n```\n\n\nembeddings = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\nprint(len(embeddings))\n\n\n\n\nprint(embeddings[:5])\n\n\n```\n\n```\n\n384\n\n\n[-0.003275700844824314, -0.011690810322761536, 0.041559211909770966, -0.03814814239740372, 0.024183044210076332]\n\n```\n\n## Benchmarking\n[Section titled \u201cBenchmarking\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#benchmarking)\nLet\u2019s try comparing using a classic large document \u2014 the IPCC climate report, chapter 3.\n```\n\n\n!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf\n\n\n```\n\n```\n\n\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n\n\n\n\nDload  Upload   Total   Spent    Left  Speed\n\n\n\n\n\n0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\n\n\n100 20.7M  100 20.7M    0     0  69.6M      0 --:--:-- --:--:-- --:--:-- 70.0M\n\n```\n\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n\n\n\n\n).load_data()\n\n\n```\n\n### Base HuggingFace Embeddings\n[Section titled \u201cBase HuggingFace Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#base-huggingface-embeddings)\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the default torch backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nembed_batch_size=8,\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 428.44it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:19<00:00, 23.32it/s]\n\n\n\n\n20.2 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### ONNX Embeddings\n[Section titled \u201cONNX Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#onnx-embeddings)\n```\n\n# pip install sentence-transformers[onnx]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the onnx backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"onnx\",\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"provider\": \"CPUExecutionProvider\"\n\n\n\n\n},# For ONNX, you can specify the provider, see https://sbert.net/docs/sentence_transformer/usage/efficiency.html\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 421.63it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:31<00:00, 14.53it/s]\n\n\n\n32.1 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### OpenVINO Embeddings\n[Section titled \u201cOpenVINO Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#openvino-embeddings)\n```\n\n# pip install sentence-transformers[openvino]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the openvino backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"openvino\",# OpenVINO is very strong on CPUs\n\n\n\n\nrevision=\"refs/pr/16\",# BAAI/bge-small-en-v1.5 itself doesn't have an OpenVINO model currently, but there's a PR with it that we can load: https://huggingface.co/BAAI/bge-small-en-v1.5/discussions/16\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"file_name\": \"openvino_model_qint8_quantized.xml\"\n\n\n\n\n},# If we're using an optimized/quantized model, we need to specify the file name like this\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 403.15it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:08<00:00, 53.83it/s]\n\n\n\n\n9.03 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### References\n[Section titled \u201cReferences\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#references)\n  * [Local Embedding Models](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#local-embedding-models) explains more about using local models like these.\n  * [Sentence Transformers > Speeding up Inference](https://sbert.net/docs/sentence_transformer/usage/efficiency.html) contains extensive documentation on how to use the backend options effectively, including optimization and quantization for ONNX and OpenVINO.\n\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#_top)\n# Local Embeddings with HuggingFace \nLlamaIndex has support for HuggingFace embedding models, including Sentence Transformer models like BGE, Mixedbread, Nomic, Jina, E5, etc. We can use these models to create embeddings for our documents and queries for retrieval.\nFurthermore, we provide utilities to create and use ONNX and OpenVINO models using the [Optimum library](https://huggingface.co/docs/optimum) from HuggingFace.\n## HuggingFaceEmbedding\n[Section titled \u201cHuggingFaceEmbedding\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#huggingfaceembedding)\nThe base `HuggingFaceEmbedding` class is a generic wrapper around any HuggingFace model for embeddings. All [embedding models](https://huggingface.co/models?library=sentence-transformers) on Hugging Face should work. You can refer to the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more recommendations.\nThis class depends on the sentence-transformers package, which you can install with `pip install sentence-transformers`.\nNOTE: if you were previously using a `HuggingFaceEmbeddings` from LangChain, this should give equivalent results.\nIf you\u2019re opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.\n```\n\n\n%pip install llama-index-embeddings-huggingface\n\n\n```\n\n```\n\n\n!pip install llama-index\n\n\n```\n\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads https://huggingface.co/BAAI/bge-small-en-v1.5\n\n\n\nembed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\n\n```\n\n```\n\n\nembeddings = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\nprint(len(embeddings))\n\n\n\n\nprint(embeddings[:5])\n\n\n```\n\n```\n\n384\n\n\n[-0.003275700844824314, -0.011690810322761536, 0.041559211909770966, -0.03814814239740372, 0.024183044210076332]\n\n```\n\n## Benchmarking\n[Section titled \u201cBenchmarking\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#benchmarking)\nLet\u2019s try comparing using a classic large document \u2014 the IPCC climate report, chapter 3.\n```\n\n\n!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf\n\n\n```\n\n```\n\n\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n\n\n\n\nDload  Upload   Total   Spent    Left  Speed\n\n\n\n\n\n0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\n\n\n100 20.7M  100 20.7M    0     0  69.6M      0 --:--:-- --:--:-- --:--:-- 70.0M\n\n```\n\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n\n\n\n\n).load_data()\n\n\n```\n\n### Base HuggingFace Embeddings\n[Section titled \u201cBase HuggingFace Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#base-huggingface-embeddings)\n```\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the default torch backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nembed_batch_size=8,\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 428.44it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:19<00:00, 23.32it/s]\n\n\n\n\n20.2 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### ONNX Embeddings\n[Section titled \u201cONNX Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#onnx-embeddings)\n```\n\n# pip install sentence-transformers[onnx]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the onnx backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"onnx\",\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"provider\": \"CPUExecutionProvider\"\n\n\n\n\n},# For ONNX, you can specify the provider, see https://sbert.net/docs/sentence_transformer/usage/efficiency.html\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 421.63it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:31<00:00, 14.53it/s]\n\n\n\n32.1 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### OpenVINO Embeddings\n[Section titled \u201cOpenVINO Embeddings\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#openvino-embeddings)\n```\n\n# pip install sentence-transformers[openvino]\n\n\n\n# loads BAAI/bge-small-en-v1.5 with the openvino backend\n\n\n\nembed_model =HuggingFaceEmbedding(\n\n\n\n\nmodel_name=\"BAAI/bge-small-en-v1.5\",\n\n\n\n\ndevice=\"cpu\",\n\n\n\n\nbackend=\"openvino\",# OpenVINO is very strong on CPUs\n\n\n\n\nrevision=\"refs/pr/16\",# BAAI/bge-small-en-v1.5 itself doesn't have an OpenVINO model currently, but there's a PR with it that we can load: https://huggingface.co/BAAI/bge-small-en-v1.5/discussions/16\n\n\n\n\nmodel_kwargs={\n\n\n\n\n\"file_name\": \"openvino_model_qint8_quantized.xml\"\n\n\n\n\n},# If we're using an optimized/quantized model, we need to specify the file name like this\n\n\n\n\n\ntest_embeds = embed_model.get_text_embedding(\"Hello World!\")\n\n\n\n\n\nSettings.embed_model = embed_model\n\n\n```\n\n```\n\n\n%%timeit -r 1-n 1\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents,show_progress=True)\n\n\n```\n\n```\n\nParsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:00<00:00, 403.15it/s]\n\n\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 459/459 [00:08<00:00, 53.83it/s]\n\n\n\n\n9.03 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n```\n\n### References\n[Section titled \u201cReferences\u201d](https://developers.llamaindex.ai/python/examples/embeddings/huggingface/#references)\n  * [Local Embedding Models](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#local-embedding-models) explains more about using local models like these.\n  * [Sentence Transformers > Speeding up Inference](https://sbert.net/docs/sentence_transformer/usage/efficiency.html) contains extensive documentation on how to use the backend options effectively, including optimization and quantization for ONNX and OpenVINO.\n\n\n"}, "__type__": "4"}, "fdc96a9a-0207-455a-94d4-1de9e3268cdf": {"__data__": {"id_": "fdc96a9a-0207-455a-94d4-1de9e3268cdf", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_vector_stores_moorchehdemo.md", "file_name": "python_examples_vector_stores_moorchehdemo.md", "file_type": "text/markdown", "file_size": 5135, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#_top)\n# Moorcheh Vector Store Demo \n## Install Required Packages\n[Section titled \u201cInstall Required Packages\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#install-required-packages)\n```\n\n\n!pip install llama_index\n\n\n\n\n!pip install moorcheh_sdk\n\n\n```\n\n## Import Required Libraries\n[Section titled \u201cImport Required Libraries\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#import-required-libraries)\ndemo.py```\n\n# --- Welcome to the Demo of the Moorcheh Vector Store ---\n\n\n# --- Import the following packages --\n\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\nimport os\n\n\n\n\nfrom moorcheh_sdk import MoorchehClient\n\n\n\n\nfrom IPython.display import Markdown, display\n\n\n\n\nfrom typing import Any, Callable, Dict, List, Optional, cast\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nStorageContext,\n\n\n\n\nSettings,\n\n\n\n\n\nfrom llama_index.core.base.embeddings.base_sparse import BaseSparseEmbedding\n\n\n\n\nfrom llama_index.core.bridge.pydantic import PrivateAttr\n\n\n\n\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nBasePydanticVectorStore,\n\n\n\n\nMetadataFilters,\n\n\n\n\nVectorStoreQuery,\n\n\n\n\nVectorStoreQueryMode,\n\n\n\n\nVectorStoreQueryResult,\n\n\n\n\n\nfrom llama_index.core.vector_stores.utils import (\n\n\n\n\nDEFAULT_TEXT_KEY,\n\n\n\n\nlegacy_metadata_dict_to_node,\n\n\n\n\nmetadata_dict_to_node,\n\n\n\n\nnode_to_metadata_dict,\n\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nMetadataFilter,\n\n\n\n\nMetadataFilters,\n\n\n\n\nFilterOperator,\n\n\n\n\nFilterCondition,\n\n\n\n```\n\n## Configure Logging\n[Section titled \u201cConfigure Logging\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#configure-logging)\n```\n\n# --- Logging Setup ---\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n## Load Moorcheh API Key\n[Section titled \u201cLoad Moorcheh API Key\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-moorcheh-api-key)\n```\n\n# --- Set the values of the API Keys in your Environment Variables ---\n\n\n\nfrom google.colab import userdata\n\n\n\n\n\napi_key = os.environ[\"MOORCHEH_API_KEY\"] = userdata.get(\"MOORCHEH_API_KEY\")\n\n\n\n\n\nif\"MOORCHEH_API_KEY\"notin os.environ:\n\n\n\n\nraiseEnvironmentError(f\"Environment variable MOORCHEH_API_KEY is not set\")\n\n\n```\n\n## Load and Chunk Documents\n[Section titled \u201cLoad and Chunk Documents\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-and-chunk-documents)\n```\n\n# --- Load Documents ---\n\n\n\ndocuments =SimpleDirectoryReader(\"./documents\").load_data()\n\n\n\n\n# --- Set chunk size and overlap ---\n\n\n\nSettings.chunk_size =1024\n\n\n\n\nSettings.chunk_overlap =20\n\n\n```\n\n## Initialize Vector Store and Create Index\n[Section titled \u201cInitialize Vector Store and Create Index\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#initialize-vector-store-and-create-index)\n```\n\n# --- Initialize the Moorcheh Vector Store ---\n\n\n\n__all__ =[\"MoorchehVectorStore\"]\n\n\n\n\n# Creates a Moorcheh Vector Store with the following parameters\n\n\n# For text-based namespaces, set namespace_type to \"text\" and vector_dimension to None\n\n\n# For vector-based namespaces, set namespace_type to \"vector\" and vector_dimension to the dimension of your uploaded vectors\n\n\n\nvector_store =MoorchehVectorStore(\n\n\n\n\napi_key=api_key,\n\n\n\n\nnamespace=\"llamaindex_moorcheh\",\n\n\n\n\nnamespace_type=\"text\",\n\n\n\n\nvector_dimension=None,\n\n\n\n\nadd_sparse_vector=False,\n\n\n\n\nbatch_size=100,\n\n\n\n\n\n# --- Create a Vector Store Index using the Vector Store and given Documents ---\n\n\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,storage_context=storage_context\n\n\n\n```\n\n## Query the Vector Store\n[Section titled \u201cQuery the Vector Store\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#query-the-vector-store)\n```\n\n# --- Generate Response ---\n\n\n# --- Set Logging to DEBUG for more Detailed Outputs ---\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nresponse = vector_store.generate_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\"\n\n\n\n\n\nmoorcheh_response = vector_store.get_generative_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\",\n\n\n\n\nai_model=\"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n\n\n\n\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nresponse,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nmoorcheh_response,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\n# --- Filters for Metadata ---\n\n\n\nfilter=MetadataFilters(\n\n\n\n\nfilters=[\n\n\n\n\nMetadataFilter(\n\n\n\n\nkey=\"file_path\",\n\n\n\n\nvalue=\"insert the file path to the document here\",\n\n\n\n\noperator=FilterOperator.EQ,\n\n\n\n\n\n\ncondition=FilterCondition.AND,\n\n\n\n```\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#_top)\n# Moorcheh Vector Store Demo \n## Install Required Packages\n[Section titled \u201cInstall Required Packages\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#install-required-packages)\n```\n\n\n!pip install llama_index\n\n\n\n\n!pip install moorcheh_sdk\n\n\n```\n\n## Import Required Libraries\n[Section titled \u201cImport Required Libraries\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#import-required-libraries)\ndemo.py```\n\n# --- Welcome to the Demo of the Moorcheh Vector Store ---\n\n\n# --- Import the following packages --\n\n\n\nimport logging\n\n\n\n\nimport sys\n\n\n\n\nimport os\n\n\n\n\nfrom moorcheh_sdk import MoorchehClient\n\n\n\n\nfrom IPython.display import Markdown, display\n\n\n\n\nfrom typing import Any, Callable, Dict, List, Optional, cast\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nStorageContext,\n\n\n\n\nSettings,\n\n\n\n\n\nfrom llama_index.core.base.embeddings.base_sparse import BaseSparseEmbedding\n\n\n\n\nfrom llama_index.core.bridge.pydantic import PrivateAttr\n\n\n\n\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nBasePydanticVectorStore,\n\n\n\n\nMetadataFilters,\n\n\n\n\nVectorStoreQuery,\n\n\n\n\nVectorStoreQueryMode,\n\n\n\n\nVectorStoreQueryResult,\n\n\n\n\n\nfrom llama_index.core.vector_stores.utils import (\n\n\n\n\nDEFAULT_TEXT_KEY,\n\n\n\n\nlegacy_metadata_dict_to_node,\n\n\n\n\nmetadata_dict_to_node,\n\n\n\n\nnode_to_metadata_dict,\n\n\n\n\n\nfrom llama_index.core.vector_stores.types import (\n\n\n\n\nMetadataFilter,\n\n\n\n\nMetadataFilters,\n\n\n\n\nFilterOperator,\n\n\n\n\nFilterCondition,\n\n\n\n```\n\n## Configure Logging\n[Section titled \u201cConfigure Logging\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#configure-logging)\n```\n\n# --- Logging Setup ---\n\n\n\nlogging.basicConfig(stream=sys.stdout,level=logging.INFO)\n\n\n\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n```\n\n## Load Moorcheh API Key\n[Section titled \u201cLoad Moorcheh API Key\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-moorcheh-api-key)\n```\n\n# --- Set the values of the API Keys in your Environment Variables ---\n\n\n\nfrom google.colab import userdata\n\n\n\n\n\napi_key = os.environ[\"MOORCHEH_API_KEY\"] = userdata.get(\"MOORCHEH_API_KEY\")\n\n\n\n\n\nif\"MOORCHEH_API_KEY\"notin os.environ:\n\n\n\n\nraiseEnvironmentError(f\"Environment variable MOORCHEH_API_KEY is not set\")\n\n\n```\n\n## Load and Chunk Documents\n[Section titled \u201cLoad and Chunk Documents\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#load-and-chunk-documents)\n```\n\n# --- Load Documents ---\n\n\n\ndocuments =SimpleDirectoryReader(\"./documents\").load_data()\n\n\n\n\n# --- Set chunk size and overlap ---\n\n\n\nSettings.chunk_size =1024\n\n\n\n\nSettings.chunk_overlap =20\n\n\n```\n\n## Initialize Vector Store and Create Index\n[Section titled \u201cInitialize Vector Store and Create Index\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#initialize-vector-store-and-create-index)\n```\n\n# --- Initialize the Moorcheh Vector Store ---\n\n\n\n__all__ =[\"MoorchehVectorStore\"]\n\n\n\n\n# Creates a Moorcheh Vector Store with the following parameters\n\n\n# For text-based namespaces, set namespace_type to \"text\" and vector_dimension to None\n\n\n# For vector-based namespaces, set namespace_type to \"vector\" and vector_dimension to the dimension of your uploaded vectors\n\n\n\nvector_store =MoorchehVectorStore(\n\n\n\n\napi_key=api_key,\n\n\n\n\nnamespace=\"llamaindex_moorcheh\",\n\n\n\n\nnamespace_type=\"text\",\n\n\n\n\nvector_dimension=None,\n\n\n\n\nadd_sparse_vector=False,\n\n\n\n\nbatch_size=100,\n\n\n\n\n\n# --- Create a Vector Store Index using the Vector Store and given Documents ---\n\n\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,storage_context=storage_context\n\n\n\n```\n\n## Query the Vector Store\n[Section titled \u201cQuery the Vector Store\u201d](https://developers.llamaindex.ai/python/examples/vector_stores/moorchehdemo/#query-the-vector-store)\n```\n\n# --- Generate Response ---\n\n\n# --- Set Logging to DEBUG for more Detailed Outputs ---\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nresponse = vector_store.generate_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\"\n\n\n\n\n\nmoorcheh_response = vector_store.get_generative_answer(\n\n\n\n\nquery=\"Which company has had the highest revenue in 2025 and why?\",\n\n\n\n\nai_model=\"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n\n\n\n\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nresponse,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\nprint(\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\nmoorcheh_response,\n\n\n\n\n\"\\n\\n================================\\n\\n\",\n\n\n\n\n\n# --- Filters for Metadata ---\n\n\n\nfilter=MetadataFilters(\n\n\n\n\nfilters=[\n\n\n\n\nMetadataFilter(\n\n\n\n\nkey=\"file_path\",\n\n\n\n\nvalue=\"insert the file path to the document here\",\n\n\n\n\noperator=FilterOperator.EQ,\n\n\n\n\n\n\ncondition=FilterCondition.AND,\n\n\n\n```\n\n"}, "__type__": "4"}, "d8408380-1cb2-4bc0-a567-26245027660a": {"__data__": {"id_": "d8408380-1cb2-4bc0-a567-26245027660a", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_examples_workflow_sub_question_query_engine.md", "file_name": "python_examples_workflow_sub_question_query_engine.md", "file_type": "text/markdown", "file_size": 38883, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#_top)\n# Sub Question Query Engine as a workflow \nLlamaIndex has a built-in Sub-Question Query Engine. Here, we replace it with a Workflow-based equivalent.\nFirst we install our dependencies:\n  * LlamaIndex core for most things\n  * OpenAI LLM and embeddings for LLM actions\n  * `llama-index-readers-file` to power the PDF reader in `SimpleDirectoryReader`\n\n\n```\n\n\n!pip install llama-index-core llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file llama-index-utils-workflow\n\n\n```\n\nBring in our dependencies as imports:\n```\n\n\nimport os, json\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nStorageContext,\n\n\n\n\nload_index_from_storage,\n\n\n\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\nfrom llama_index.core.workflow import (\n\n\n\n\nstep,\n\n\n\n\nContext,\n\n\n\n\nWorkflow,\n\n\n\n\nEvent,\n\n\n\n\nStartEvent,\n\n\n\n\nStopEvent,\n\n\n\n\n\nfrom llama_index.core.agent import ReActAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.utils.workflow import draw_all_possible_flows\n\n\n```\n\n# Define the Sub Question Query Engine as a Workflow\n[Section titled \u201cDefine the Sub Question Query Engine as a Workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#define-the-sub-question-query-engine-as-a-workflow)\n  * Our StartEvent goes to `query()`, which takes care of several things:\n    * Accepts and stores the original query\n    * Stores the LLM to handle the queries\n    * Stores the list of tools to enable sub-questions\n    * Passes the original question to the LLM, asking it to split up the question into sub-questions\n    * Fires off a `QueryEvent` for every sub-question generated\n  * QueryEvents go to `sub_question()`, which instantiates a new ReAct agent with the full list of tools available and lets it select which one to use.\n    * This is slightly better than the actual SQQE built-in to LlamaIndex, which cannot use multiple tools\n    * Each QueryEvent generates an `AnswerEvent`\n  * AnswerEvents go to `combine_answers()`.\n    * This uses `self.collect_events()` to wait for every QueryEvent to return an answer.\n    * All the answers are then combined into a final prompt for the LLM to consolidate them into a single response\n    * A StopEvent is generated to return the final result\n\n\n```\n\n\nclassQueryEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\n\n\nclassAnswerEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\nanswer: str\n\n\n\n\n\n\nclassSubQuestionQueryEngine(Workflow):\n\n\n\n\n@step\n\n\n\n\nasyncdefquery(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n\n\n\n\nifhasattr(ev,\"query\"):\n\n\n\n\nawait ctx.store.set(\"original_query\", ev.query)\n\n\n\n\nprint(f\"Query is {await ctx.store.get('original_query')}\")\n\n\n\n\n\nifhasattr(ev,\"llm\"):\n\n\n\n\nawait ctx.store.set(\"llm\", ev.llm)\n\n\n\n\n\nifhasattr(ev,\"tools\"):\n\n\n\n\nawait ctx.store.set(\"tools\", ev.tools)\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(\n\n\n\n\nf\"\"\"\n\n\n\n\nGiven a user question, and a list of tools, output a list of\n\n\n\n\nrelevant sub-questions, such that the answers to all the\n\n\n\n\nsub-questions put together will answer the question. Respond\n\n\n\n\nin pure JSON without any markdown, like this:\n\n\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What is the population of San Francisco?\",\n\n\n\n\n\"What is the budget of San Francisco?\",\n\n\n\n\n\"What is the GDP of San Francisco?\"\n\n\n\n\n\n\nHere is the user question: {await ctx.store.get('original_query')}\n\n\n\n\n\nAnd here is the list of tools: {await ctx.store.get('tools')}\n\n\n\n\n\n\n\nprint(f\"Sub-questions are {response}\")\n\n\n\n\n\nresponse_obj = json.loads(str(response))\n\n\n\n\nsub_questions = response_obj[\"sub_questions\"]\n\n\n\n\n\nawait ctx.store.set(\"sub_question_count\",(sub_questions))\n\n\n\n\n\nfor question in sub_questions:\n\n\n\n\nself.send_event(QueryEvent(question=question))\n\n\n\n\n\nreturnNone\n\n\n\n\n\n@step\n\n\n\n\nasyncdefsub_question(self, ctx: Context, ev: QueryEvent) -> AnswerEvent:\n\n\n\n\nprint(f\"Sub-question is {ev.question}\")\n\n\n\n\n\nagent = ReActAgent.from_tools(\n\n\n\n\nawait ctx.store.get(\"tools\"),\n\n\n\n\nllm=await ctx.store.get(\"llm\"),\n\n\n\n\nverbose=True,\n\n\n\n\n\nresponse = agent.chat(ev.question)\n\n\n\n\n\nreturnAnswerEvent(question=ev.question,answer=str(response))\n\n\n\n\n\n@step\n\n\n\n\nasyncdefcombine_answers(\n\n\n\n\nself, ctx: Context, ev: AnswerEvent\n\n\n\n\n) -> StopEvent |None:\n\n\n\n\nready = ctx.collect_events(\n\n\n\n\nev,[AnswerEvent]*await ctx.store.get(\"sub_question_count\")\n\n\n\n\n\nif ready isNone:\n\n\n\n\nreturnNone\n\n\n\n\n\nanswers =\"\\n\\n\".join(\n\n\n\n\n\nf\"Question: {event.question}: \\n Answer: {event.answer}\"\n\n\n\n\nfor event in ready\n\n\n\n\n\n\n\nprompt =f\"\"\"\n\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: {await ctx.store.get('original_query')}\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\n{answers}\n\n\n\n\n\n\nprint(f\"Final prompt is {prompt}\")\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(prompt)\n\n\n\n\n\nprint(\"Final response is\", response)\n\n\n\n\n\nreturnStopEvent(result=str(response))\n\n\n```\n\n```\n\n\ndraw_all_possible_flows(\n\n\n\n\nSubQuestionQueryEngine,filename=\"sub_question_query_engine.html\"\n\n\n\n```\n\n```\n\nsub_question_query_engine.html\n\n```\n\nVisualizing this flow looks pretty linear, since it doesn\u2019t capture that `query()` can generate multiple parallel `QueryEvents` which get collected into `combine_answers`.\n# Download data to demo\n[Section titled \u201cDownload data to demo\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#download-data-to-demo)\n```\n\n\n!mkdir -p \"./data/sf_budgets/\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\"-O \"./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\"-O \"./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\"-O \"./data/sf_budgets/2018 - 2018-o0181-18.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\"-O \"./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\"-O \"./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\"-O \"./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\"-O \"./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\"\n\n\n```\n\n```\n\n--2024-08-07 18:21:11--  https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file# [following]\n\n\n--2024-08-07 18:21:12--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file\n\n\nResolving ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file [following]\n\n\n--2024-08-07 18:21:13--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file\n\n\nReusing existing connection to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 29467998 (28M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  28.10M   173MB/s    in 0.2s\n\n\n\n2024-08-07 18:21:14 (173 MB/s) - \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019 saved [29467998/29467998]\n\n\n\n--2024-08-07 18:21:14--  https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file# [following]\n\n\n--2024-08-07 18:21:15--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file\n\n\nResolving uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file [following]\n\n\n--2024-08-07 18:21:16--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file\n\n\nReusing existing connection to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13463517 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.84M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:17 (136 MB/s) - \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019 saved [13463517/13463517]\n\n\n\n--2024-08-07 18:21:17--  https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file# [following]\n\n\n--2024-08-07 18:21:17--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file\n\n\nResolving uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file [following]\n\n\n--2024-08-07 18:21:18--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file\n\n\nReusing existing connection to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 18487865 (18M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  17.63M  --.-KB/s    in 0.1s\n\n\n\n2024-08-07 18:21:19 (149 MB/s) - \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019 saved [18487865/18487865]\n\n\n\n--2024-08-07 18:21:19--  https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file# [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file\n\n\nResolving uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file\n\n\nReusing existing connection to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13123938 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.52M  --.-KB/s    in 0.08s\n\n\n\n2024-08-07 18:21:22 (161 MB/s) - \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019 saved [13123938/13123938]\n\n\n\n--2024-08-07 18:21:22--  https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file# [following]\n\n\n--2024-08-07 18:21:22--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file\n\n\nResolving uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file [following]\n\n\n--2024-08-07 18:21:24--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file\n\n\nReusing existing connection to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3129122 (3.0M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   2.98M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:24 (66.3 MB/s) - \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019 saved [3129122/3129122]\n\n\n\n--2024-08-07 18:21:24--  https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file# [following]\n\n\n--2024-08-07 18:21:26--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file\n\n\nResolving uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file [following]\n\n\n--2024-08-07 18:21:27--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file\n\n\nReusing existing connection to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3233272 (3.1M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   3.08M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:28 (61.4 MB/s) - \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019 saved [3233272/3233272]\n\n\n\n--2024-08-07 18:21:28--  https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file# [following]\n\n\n--2024-08-07 18:21:29--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file\n\n\nResolving uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file [following]\n\n\n--2024-08-07 18:21:30--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file\n\n\nReusing existing connection to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 10550407 (10M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  10.06M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:31 (110 MB/s) - \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019 saved [10550407/10550407]\n\n```\n\n# Load data and run the workflow\n[Section titled \u201cLoad data and run the workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#load-data-and-run-the-workflow)\nJust like using the built-in Sub-Question Query Engine, we create our query tools and instantiate an LLM and pass them in.\nEach tool is its own query engine based on a single (very lengthy) San Francisco budget document, each of which is 300+ pages. To save time on repeated runs, we persist our generated indexes to disk.\n```\n\n\nfrom google.colab import userdata\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = userdata.get(\"openai-key\")\n\n\n\n\n\nfolder =\"./data/sf_budgets/\"\n\n\n\n\nfiles = os.listdir(folder)\n\n\n\n\n\nquery_engine_tools =[]\n\n\n\n\nforfilein files:\n\n\n\n\nyear =file.split(\"\")[0]\n\n\n\n\nindex_persist_path =f\"./storage/budget-{year}/\"\n\n\n\n\n\nif os.path.exists(index_persist_path):\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=index_persist_path\n\n\n\n\n\nindex =load_index_from_storage(storage_context)\n\n\n\n\nelse:\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[folder +]\n\n\n\n\n).load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nindex.storage_context.persist(index_persist_path)\n\n\n\n\n\nengine = index.as_query_engine()\n\n\n\n\nquery_engine_tools.append(\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"budget_{year}\",\n\n\n\n\ndescription=f\"Information about San Francisco's budget in {year}\",\n\n\n\n\n\n\n\n\nengine =SubQuestionQueryEngine(timeout=120,verbose=True)\n\n\n\n\nllm =OpenAI(model=\"gpt-4o\")\n\n\n\n\nresult =await engine.run(\n\n\n\n\nllm=llm,\n\n\n\n\ntools=query_engine_tools,\n\n\n\n\nquery=\"How has the total amount of San Francisco's budget changed from 2016 to 2023?\",\n\n\n\n\n\n\nprint(result)\n\n\n```\n\n```\n\nRunning step query\n\n\nQuery is How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\nSub-questions are {\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2016?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2017?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2018?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2019?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2020?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2021?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2022?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2023?\"\n\n\n\n\n\nStep query produced no event\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2016?\n\n\n> Running step 61365946-614c-4895-8fc3-0968f2d63387. Step input: What was the total amount of San Francisco's budget in 2016?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2016\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2016\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0m> Running step a85aa30e-a980-4897-a52e-e82b8fb25c72. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2017?\n\n\n> Running step 5d14466c-1400-4a26-ac42-021e7143d3b1. Step input: What was the total amount of San Francisco's budget in 2017?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2017\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2017\"}\n\n\n[0m[1;3;34mObservation: $10,106.9 million\n\n\n[0m> Running step 586a5fab-95ee-44e9-9a35-fcf19993b13e. Step input: None\n\n\n[1;3;38;5;200mThought: I have the information needed to answer the question.\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2018?\n\n\n> Running step d39f64d0-65f6-4571-95ad-d28a16198ea5. Step input: What was the total amount of San Francisco's budget in 2018?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2018\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2018\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0m> Running step 3f67feee-489c-4b9e-8f27-37f0d48e3b0d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2019?\n\n\n> Running step d5ac0866-b02c-4c4c-94c6-f0e047ebb0fe. Step input: What was the total amount of San Francisco's budget in 2019?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2019\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2019\"}\n\n\n[0m[1;3;34mObservation: $12.3 billion\n\n\n[0m> Running step 3b62859b-bbd3-4dce-b284-f9b398e370c2. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2020?\n\n\n> Running step 41f6ed9f-d695-43df-8743-39dfcc3d919d. Step input: What was the total amount of San Francisco's budget in 2020?\n\n\n[1;3;38;5;200mThought: The user is asking for the total amount of San Francisco's budget in 2020. I do not have a tool specifically for the 2020 budget. I will check the available tools to see if they provide any relevant information or if I can infer the 2020 budget from adjacent years.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"What was the total amount of San Francisco's budget in 2020?\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0m> Running step ea39f7c6-e942-4a41-8963-f37d4a27d559. Step input: None\n\n\n[1;3;38;5;200mThought: I now have the information needed to answer the user's question about the total amount of San Francisco's budget in 2020.\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2021?\n\n\n> Running step 6662fd06-e86a-407c-bb89-4828f63caa72. Step input: What was the total amount of San Francisco's budget in 2021?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2021\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2021 is $14,166,496,000.\n\n\n[0m> Running step 5d0cf9da-2c14-407c-8ae5-5cd638c1fb5c. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2022?\n\n\n> Running step 62fa9a5f-f40b-489d-9773-5ee4e4eaba9e. Step input: What was the total amount of San Francisco's budget in 2022?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2022\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2022\"}\n\n\n[0m[1;3;34mObservation: $14,550,060\n\n\n[0m> Running step 7a2d5623-cc75-4c9d-8c58-3dbc2faf163d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2023?\n\n\n> Running step 839eb994-b4e2-4019-a170-9471c1e0d764. Step input: What was the total amount of San Francisco's budget in 2023?\n\n\n[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2023\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2023\"}\n\n\n[0m[1;3;34mObservation: $14.6 billion\n\n\n[0m> Running step 38779f6c-d0c7-4c95-b5c4-0b170c5ed0d5. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nFinal prompt is\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2016?:\n\n\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2017?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2018?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2019?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2020?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2021?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2022?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2023?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n\n\nFinal response is From 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n\nStep combine_answers produced event StopEvent\n\n\nFrom 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n```\n\nOur debug output is lengthy! You can see the sub-questions being generated and then `sub_question()` being repeatedly invoked, each time generating a brief log of ReAct agent thoughts and actions to answer each smaller question.\nYou can see `combine_answers` running multiple times; these were triggered by each `AnswerEvent` but before all 8 `AnswerEvents` were collected. On its final run it generates a full prompt, combines the answers and returns the result.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#_top)\n# Sub Question Query Engine as a workflow \nLlamaIndex has a built-in Sub-Question Query Engine. Here, we replace it with a Workflow-based equivalent.\nFirst we install our dependencies:\n  * LlamaIndex core for most things\n  * OpenAI LLM and embeddings for LLM actions\n  * `llama-index-readers-file` to power the PDF reader in `SimpleDirectoryReader`\n\n\n```\n\n\n!pip install llama-index-core llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file llama-index-utils-workflow\n\n\n```\n\nBring in our dependencies as imports:\n```\n\n\nimport os, json\n\n\n\n\nfrom llama_index.core import (\n\n\n\n\nSimpleDirectoryReader,\n\n\n\n\nVectorStoreIndex,\n\n\n\n\nStorageContext,\n\n\n\n\nload_index_from_storage,\n\n\n\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\nfrom llama_index.core.workflow import (\n\n\n\n\nstep,\n\n\n\n\nContext,\n\n\n\n\nWorkflow,\n\n\n\n\nEvent,\n\n\n\n\nStartEvent,\n\n\n\n\nStopEvent,\n\n\n\n\n\nfrom llama_index.core.agent import ReActAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.utils.workflow import draw_all_possible_flows\n\n\n```\n\n# Define the Sub Question Query Engine as a Workflow\n[Section titled \u201cDefine the Sub Question Query Engine as a Workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#define-the-sub-question-query-engine-as-a-workflow)\n  * Our StartEvent goes to `query()`, which takes care of several things:\n    * Accepts and stores the original query\n    * Stores the LLM to handle the queries\n    * Stores the list of tools to enable sub-questions\n    * Passes the original question to the LLM, asking it to split up the question into sub-questions\n    * Fires off a `QueryEvent` for every sub-question generated\n  * QueryEvents go to `sub_question()`, which instantiates a new ReAct agent with the full list of tools available and lets it select which one to use.\n    * This is slightly better than the actual SQQE built-in to LlamaIndex, which cannot use multiple tools\n    * Each QueryEvent generates an `AnswerEvent`\n  * AnswerEvents go to `combine_answers()`.\n    * This uses `self.collect_events()` to wait for every QueryEvent to return an answer.\n    * All the answers are then combined into a final prompt for the LLM to consolidate them into a single response\n    * A StopEvent is generated to return the final result\n\n\n```\n\n\nclassQueryEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\n\n\nclassAnswerEvent(Event):\n\n\n\n\nquestion: str\n\n\n\n\nanswer: str\n\n\n\n\n\n\nclassSubQuestionQueryEngine(Workflow):\n\n\n\n\n@step\n\n\n\n\nasyncdefquery(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n\n\n\n\nifhasattr(ev,\"query\"):\n\n\n\n\nawait ctx.store.set(\"original_query\", ev.query)\n\n\n\n\nprint(f\"Query is {await ctx.store.get('original_query')}\")\n\n\n\n\n\nifhasattr(ev,\"llm\"):\n\n\n\n\nawait ctx.store.set(\"llm\", ev.llm)\n\n\n\n\n\nifhasattr(ev,\"tools\"):\n\n\n\n\nawait ctx.store.set(\"tools\", ev.tools)\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(\n\n\n\n\nf\"\"\"\n\n\n\n\nGiven a user question, and a list of tools, output a list of\n\n\n\n\nrelevant sub-questions, such that the answers to all the\n\n\n\n\nsub-questions put together will answer the question. Respond\n\n\n\n\nin pure JSON without any markdown, like this:\n\n\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What is the population of San Francisco?\",\n\n\n\n\n\"What is the budget of San Francisco?\",\n\n\n\n\n\"What is the GDP of San Francisco?\"\n\n\n\n\n\n\nHere is the user question: {await ctx.store.get('original_query')}\n\n\n\n\n\nAnd here is the list of tools: {await ctx.store.get('tools')}\n\n\n\n\n\n\n\nprint(f\"Sub-questions are {response}\")\n\n\n\n\n\nresponse_obj = json.loads(str(response))\n\n\n\n\nsub_questions = response_obj[\"sub_questions\"]\n\n\n\n\n\nawait ctx.store.set(\"sub_question_count\",(sub_questions))\n\n\n\n\n\nfor question in sub_questions:\n\n\n\n\nself.send_event(QueryEvent(question=question))\n\n\n\n\n\nreturnNone\n\n\n\n\n\n@step\n\n\n\n\nasyncdefsub_question(self, ctx: Context, ev: QueryEvent) -> AnswerEvent:\n\n\n\n\nprint(f\"Sub-question is {ev.question}\")\n\n\n\n\n\nagent = ReActAgent.from_tools(\n\n\n\n\nawait ctx.store.get(\"tools\"),\n\n\n\n\nllm=await ctx.store.get(\"llm\"),\n\n\n\n\nverbose=True,\n\n\n\n\n\nresponse = agent.chat(ev.question)\n\n\n\n\n\nreturnAnswerEvent(question=ev.question,answer=str(response))\n\n\n\n\n\n@step\n\n\n\n\nasyncdefcombine_answers(\n\n\n\n\nself, ctx: Context, ev: AnswerEvent\n\n\n\n\n) -> StopEvent |None:\n\n\n\n\nready = ctx.collect_events(\n\n\n\n\nev,[AnswerEvent]*await ctx.store.get(\"sub_question_count\")\n\n\n\n\n\nif ready isNone:\n\n\n\n\nreturnNone\n\n\n\n\n\nanswers =\"\\n\\n\".join(\n\n\n\n\n\nf\"Question: {event.question}: \\n Answer: {event.answer}\"\n\n\n\n\nfor event in ready\n\n\n\n\n\n\n\nprompt =f\"\"\"\n\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: {await ctx.store.get('original_query')}\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\n{answers}\n\n\n\n\n\n\nprint(f\"Final prompt is {prompt}\")\n\n\n\n\n\nresponse = (await ctx.store.get(\"llm\")).complete(prompt)\n\n\n\n\n\nprint(\"Final response is\", response)\n\n\n\n\n\nreturnStopEvent(result=str(response))\n\n\n```\n\n```\n\n\ndraw_all_possible_flows(\n\n\n\n\nSubQuestionQueryEngine,filename=\"sub_question_query_engine.html\"\n\n\n\n```\n\n```\n\nsub_question_query_engine.html\n\n```\n\nVisualizing this flow looks pretty linear, since it doesn\u2019t capture that `query()` can generate multiple parallel `QueryEvents` which get collected into `combine_answers`.\n# Download data to demo\n[Section titled \u201cDownload data to demo\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#download-data-to-demo)\n```\n\n\n!mkdir -p \"./data/sf_budgets/\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\"-O \"./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\"-O \"./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\"-O \"./data/sf_budgets/2018 - 2018-o0181-18.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\"-O \"./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\"-O \"./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\"-O \"./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\"\n\n\n\n\n!wget \"https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\"-O \"./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\"\n\n\n```\n\n```\n\n--2024-08-07 18:21:11--  https://www.dropbox.com/scl/fi/xt3squt47djba0j7emmjb/2016-CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf?rlkey=xs064cjs8cb4wma6t5pw2u2bl&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file# [following]\n\n\n--2024-08-07 18:21:12--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline/CYOq1NGkhLkELMmygLIg_gyLPXsOO7xOLjc3jW-mb09kevMykGPogSQx_icUTBEfHshxiSainXTynZYnh5O6uZ4ITeGiMkpvjl1QqXkKI34Ea8WzLr4FEyzkwohAC2WCQAU/file\n\n\nResolving ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com (ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file [following]\n\n\n--2024-08-07 18:21:13--  https://ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com/cd/0/inline2/CYMeJ4nM2JL44i1kCE4kttRGFOk-_34sr37ALElZu9szHfn-VhihA7l4cjIIFKHNN1ajRfeYYspGW3zPK1BZShxO3O7SEaXnHpUwUaziUcoz6b5IkdtXww3M6tRf8K2MZB4pHMSwxiuKe_vw9jitwHNeHn-jVzVRMw9feenAHN21LDudw5PxmsvqXSLeHMAGgs_tjeo1o92vltmhL6FpHs2czHsQFlYuaFMzwecv2xAMzHUGCGOhfNkmg2af16lP2QKLKgWAPK4ttCePTv-Ivy2KQ_GYVKKXRFlYHkIwhCQ_JFOyrtl_n14xls76NyPZRSZWmygSHJ-HH6Hntqvi86XpgCF-N_dZJh_HhSxuAaZd2g/file\n\n\nReusing existing connection to ucca241228aca55dbf9fcd60ae81.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 29467998 (28M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  28.10M   173MB/s    in 0.2s\n\n\n\n2024-08-07 18:21:14 (173 MB/s) - \u2018./data/sf_budgets/2016 - CSF_Budget_Book_2016_FINAL_WEB_with-cover-page.pdf\u2019 saved [29467998/29467998]\n\n\n\n--2024-08-07 18:21:14--  https://www.dropbox.com/scl/fi/jvw59g5nscu1m7f96tjre/2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf?rlkey=v988oigs2whtcy87ti9wti6od&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file# [following]\n\n\n--2024-08-07 18:21:15--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline/CYNMSJ2zt2I5765XfzleiddbUXb-TkZP91r9LuVw_6wBH0USNyLT6lclDE7x6I0-_WEaGM7zqqCipxx7Uyp5owmnwMx8JyfbHG3fZ4LSDYM6QzubFok7NSc0R2KRd3DX0qg/file\n\n\nResolving uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com (uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file [following]\n\n\n--2024-08-07 18:21:16--  https://uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com/cd/0/inline2/CYNJm2hH6OlYZdhW6cv8AuAYvgEiuyOY1KUwzlH1Nq4RrvmmOHg2ipVgEq88bfVDEC_xV0SegX6DL-4CUB_6_2AjHC7iS5VnZVxsjkbpQHTqEKr7OK6mAlsGNPQi--ocxwOsUbQNpLVNSjEc2zA98VZLpntTl3AoJEvl4wmpvBhNCs_ChiY2TDNcQGFDPH5AjvEEHImiNQqCzrOzoSpFh9Ut9NQty6vjADUHg1yXFcPa5R-ODch6hb4FgTCQZv7WYQJ7H_MRHVJyLoIyCX8bqwZAblnXC9SbUuIxdgmkiAB_wwjJKuFLV7YNNjJX5kg9spGoYnRv7gNDqUhjvXBwKW_IQxsYc1HjsaabrrRFjXntAw/file\n\n\nReusing existing connection to uca6ee7d218771b609b8ecdf23d7.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13463517 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.84M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:17 (136 MB/s) - \u2018./data/sf_budgets/2017 - 2017-Proposed-Budget-FY2017-18-FY2018-19_1.pdf\u2019 saved [13463517/13463517]\n\n\n\n--2024-08-07 18:21:17--  https://www.dropbox.com/scl/fi/izknlwmbs7ia0lbn7zzyx/2018-o0181-18.pdf?rlkey=p5nv2ehtp7272ege3m9diqhei&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file# [following]\n\n\n--2024-08-07 18:21:17--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline/CYOEOqz8prU7eZPDzgM8fwVVcHoP1lWOLF--9VoNPtzVDSvDCXUDxR1CeN_VMzOp4JGTG6V-CeYm7oLwrrEIjuWThf5rHt8eLh52TF1nJ4-jVPrn7nAjFrealf436uezAs0/file\n\n\nResolving uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com (uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file [following]\n\n\n--2024-08-07 18:21:18--  https://uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com/cd/0/inline2/CYNZxULkXqH5RXSO_Tu0-X2BLjKqLUg3ZAH3vZeEHw-ic156C2iVH3wjJtcm6mkh-RpMfru6d3ZBBNTpf_EWLTWBywklJbD4ZhRInyrnF6s5oK4NWS6UQ_7GBHy11itN5OKGF9U0090wCFaQeaPwFyLxwIjhg_gZdTc8smr1YFyESsFTIJTLPq8QjI5uPvYyug6Oidh8RxOP2N2f2mBKDRS2R8cazDZRDrAxhVeAuSXPGpYzQc0lBcsTJQ8ZAXuYKww0e_qlpyHmDv6tRVHpdFNh1dyKyikOHqtGd4p3pYjBr2Kwn-jzJ1zkZf_Fpc_H9vX0Xkk6P9U25oOGvSnmIUC3LFkfHB_CJTGNSZUh36w5cA/file\n\n\nReusing existing connection to uc922ffad4a58390c4df80dcafbf.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 18487865 (18M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  17.63M  --.-KB/s    in 0.1s\n\n\n\n2024-08-07 18:21:19 (149 MB/s) - \u2018./data/sf_budgets/2018 - 2018-o0181-18.pdf\u2019 saved [18487865/18487865]\n\n\n\n--2024-08-07 18:21:19--  https://www.dropbox.com/scl/fi/1rstqm9rh5u5fr0tcjnxj/2019-Proposed-Budget-FY2019-20-FY2020-21.pdf?rlkey=3s2ivfx7z9bev1r840dlpbcgg&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file# [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline/CYNSfAOo0ymwbrL62gVbRB_NTvZpU2t5SZqnLuZDW-OaDOssaoY8SkQxPM9csoAq0-Y3Y8rYA1E6cDD44K1pSJcsuRSyoRRVLHRmXvWdayHKMK_PWAo08V3murDu9ZZAu4s/file\n\n\nResolving uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com (uce28a421063a08c4ce431616623.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file [following]\n\n\n--2024-08-07 18:21:20--  https://uce28a421063a08c4ce431616623.dl.dropboxusercontent.com/cd/0/inline2/CYOFZQRiPKCvnUe8S4h3AQ8gmhPC0MW_0vNg2GTCzxiUPSVRSgUXDsH8XYOgKuU905goGB1ZmWgs00sNArASToS2iE6pJgGfqsk3DYELK3xYZJOwJ_AscWEAjoISiZQEPhi9-QyQpyeXAr5gxavu9eMq3XFNzo9SCUA-SWFIuSCU5Tf5_ZfW_uAU41NZE4dDVsdvaD7rG4Ouci6dp6c902A2dHsNs0O-wRZEEKKFZs5KeHNLvZkdTaUGxYcgQn8vwWgTbuvAz36XycX6Sdhdp32mFF73U30G5ZTUmqAvgYDMlUilhdcJLPhhbrUyhFUWcXrfluUHkK8LkjKCPl4ywKmr8oJGji5ZOwehdXWgrL7ALg/file\n\n\nReusing existing connection to uce28a421063a08c4ce431616623.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 13123938 (13M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  12.52M  --.-KB/s    in 0.08s\n\n\n\n2024-08-07 18:21:22 (161 MB/s) - \u2018./data/sf_budgets/2019 - 2019-Proposed-Budget-FY2019-20-FY2020-21.pdf\u2019 saved [13123938/13123938]\n\n\n\n--2024-08-07 18:21:22--  https://www.dropbox.com/scl/fi/7teuwxrjdyvgw0n8jjvk0/2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf?rlkey=6br3wzxwj5fv1f1l8e69nbmhk&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file# [following]\n\n\n--2024-08-07 18:21:22--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline/CYMRjMFyYInwu1LATw9fLxGctgY-zI7_0nI1zgKVeJJf55J9CxQivdYpDYLjkYlXCKv2t6rQ9NCns9A5jDEU3xiQ0Ycrd6VrPv7tiYSYvNY7pXMBiV2LvXu7ZDtQgBH1334/file\n\n\nResolving uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com (uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file [following]\n\n\n--2024-08-07 18:21:24--  https://uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com/cd/0/inline2/CYOOkJRGOrBeY0GY5xS_84ayGgfFapr4kvbiFcnAUkvwENgCw8Z3qTT_G2oQpBq6h-RVzjOh4SPrgusfRfbEWg9ZxXwxyWPo5I4yJ7eVhhqTi2jZN42r_k1FWF4IjxgRhMA237BSrCcKkweLmMNm3oN4cFap5dw2fyesDaZg0xa-fRAEjF5MubgvXVAwNVmEvrL8M7Sm4s4VsguOPsytt9GqfPkuARDvYXGLfvZeCx4hRfqOaNXdeGyBSy3GUBKyf8bH3YTHw6wEBk8Yp2dG64Q8FJyUgAXkpn1wZpBQe0dnk5WdoWrKrtkL4RDbBPo1k0fDfKeuajw_h5BhtEAl5XVE-11C0IEzcse1D-19TNlSuQ/file\n\n\nReusing existing connection to uc8421b1eeadb07bda3c7e093660.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3129122 (3.0M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   2.98M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:24 (66.3 MB/s) - \u2018./data/sf_budgets/2021 - 2021-AAO-FY20-21-FY21-22-09-11-2020-FINAL.pdf\u2019 saved [3129122/3129122]\n\n\n\n--2024-08-07 18:21:24--  https://www.dropbox.com/scl/fi/zhgqch4n6xbv9skgcknij/2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf?rlkey=h78t65dfaz3mqbpbhl1u9e309&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file# [following]\n\n\n--2024-08-07 18:21:26--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline/CYPqlj1-wREOG6CVYV9KgsQ4Pyu3rqHgdY_UD2MqZIAndb3fAaRZeCB8kTXrOnILu6iGZZcjERz2tqT2mMiIcM86nxXDH6_J7tva-D9ZOwLROXr64weKF_NFuWTHcenrINM/file\n\n\nResolving uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com (uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file [following]\n\n\n--2024-08-07 18:21:27--  https://uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com/cd/0/inline2/CYNbMKpeTmC_in9_57ZDTlkiMBzRJiPbEXNEcIxLjRQJHTQEYhPcMmdqHcWdoP9Fxi1LYMKQDt1DUW1ZJYX1TxpLjIDxFyezLCprT2JfhkCROToyraIBrDpXPFgMEbBxNJIsBT1x70oL7BXSbW-pKomX6OKsy_nAP1B5jDVxhXOZtJwW8xFJwkvhNo71Aam2bT1wENAWKLdZOcVz4WRIdDI7e4Ri5FZ27Sjy2RCojgcFYusbpMWZFrxui-ssQzHsXvD1ZrZpKjyUXMIq_pdkbonY0V-8Iuq7PudclrjCIsDU2fD0bqo2MLdXw69PDLy2m5uVohTgcM0qCykha7dfGiP3BWfBpEM0PbmcfHx_IDqWDw/file\n\n\nReusing existing connection to uc769a7232da4f728018e664ed74.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 3233272 (3.1M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]   3.08M  --.-KB/s    in 0.05s\n\n\n\n2024-08-07 18:21:28 (61.4 MB/s) - \u2018./data/sf_budgets/2022 - 2022-AAO-FY2021-22-FY2022-23-FINAL-20210730.pdf\u2019 saved [3233272/3233272]\n\n\n\n--2024-08-07 18:21:28--  https://www.dropbox.com/scl/fi/vip161t63s56vd94neqlt/2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf?rlkey=hemoce3w1jsuf6s2bz87g549i&dl=0\n\n\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6057:18::a27d:d12\n\n\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file# [following]\n\n\n--2024-08-07 18:21:29--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline/CYOKIz5n4gWk1Ywf1Ovmc-Dua40rRvPhK4YtffCdTlHM3tOiFbzgN6pyDNBx0vNo5fnHFEr5ilQwYHekMrlKykqII8thu9wiDbfAifKojwVXbgxJ1-Bqz6GXkPlLPp4rXkw/file\n\n\nResolving uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n\n\nConnecting to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com (uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n\n\nHTTP request sent, awaiting response... 302 Found\n\n\nLocation: /cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file [following]\n\n\n--2024-08-07 18:21:30--  https://uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com/cd/0/inline2/CYOKgVW-_SqOvVicBez1JsKaYs81mU1xzB4gynkTKGfcI9xEPnjv2pLp8NTtEuaREbjOoLQBNeBO9bLhjMMPubNVHYnWl8KSMk_nJ4WNWlIlK0UjNllsYqOzvtAD6gSDFlYt21i_WaYBOFR6wjOI4ZM69i6uREONYUBODDZ_tfdcbv5rfX87wGP8eZ47KeO9nBUwvpMNhj9Tby7bBuI0qVaIrjREqzYMap1VNN68SXOoDJbF2bdCS6O55U2vL9CvSXjuehi-fWcaEKisFhQCIGT-PyzNY1F2Vd3zl5DH-aqeEInObuL26LGOgAIEbU6c0PHHq10-GKWo40fv2ECnrTxXLD89T5dhJQJ9mCamCA_COg/file\n\n\nReusing existing connection to uc3fa3b8bc2f92ed126eb3788d35.dl.dropboxusercontent.com:443.\n\n\nHTTP request sent, awaiting response... 200 OK\n\n\nLength: 10550407 (10M) [application/pdf]\n\n\nSaving to: \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019\n\n\n\n./data/sf_budgets/2 100%[===================>]  10.06M  --.-KB/s    in 0.09s\n\n\n\n2024-08-07 18:21:31 (110 MB/s) - \u2018./data/sf_budgets/2023 - 2023-CSF_Proposed_Budget_Book_June_2023_Master_Web.pdf\u2019 saved [10550407/10550407]\n\n```\n\n# Load data and run the workflow\n[Section titled \u201cLoad data and run the workflow\u201d](https://developers.llamaindex.ai/python/examples/workflow/sub_question_query_engine/#load-data-and-run-the-workflow)\nJust like using the built-in Sub-Question Query Engine, we create our query tools and instantiate an LLM and pass them in.\nEach tool is its own query engine based on a single (very lengthy) San Francisco budget document, each of which is 300+ pages. To save time on repeated runs, we persist our generated indexes to disk.\n```\n\n\nfrom google.colab import userdata\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = userdata.get(\"openai-key\")\n\n\n\n\n\nfolder =\"./data/sf_budgets/\"\n\n\n\n\nfiles = os.listdir(folder)\n\n\n\n\n\nquery_engine_tools =[]\n\n\n\n\nforfilein files:\n\n\n\n\nyear =file.split(\"\")[0]\n\n\n\n\nindex_persist_path =f\"./storage/budget-{year}/\"\n\n\n\n\n\nif os.path.exists(index_persist_path):\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=index_persist_path\n\n\n\n\n\nindex =load_index_from_storage(storage_context)\n\n\n\n\nelse:\n\n\n\n\ndocuments =SimpleDirectoryReader(\n\n\n\n\ninput_files=[folder +]\n\n\n\n\n).load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nindex.storage_context.persist(index_persist_path)\n\n\n\n\n\nengine = index.as_query_engine()\n\n\n\n\nquery_engine_tools.append(\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"budget_{year}\",\n\n\n\n\ndescription=f\"Information about San Francisco's budget in {year}\",\n\n\n\n\n\n\n\n\nengine =SubQuestionQueryEngine(timeout=120,verbose=True)\n\n\n\n\nllm =OpenAI(model=\"gpt-4o\")\n\n\n\n\nresult =await engine.run(\n\n\n\n\nllm=llm,\n\n\n\n\ntools=query_engine_tools,\n\n\n\n\nquery=\"How has the total amount of San Francisco's budget changed from 2016 to 2023?\",\n\n\n\n\n\n\nprint(result)\n\n\n```\n\n```\n\nRunning step query\n\n\nQuery is How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\nSub-questions are {\n\n\n\n\"sub_questions\": [\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2016?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2017?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2018?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2019?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2020?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2021?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2022?\",\n\n\n\n\n\"What was the total amount of San Francisco's budget in 2023?\"\n\n\n\n\n\nStep query produced no event\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2016?\n\n\n> Running step 61365946-614c-4895-8fc3-0968f2d63387. Step input: What was the total amount of San Francisco's budget in 2016?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2016\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2016\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0m> Running step a85aa30e-a980-4897-a52e-e82b8fb25c72. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2017?\n\n\n> Running step 5d14466c-1400-4a26-ac42-021e7143d3b1. Step input: What was the total amount of San Francisco's budget in 2017?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2017\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2017\"}\n\n\n[0m[1;3;34mObservation: $10,106.9 million\n\n\n[0m> Running step 586a5fab-95ee-44e9-9a35-fcf19993b13e. Step input: None\n\n\n[1;3;38;5;200mThought: I have the information needed to answer the question.\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2018?\n\n\n> Running step d39f64d0-65f6-4571-95ad-d28a16198ea5. Step input: What was the total amount of San Francisco's budget in 2018?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2018\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2018\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0m> Running step 3f67feee-489c-4b9e-8f27-37f0d48e3b0d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2019?\n\n\n> Running step d5ac0866-b02c-4c4c-94c6-f0e047ebb0fe. Step input: What was the total amount of San Francisco's budget in 2019?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2019\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2019\"}\n\n\n[0m[1;3;34mObservation: $12.3 billion\n\n\n[0m> Running step 3b62859b-bbd3-4dce-b284-f9b398e370c2. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2020?\n\n\n> Running step 41f6ed9f-d695-43df-8743-39dfcc3d919d. Step input: What was the total amount of San Francisco's budget in 2020?\n\n\n[1;3;38;5;200mThought: The user is asking for the total amount of San Francisco's budget in 2020. I do not have a tool specifically for the 2020 budget. I will check the available tools to see if they provide any relevant information or if I can infer the 2020 budget from adjacent years.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"What was the total amount of San Francisco's budget in 2020?\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0m> Running step ea39f7c6-e942-4a41-8963-f37d4a27d559. Step input: None\n\n\n[1;3;38;5;200mThought: I now have the information needed to answer the user's question about the total amount of San Francisco's budget in 2020.\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2021?\n\n\n> Running step 6662fd06-e86a-407c-bb89-4828f63caa72. Step input: What was the total amount of San Francisco's budget in 2021?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2021\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2021\"}\n\n\n[0m[1;3;34mObservation: The total amount of San Francisco's budget in 2021 is $14,166,496,000.\n\n\n[0m> Running step 5d0cf9da-2c14-407c-8ae5-5cd638c1fb5c. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2022?\n\n\n> Running step 62fa9a5f-f40b-489d-9773-5ee4e4eaba9e. Step input: What was the total amount of San Francisco's budget in 2022?\n\n\n[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2022\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2022\"}\n\n\n[0m[1;3;34mObservation: $14,550,060\n\n\n[0m> Running step 7a2d5623-cc75-4c9d-8c58-3dbc2faf163d. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step sub_question\n\n\nSub-question is What was the total amount of San Francisco's budget in 2023?\n\n\n> Running step 839eb994-b4e2-4019-a170-9471c1e0d764. Step input: What was the total amount of San Francisco's budget in 2023?\n\n\n[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\n\nAction: budget_2023\n\n\nAction Input: {'input': \"total amount of San Francisco's budget in 2023\"}\n\n\n[0m[1;3;34mObservation: $14.6 billion\n\n\n[0m> Running step 38779f6c-d0c7-4c95-b5c4-0b170c5ed0d5. Step input: None\n\n\n[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n[0mStep sub_question produced event AnswerEvent\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nStep combine_answers produced no event\n\n\nRunning step combine_answers\n\n\nFinal prompt is\n\n\n\nYou are given an overall question that has been split into sub-questions,\n\n\n\n\neach of which has been answered. Combine the answers to all the sub-questions\n\n\n\n\ninto a single answer to the original question.\n\n\n\n\n\nOriginal question: How has the total amount of San Francisco's budget changed from 2016 to 2023?\n\n\n\n\n\nSub-questions and answers:\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2016?:\n\n\n\n\nAnswer: The total amount of San Francisco's budget in 2016 was $9.6 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2017?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2017 was $10,106.9 million.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2018?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2018 was $12,659,306,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2019?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2019 was $12.3 billion.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2020?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2020 was $15,373,192 (in thousands of dollars).\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2021?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2021 was $14,166,496,000.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2022?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2022 was $14,550,060.\n\n\n\n\nQuestion: What was the total amount of San Francisco's budget in 2023?:\n\n\n\nAnswer: The total amount of San Francisco's budget in 2023 was $14.6 billion.\n\n\n\n\nFinal response is From 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n\nStep combine_answers produced event StopEvent\n\n\nFrom 2016 to 2023, the total amount of San Francisco's budget has seen significant changes. In 2016, the budget was $9.6 billion. It increased to $10,106.9 million in 2017 and further to $12,659,306,000 in 2018. In 2019, the budget was $12.3 billion. The budget saw a substantial rise in 2020, reaching $15,373,192 (in thousands of dollars), which translates to approximately $15.4 billion. In 2021, the budget was $14,166,496,000, and in 2022, it was $14,550,060. By 2023, the budget had increased to $14.6 billion. Overall, from 2016 to 2023, San Francisco's budget grew from $9.6 billion to $14.6 billion.\n\n```\n\nOur debug output is lengthy! You can see the sub-questions being generated and then `sub_question()` being repeatedly invoked, each time generating a brief log of ReAct agent thoughts and actions to answer each smaller question.\nYou can see `combine_answers` running multiple times; these were triggered by each `AnswerEvent` but before all 8 `AnswerEvents` were collected. On its final run it generates a full prompt, combines the answers and returns the result.\n"}, "__type__": "4"}, "5b273a66-5a72-4f2c-aa64-3a00694facbf": {"__data__": {"id_": "5b273a66-5a72-4f2c-aa64-3a00694facbf", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework-api-reference_schema.md", "file_name": "python_framework-api-reference_schema.md", "file_type": "text/markdown", "file_size": 86114, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "# Index\nBase schema for data structures.\n##  BaseComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent \"Permanent link\")\nBases: `BaseModel`\nBase component object to capture class names.\nSource code in `llama_index/core/schema.py`\n```\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n```\n| ```\nclass BaseComponent(BaseModel):\n\"\"\"Base component object to capture class names.\"\"\"\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)\n        json_schema = handler.resolve_ref_schema(json_schema)\n\n        # inject class name to help with serde\n        if \"properties\" in json_schema:\n            json_schema[\"properties\"][\"class_name\"] = {\n                \"title\": \"Class Name\",\n                \"type\": \"string\",\n                \"default\": cls.class_name(),\n            }\n        return json_schema\n\n    @classmethod\n    def class_name(cls) -> str:\n\"\"\"\n        Get the class name, used as a unique ID in serialization.\n\n        This provides a key that makes serialization robust against actual class\n        name changes.\n        \"\"\"\n        return \"base_component\"\n\n    def json(self, **kwargs: Any) -> str:\n        return self.to_json(**kwargs)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n        data = handler(self)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        return self.model_dump(**kwargs)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = super().__getstate__()\n\n        # remove attributes that are not pickleable -- kind of dangerous\n        keys_to_remove = []\n        for key, val in state[\"__dict__\"].items():\n            try:\n                pickle.dumps(val)\n            except Exception:\n                keys_to_remove.append(key)\n\n        for key in keys_to_remove:\n            logging.warning(f\"Removing unpickleable attribute {key}\")\n            del state[\"__dict__\"][key]\n\n        # remove private attributes if they aren't pickleable -- kind of dangerous\n        keys_to_remove = []\n        private_attrs = state.get(\"__pydantic_private__\", None)\n        if private_attrs:\n            for key, val in state[\"__pydantic_private__\"].items():\n                try:\n                    pickle.dumps(val)\n                except Exception:\n                    keys_to_remove.append(key)\n\n            for key in keys_to_remove:\n                logging.warning(f\"Removing unpickleable private attribute {key}\")\n                del state[\"__pydantic_private__\"][key]\n\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        # Use the __dict__ and __init__ method to set state\n        # so that all variables initialize\n        try:\n            self.__init__(**state[\"__dict__\"])  # type: ignore\n        except Exception:\n            # Fall back to the default __setstate__ method\n            # This may not work if the class had unpickleable attributes\n            super().__setstate__(state)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = self.dict(**kwargs)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def to_json(self, **kwargs: Any) -> str:\n        data = self.to_dict(**kwargs)\n        return json.dumps(data)\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        # In SimpleKVStore we rely on shallow coping. Hence, the data will be modified in the store directly.\n        # And it is the same when the user is passing a dictionary to create a component. We can't modify the passed down dictionary.\n        data = dict(data)\n        if isinstance(kwargs, dict):\n            data.update(kwargs)\n        data.pop(\"class_name\", None)\n        return cls(**data)\n\n    @classmethod\n    def from_json(cls, data_str: str, **kwargs: Any) -> Self:  # type: ignore\n        data = json.loads(data_str)\n        return cls.from_dict(data, **kwargs)\n\n```\n  \n---|---  \n###  class_name `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent.class_name \"Permanent link\")\n```\nclass_name() -> \n\n```\n\nGet the class name, used as a unique ID in serialization.\nThis provides a key that makes serialization robust against actual class name changes.\nSource code in `llama_index/core/schema.py`\n```\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n```\n| ```\n@classmethod\ndef class_name(cls) -> str:\n\"\"\"\n    Get the class name, used as a unique ID in serialization.\n\n    This provides a key that makes serialization robust against actual class\n    name changes.\n    \"\"\"\n    return \"base_component\"\n\n```\n  \n---|---  \n##  TransformComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent \"Permanent link\")\nBases: , `DispatcherSpanMixin`\nBase class for transform components.\nSource code in `llama_index/core/schema.py`\n```\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n```\n| ```\nclass TransformComponent(BaseComponent, DispatcherSpanMixin):\n\"\"\"Base class for transform components.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:\n\"\"\"Transform nodes.\"\"\"\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n        return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n###  acall `async` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent.acall \"Permanent link\")\n```\nacall(nodes: Sequence[], **kwargs: ) -> Sequence[]\n\n```\n\nAsync transform nodes.\nSource code in `llama_index/core/schema.py`\n```\n199\n200\n201\n202\n203\n```\n| ```\nasync def acall(\n    self, nodes: Sequence[BaseNode], **kwargs: Any\n) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n    return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n##  NodeRelationship [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeRelationship \"Permanent link\")\nBases: `str`, `Enum`\nNode relationships used in `BaseNode` class.\nAttributes:\nName | Type | Description  \n---|---|---  \n`SOURCE` |  The node is the source document.  \n`PREVIOUS` |  The node is the previous node in the document.  \nThe node is the next node in the document.  \n`PARENT` |  The node is the parent node in the document.  \n`CHILD` |  The node is a child node in the document.  \nSource code in `llama_index/core/schema.py`\n```\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n```\n| ```\nclass NodeRelationship(str, Enum):\n\"\"\"\n    Node relationships used in `BaseNode` class.\n\n    Attributes:\n        SOURCE: The node is the source document.\n        PREVIOUS: The node is the previous node in the document.\n        NEXT: The node is the next node in the document.\n        PARENT: The node is the parent node in the document.\n        CHILD: The node is a child node in the document.\n\n    \"\"\"\n\n    SOURCE = auto()\n    PREVIOUS = auto()\n    NEXT = auto()\n    PARENT = auto()\n    CHILD = auto()\n\n```\n  \n---|---  \n##  RelatedNodeInfo [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.RelatedNodeInfo \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node_id` |  _required_  \n`node_type` |  `Annotated[ObjectType, PlainSerializer] | str | None` |  `None`  \n`hash` |  `str | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n248\n249\n250\n251\n252\n253\n254\n255\n256\n```\n| ```\nclass RelatedNodeInfo(BaseComponent):\n    node_id: str\n    node_type: Annotated[ObjectType, EnumNameSerializer] | str | None = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    hash: Optional[str] = None\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"RelatedNodeInfo\"\n\n```\n  \n---|---  \n##  BaseNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode \"Permanent link\")\nBases: \nBase node Object.\nGeneric abstract interface for retrievable nodes\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`id_` |  Unique ID of the node. |  `'f64e9cec-5009-4429-bf4d-fa73983e3f01'`  \n`embedding` |  `List[float] | None` |  Embedding of the node. |  `None`  \n`excluded_embed_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the embed model. |  `<dynamic>`  \n`excluded_llm_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the LLM. |  `<dynamic>`  \n`metadata_template` |  Template for how metadata is formatted, with {key} and {value} placeholders. |  `'{key}: {value}'`  \n`metadata_separator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \nSource code in `llama_index/core/schema.py`\n```\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\nclass BaseNode(BaseComponent):\n\"\"\"\n    Base node Object.\n\n    Generic abstract interface for retrievable nodes\n\n    \"\"\"\n\n    # hash is computed on local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\"\n    )\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"Embedding of the node.\"\n    )\n\n\"\"\"\"\n    metadata fields\n    - injected as part of the text shown to LLMs as context\n    - injected as part of the text for generating embeddings\n    - used by vector DBs for metadata filtering\n\n    \"\"\"\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"A flat dictionary of metadata fields\",\n        alias=\"extra_info\",\n    )\n    excluded_embed_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the embed model.\",\n    )\n    excluded_llm_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the LLM.\",\n    )\n    relationships: Dict[\n        Annotated[NodeRelationship, EnumNameSerializer],\n        RelatedNodeType,\n    ] = Field(\n        default_factory=dict,\n        description=\"A mapping of relationships to other node information.\",\n    )\n    metadata_template: str = Field(\n        default=DEFAULT_METADATA_TMPL,\n        description=(\n            \"Template for how metadata is formatted, with {key} and \"\n            \"{value} placeholders.\"\n        ),\n    )\n    metadata_separator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n        alias=\"metadata_seperator\",\n    )\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n    @abstractmethod\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_separator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    @abstractmethod\n    def set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n    @property\n    @abstractmethod\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n\n    @property\n    def node_id(self) -> str:\n        return self.id_\n\n    @node_id.setter\n    def node_id(self, value: str) -> None:\n        self.id_ = value\n\n    @property\n    def source_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"\n        Source object node.\n\n        Extracted from the relationships field.\n\n        \"\"\"\n        if NodeRelationship.SOURCE not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.SOURCE]\n        if isinstance(relation, list):\n            raise ValueError(\"Source object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def prev_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Prev node.\"\"\"\n        if NodeRelationship.PREVIOUS not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PREVIOUS]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Previous object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def next_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Next node.\"\"\"\n        if NodeRelationship.NEXT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.NEXT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Next object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def parent_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Parent node.\"\"\"\n        if NodeRelationship.PARENT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PARENT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Parent object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def child_nodes(self) -> Optional[List[RelatedNodeInfo]]:\n\"\"\"Child nodes.\"\"\"\n        if NodeRelationship.CHILD not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.CHILD]\n        if not isinstance(relation, list):\n            raise ValueError(\"Child objects must be a list of RelatedNodeInfo objects.\")\n        return relation\n\n    @property\n    def ref_doc_id(self) -> Optional[str]:  # pragma: no cover\n\"\"\"Deprecated: Get ref doc id.\"\"\"\n        source_node = self.source_node\n        if source_node is None:\n            return None\n        return source_node.node_id\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self) -> dict[str, Any]:  # pragma: no coverde\n        return self.metadata\n\n    @extra_info.setter\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self, extra_info: dict[str, Any]) -> None:  # pragma: no coverde\n        self.metadata = extra_info\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Node ID: {self.node_id}\\n{source_text_wrapped}\"\n\n    def get_embedding(self) -> List[float]:\n\"\"\"\n        Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    def as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n        return RelatedNodeInfo(\n            node_id=self.node_id,\n            node_type=self.get_type(),\n            metadata=self.metadata,\n            hash=self.hash,\n        )\n\n```\n  \n---|---  \n###  embedding `class-attribute` `instance-attribute` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.embedding \"Permanent link\")\n```\nembedding: Optional[[float]] = (default=None, description='Embedding of the node.')\n\n```\n\n\" metadata fields - injected as part of the text shown to LLMs as context - injected as part of the text for generating embeddings - used by vector DBs for metadata filtering\n###  hash `abstractmethod` `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  source_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.source_node \"Permanent link\")\n```\nsource_node: Optional[]\n\n```\n\nSource object node.\nExtracted from the relationships field.\n###  prev_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.prev_node \"Permanent link\")\n```\nprev_node: Optional[]\n\n```\n\nPrev node.\n###  next_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.next_node \"Permanent link\")\n```\nnext_node: Optional[]\n\n```\n\nNext node.\n###  parent_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.parent_node \"Permanent link\")\n```\nparent_node: Optional[]\n\n```\n\nParent node.\n###  child_nodes `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.child_nodes \"Permanent link\")\n```\nchild_nodes: Optional[[]]\n\n```\n\nChild nodes.\n###  ref_doc_id `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.ref_doc_id \"Permanent link\")\n```\nref_doc_id: Optional[]\n\n```\n\nDeprecated: Get ref doc id.\n###  get_type `abstractmethod` `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n321\n322\n323\n324\n```\n| ```\n@classmethod\n@abstractmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n```\n  \n---|---  \n###  get_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n326\n327\n328\n```\n| ```\n@abstractmethod\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_separator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n353\n354\n355\n```\n| ```\n@abstractmethod\ndef set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n```\n  \n---|---  \n###  get_embedding [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_embedding \"Permanent link\")\n```\nget_embedding() -> [float]\n\n```\n\nGet embedding.\nErrors if embedding is None.\nSource code in `llama_index/core/schema.py`\n```\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n```\n| ```\ndef get_embedding(self) -> List[float]:\n\"\"\"\n    Get embedding.\n\n    Errors if embedding is None.\n\n    \"\"\"\n    if self.embedding is None:\n        raise ValueError(\"embedding not set.\")\n    return self.embedding\n\n```\n  \n---|---  \n###  as_related_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.as_related_node_info \"Permanent link\")\n```\nas_related_node_info() -> \n\n```\n\nGet node as RelatedNodeInfo.\nSource code in `llama_index/core/schema.py`\n```\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\ndef as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n    return RelatedNodeInfo(\n        node_id=self.node_id,\n        node_type=self.get_type(),\n        metadata=self.metadata,\n        hash=self.hash,\n    )\n\n```\n  \n---|---  \n##  MediaResource [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"Permanent link\")\nBases: `BaseModel`\nA container class for media content.\nThis class represents a generic media resource that can be stored and accessed in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports storing vector embeddings for the media content.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`embeddings` |  `dict[Literal['sparse', 'dense'], list[float]] | None` |  Vector representation of this resource. |  `None`  \n`data` |  `bytes | None` |  base64 binary representation of this resource. |  `None`  \n`text` |  `str | None` |  Text representation of this resource. |  `None`  \n`path` |  `Path | None` |  Filesystem path of this resource. |  `None`  \n`url` |  `AnyUrl | None` |  URL to reach this resource. |  `None`  \n`mimetype` |  `str | None` |  MIME type of this resource. |  `None`  \nAttributes:\nName | Type | Description  \n---|---|---  \n`embeddings` |  Multi-vector dict representation of this resource for embedding-based search/retrieval  \n`text` |  Plain text representation of this resource  \n`data` |  Raw binary data of the media content  \n`mimetype` |  The MIME type indicating the format/type of the media content  \n`path` |  Local filesystem path where the media content can be accessed  \nURL where the media content can be accessed remotely  \nSource code in `llama_index/core/schema.py`\n```\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n```\n| ```\nclass MediaResource(BaseModel):\n\"\"\"\n    A container class for media content.\n\n    This class represents a generic media resource that can be stored and accessed\n    in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports\n    storing vector embeddings for the media content.\n\n    Attributes:\n        embeddings: Multi-vector dict representation of this resource for embedding-based search/retrieval\n        text: Plain text representation of this resource\n        data: Raw binary data of the media content\n        mimetype: The MIME type indicating the format/type of the media content\n        path: Local filesystem path where the media content can be accessed\n        url: URL where the media content can be accessed remotely\n\n    \"\"\"\n\n    embeddings: dict[EmbeddingKind, list[float]] | None = Field(\n        default=None, description=\"Vector representation of this resource.\"\n    )\n    data: bytes | None = Field(\n        default=None,\n        exclude=True,\n        description=\"base64 binary representation of this resource.\",\n    )\n    text: str | None = Field(\n        default=None, description=\"Text representation of this resource.\"\n    )\n    path: Path | None = Field(\n        default=None, description=\"Filesystem path of this resource.\"\n    )\n    url: AnyUrl | None = Field(default=None, description=\"URL to reach this resource.\")\n    mimetype: str | None = Field(\n        default=None, description=\"MIME type of this resource.\"\n    )\n\n    model_config = {\n        # This ensures validation runs even for None values\n        \"validate_default\": True\n    }\n\n    @field_validator(\"data\", mode=\"after\")\n    @classmethod\n    def validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n        If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n        In case the model was built passing binary data but without a mimetype,\n        we try to guess it using the filetype library. To avoid resource-intense\n        operations, we won't load the path or the URL to guess the mimetype.\n        \"\"\"\n        if v is None:\n            return v\n\n        try:\n            # Check if data is already base64 encoded.\n            # b64decode() can succeed on random binary data, so we\n            # pass verify=True to make sure it's not a false positive\n            decoded = base64.b64decode(v, validate=True)\n        except BinasciiError:\n            # b64decode failed, return encoded\n            return base64.b64encode(v)\n\n        # Good as is, return unchanged\n        return v\n\n    @field_validator(\"mimetype\", mode=\"after\")\n    @classmethod\n    def validate_mimetype(cls, v: str | None, info: ValidationInfo) -> str | None:\n        if v is not None:\n            return v\n\n        # Since this field validator runs after the one for `data`\n        # then the contents of `data` should be encoded already\n        b64_data = info.data.get(\"data\")\n        if b64_data:  # encoded bytes\n            decoded_data = base64.b64decode(b64_data)\n            if guess := filetype.guess(decoded_data):\n                return guess.mime\n\n        # guess from path\n        rpath: str | None = info.data[\"path\"]\n        if rpath:\n            extension = Path(rpath).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                return ftype.mime\n\n        return v\n\n    @field_serializer(\"path\")  # type: ignore\n    def serialize_path(\n        self, path: Optional[Path], _info: ValidationInfo\n    ) -> Optional[str]:\n        if path is None:\n            return path\n        return str(path)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash to uniquely identify the media resource.\n\n        The hash is generated based on the available content (data, path, text or url).\n        Returns an empty string if no content is available.\n        \"\"\"\n        bits: list[str] = []\n        if self.text is not None:\n            bits.append(self.text)\n        if self.data is not None:\n            # Hash the binary data if available\n            bits.append(str(sha256(self.data).hexdigest()))\n        if self.path is not None:\n            # Hash the file path if provided\n            bits.append(str(sha256(str(self.path).encode(\"utf-8\")).hexdigest()))\n        if self.url is not None:\n            # Use the URL string as basis for hash\n            bits.append(str(sha256(str(self.url).encode(\"utf-8\")).hexdigest()))\n\n        doc_identity = \"\".join(bits)\n        if not doc_identity:\n            return \"\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash to uniquely identify the media resource.\nThe hash is generated based on the available content (data, path, text or url). Returns an empty string if no content is available.\n###  validate_data `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.validate_data \"Permanent link\")\n```\nvalidate_data(v: bytes | None, info: ValidationInfo) -> bytes | None\n\n```\n\nIf binary data was passed, store the resource as base64 and guess the mimetype when possible.\nIn case the model was built passing binary data but without a mimetype, we try to guess it using the filetype library. To avoid resource-intense operations, we won't load the path or the URL to guess the mimetype.\nSource code in `llama_index/core/schema.py`\n```\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n```\n| ```\n@field_validator(\"data\", mode=\"after\")\n@classmethod\ndef validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n    If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n    In case the model was built passing binary data but without a mimetype,\n    we try to guess it using the filetype library. To avoid resource-intense\n    operations, we won't load the path or the URL to guess the mimetype.\n    \"\"\"\n    if v is None:\n        return v\n\n    try:\n        # Check if data is already base64 encoded.\n        # b64decode() can succeed on random binary data, so we\n        # pass verify=True to make sure it's not a false positive\n        decoded = base64.b64decode(v, validate=True)\n    except BinasciiError:\n        # b64decode failed, return encoded\n        return base64.b64encode(v)\n\n    # Good as is, return unchanged\n    return v\n\n```\n  \n---|---  \n##  Node [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Text content of the node. |  `None`  \n`image_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Image content of the node. |  `None`  \n`audio_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Audio content of the node. |  `None`  \n`video_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Video content of the node. |  `None`  \n`text_template` |  Template for how text_resource is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n```\n| ```\nclass Node(BaseNode):\n    text_resource: MediaResource | None = Field(\n        default=None, description=\"Text content of the node.\"\n    )\n    image_resource: MediaResource | None = Field(\n        default=None, description=\"Image content of the node.\"\n    )\n    audio_resource: MediaResource | None = Field(\n        default=None, description=\"Audio content of the node.\"\n    )\n    video_resource: MediaResource | None = Field(\n        default=None, description=\"Video content of the node.\"\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text_resource is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Node\"\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.MULTIMODAL\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n        Get the text content for the node if available.\n\n        Provided for backward compatibility, use self.text_resource directly instead.\n        \"\"\"\n        if self.text_resource:\n            metadata_str = self.get_metadata_str(metadata_mode)\n            if metadata_mode == MetadataMode.NONE or not metadata_str:\n                return self.text_resource.text or \"\"\n\n            return self.text_template.format(\n                content=self.text_resource.text or \"\",\n                metadata_str=metadata_str,\n            ).strip()\n        return \"\"\n\n    def set_content(self, value: str) -> None:\n\"\"\"\n        Set the text content of the node.\n\n        Provided for backward compatibility, set self.text_resource instead.\n        \"\"\"\n        self.text_resource = MediaResource(text=value)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash representing the state of the node.\n\n        The hash is generated based on the available resources (audio, image, text or video) and its metadata.\n        \"\"\"\n        doc_identities = []\n        metadata_str = self.get_metadata_str(mode=MetadataMode.ALL)\n        if metadata_str:\n            doc_identities.append(metadata_str)\n        if self.audio_resource is not None:\n            doc_identities.append(self.audio_resource.hash)\n        if self.image_resource is not None:\n            doc_identities.append(self.image_resource.hash)\n        if self.text_resource is not None:\n            doc_identities.append(self.text_resource.hash)\n        if self.video_resource is not None:\n            doc_identities.append(self.video_resource.hash)\n\n        doc_identity = \"-\".join(doc_identities)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash representing the state of the node.\nThe hash is generated based on the available resources (audio, image, text or video) and its metadata.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n637\n638\n639\n640\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.MULTIMODAL\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet the text content for the node if available.\nProvided for backward compatibility, use self.text_resource directly instead.\nSource code in `llama_index/core/schema.py`\n```\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n    Get the text content for the node if available.\n\n    Provided for backward compatibility, use self.text_resource directly instead.\n    \"\"\"\n    if self.text_resource:\n        metadata_str = self.get_metadata_str(metadata_mode)\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text_resource.text or \"\"\n\n        return self.text_template.format(\n            content=self.text_resource.text or \"\",\n            metadata_str=metadata_str,\n        ).strip()\n    return \"\"\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the text content of the node.\nProvided for backward compatibility, set self.text_resource instead.\nSource code in `llama_index/core/schema.py`\n```\n659\n660\n661\n662\n663\n664\n665\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"\n    Set the text content of the node.\n\n    Provided for backward compatibility, set self.text_resource instead.\n    \"\"\"\n    self.text_resource = MediaResource(text=value)\n\n```\n  \n---|---  \n##  TextNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode \"Permanent link\")\nBases: \nProvided for backward compatibility.\nNote: we keep the field with the typo \"seperator\" to maintain backward compatibility for serialized objects.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text` |  Text content of the node.  \n`mimetype` |  MIME type of the node content. |  `'text/plain'`  \n`start_char_idx` |  `int | None` |  Start char index of the node. |  `None`  \n`end_char_idx` |  `int | None` |  End char index of the node. |  `None`  \n`metadata_seperator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \n`text_template` |  Template for how text is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n```\n| ```\nclass TextNode(BaseNode):\n\"\"\"\n    Provided for backward compatibility.\n\n    Note: we keep the field with the typo \"seperator\" to maintain backward compatibility for\n    serialized objects.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make TextNode forward-compatible with Node by supporting 'text_resource' in the constructor.\"\"\"\n        if \"text_resource\" in kwargs:\n            tr = kwargs.pop(\"text_resource\")\n            if isinstance(tr, MediaResource):\n                kwargs[\"text\"] = tr.text\n            else:\n                kwargs[\"text\"] = tr[\"text\"]\n        super().__init__(*args, **kwargs)\n\n    text: str = Field(default=\"\", description=\"Text content of the node.\")\n    mimetype: str = Field(\n        default=\"text/plain\", description=\"MIME type of the node content.\"\n    )\n    start_char_idx: Optional[int] = Field(\n        default=None, description=\"Start char index of the node.\"\n    )\n    end_char_idx: Optional[int] = Field(\n        default=None, description=\"End char index of the node.\"\n    )\n    metadata_seperator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TextNode\"\n\n    @property\n    def hash(self) -> str:\n        doc_identity = str(self.text) + str(self.metadata)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.TEXT\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n        metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text\n\n        return self.text_template.format(\n            content=self.text, metadata_str=metadata_str\n        ).strip()\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_seperator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    def set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n        self.text = value\n\n    def get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n        return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n    def get_text(self) -> str:\n        return self.get_content(metadata_mode=MetadataMode.NONE)\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'node_info' is deprecated, use 'get_node_info' instead.\",\n    )\n    def node_info(self) -> Dict[str, Any]:\n\"\"\"Deprecated: Get node info.\"\"\"\n        return self.get_node_info()\n\n```\n  \n---|---  \n###  node_info `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.node_info \"Permanent link\")\n```\nnode_info: [, ]\n\n```\n\nDeprecated: Get node info.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n740\n741\n742\n743\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.TEXT\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n745\n746\n747\n748\n749\n750\n751\n752\n753\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n    metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n    if metadata_mode == MetadataMode.NONE or not metadata_str:\n        return self.text\n\n    return self.text_template.format(\n        content=self.text, metadata_str=metadata_str\n    ).strip()\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_seperator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n778\n779\n780\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n    self.text = value\n\n```\n  \n---|---  \n###  get_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_node_info \"Permanent link\")\n```\nget_node_info() -> [, ]\n\n```\n\nGet node info.\nSource code in `llama_index/core/schema.py`\n```\n782\n783\n784\n```\n| ```\ndef get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n    return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n```\n  \n---|---  \n##  ImageNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode \"Permanent link\")\nBases: \nNode with image.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`image` |  `str | None` |  `None`  \n`image_path` |  `str | None` |  `None`  \n`image_url` |  `str | None` |  `None`  \n`image_mimetype` |  `str | None` |  `None`  \n`text_embedding` |  `List[float] | None` |  Text embedding of image node, if text field is filled out |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n```\n| ```\nclass ImageNode(TextNode):\n\"\"\"Node with image.\"\"\"\n\n    # TODO: store reference instead of actual image\n    # base64 encoded image str\n    image: Optional[str] = None\n    image_path: Optional[str] = None\n    image_url: Optional[str] = None\n    image_mimetype: Optional[str] = None\n    text_embedding: Optional[List[float]] = Field(\n        default=None,\n        description=\"Text embedding of image node, if text field is filled out\",\n    )\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make ImageNode forward-compatible with Node by supporting 'image_resource' in the constructor.\"\"\"\n        if \"image_resource\" in kwargs:\n            ir = kwargs.pop(\"image_resource\")\n            if isinstance(ir, MediaResource):\n                kwargs[\"image_path\"] = ir.path.as_posix() if ir.path else None\n                kwargs[\"image_url\"] = ir.url\n                kwargs[\"image_mimetype\"] = ir.mimetype\n            else:\n                kwargs[\"image_path\"] = ir.get(\"path\", None)\n                kwargs[\"image_url\"] = ir.get(\"url\", None)\n                kwargs[\"image_mimetype\"] = ir.get(\"mimetype\", None)\n\n        mimetype = kwargs.get(\"image_mimetype\")\n        if not mimetype and kwargs.get(\"image_path\") is not None:\n            # guess mimetype from image_path\n            extension = Path(kwargs[\"image_path\"]).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                kwargs[\"image_mimetype\"] = ftype.mime\n\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.IMAGE\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageNode\"\n\n    def resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n        if self.image is not None:\n            import base64\n\n            return BytesIO(base64.b64decode(self.image))\n        elif self.image_path is not None:\n            return self.image_path\n        elif self.image_url is not None:\n            # load image from URL\n            import requests\n\n            response = requests.get(self.image_url, timeout=(60, 60))\n            return BytesIO(response.content)\n        else:\n            raise ValueError(\"No image found in node.\")\n\n    @property\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n        # doc identity depends on if image, image_path, or image_url is set\n        image_str = self.image or \"None\"\n        image_path_str = self.image_path or \"None\"\n        image_url_str = self.image_url or \"None\"\n        image_text = self.text or \"None\"\n        doc_identity = f\"{image_str}-{image_path_str}-{image_url_str}-{image_text}\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.resolve_image \"Permanent link\")\n```\nresolve_image() -> ImageType\n\n```\n\nResolve an image such that PIL can read it.\nSource code in `llama_index/core/schema.py`\n```\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n```\n| ```\ndef resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n    if self.image is not None:\n        import base64\n\n        return BytesIO(base64.b64decode(self.image))\n    elif self.image_path is not None:\n        return self.image_path\n    elif self.image_url is not None:\n        # load image from URL\n        import requests\n\n        response = requests.get(self.image_url, timeout=(60, 60))\n        return BytesIO(response.content)\n    else:\n        raise ValueError(\"No image found in node.\")\n\n```\n  \n---|---  \n##  IndexNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode \"Permanent link\")\nBases: \nNode with reference to any object.\nThis can include other indices, query engines, retrievers.\nThis can also include other nodes (though this is overlapping with `relationships` on the Node class).\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`index_id` |  _required_  \n`obj` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n```\n| ```\nclass IndexNode(TextNode):\n\"\"\"\n    Node with reference to any object.\n\n    This can include other indices, query engines, retrievers.\n\n    This can also include other nodes (though this is overlapping with `relationships`\n    on the Node class).\n\n    \"\"\"\n\n    index_id: str\n    obj: Any = None\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        from llama_index.core.storage.docstore.utils import doc_to_json\n\n        data = super().dict(**kwargs)\n\n        try:\n            if self.obj is None:\n                data[\"obj\"] = None\n            elif isinstance(self.obj, BaseNode):\n                data[\"obj\"] = doc_to_json(self.obj)\n            elif isinstance(self.obj, BaseModel):\n                data[\"obj\"] = self.obj.model_dump()\n            else:\n                data[\"obj\"] = json.dumps(self.obj)\n        except Exception:\n            raise ValueError(\"IndexNode obj is not serializable: \" + str(self.obj))\n\n        return data\n\n    @classmethod\n    def from_text_node(\n        cls,\n        node: TextNode,\n        index_id: str,\n    ) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n        # copy all attributes from text node, add index id\n        return cls(\n            **node.dict(),\n            index_id=index_id,\n        )\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        output = super().from_dict(data, **kwargs)\n\n        obj = data.get(\"obj\")\n        parsed_obj = None\n\n        if isinstance(obj, str):\n            parsed_obj = TextNode(text=obj)\n        elif isinstance(obj, dict):\n            from llama_index.core.storage.docstore.utils import json_to_doc\n\n            # check if its a node, else assume stringable\n            try:\n                parsed_obj = json_to_doc(obj)  # type: ignore[assignment]\n            except Exception:\n                parsed_obj = TextNode(text=str(obj))\n\n        output.obj = parsed_obj\n\n        return output\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.INDEX\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"IndexNode\"\n\n```\n  \n---|---  \n###  from_text_node `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode.from_text_node \"Permanent link\")\n```\nfrom_text_node(node: , index_id: ) -> \n\n```\n\nCreate index node from text node.\nSource code in `llama_index/core/schema.py`\n```\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n```\n| ```\n@classmethod\ndef from_text_node(\n    cls,\n    node: TextNode,\n    index_id: str,\n) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n    # copy all attributes from text node, add index id\n    return cls(\n        **node.dict(),\n        index_id=index_id,\n    )\n\n```\n  \n---|---  \n##  NodeWithScore [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node` |  |  _required_  \n`score` |  `float | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n```\n| ```\nclass NodeWithScore(BaseComponent):\n    node: SerializeAsAny[BaseNode]\n    score: Optional[float] = None\n\n    def __str__(self) -> str:\n        score_str = \"None\" if self.score is None else f\"{self.score: 0.3f}\"\n        return f\"{self.node}\\nScore: {score_str}\\n\"\n\n    def get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n        if self.score is None:\n            if raise_error:\n                raise ValueError(\"Score not set.\")\n            else:\n                return 0.0\n        else:\n            return self.score\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"NodeWithScore\"\n\n    ##### pass through methods to BaseNode #####\n    @property\n    def node_id(self) -> str:\n        return self.node.node_id\n\n    @property\n    def id_(self) -> str:\n        return self.node.id_\n\n    @property\n    def text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.text\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        return self.node.metadata\n\n    @property\n    def embedding(self) -> Optional[List[float]]:\n        return self.node.embedding\n\n    def get_text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.get_text()\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n        return self.node.get_content(metadata_mode=metadata_mode)\n\n    def get_embedding(self) -> List[float]:\n        return self.node.get_embedding()\n\n```\n  \n---|---  \n###  get_score [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore.get_score \"Permanent link\")\n```\nget_score(raise_error:  = False) -> float\n\n```\n\nGet score.\nSource code in `llama_index/core/schema.py`\n```\n958\n959\n960\n961\n962\n963\n964\n965\n966\n```\n| ```\ndef get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n    if self.score is None:\n        if raise_error:\n            raise ValueError(\"Score not set.\")\n        else:\n            return 0.0\n    else:\n        return self.score\n\n```\n  \n---|---  \n##  Document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document \"Permanent link\")\nBases: \nGeneric interface for a data document.\nThis document connects to data sources.\nSource code in `llama_index/core/schema.py`\n```\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\nclass Document(Node):\n\"\"\"\n    Generic interface for a data document.\n\n    This document connects to data sources.\n    \"\"\"\n\n    def __init__(self, **data: Any) -> None:\n\"\"\"\n        Keeps backward compatibility with old 'Document' versions.\n\n        If 'text' was passed, store it in 'text_resource'.\n        If 'doc_id' was passed, store it in 'id_'.\n        If 'extra_info' was passed, store it in 'metadata'.\n        \"\"\"\n        if \"doc_id\" in data:\n            value = data.pop(\"doc_id\")\n            if \"id_\" in data:\n                msg = \"'doc_id' is deprecated and 'id_' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"id_\"] = value\n\n        if \"extra_info\" in data:\n            value = data.pop(\"extra_info\")\n            if \"metadata\" in data:\n                msg = \"'extra_info' is deprecated and 'metadata' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"metadata\"] = value\n\n        if data.get(\"text\"):\n            text = data.pop(\"text\")\n            if \"text_resource\" in data:\n                text_resource = (\n                    data[\"text_resource\"]\n                    if isinstance(data[\"text_resource\"], MediaResource)\n                    else MediaResource.model_validate(data[\"text_resource\"])\n                )\n                if (text_resource.text or \"\").strip() != text.strip():\n                    msg = (\n                        \"'text' is deprecated and 'text_resource' will be used instead\"\n                    )\n                    logging.warning(msg)\n            else:\n                data[\"text_resource\"] = MediaResource(text=text)\n\n        super().__init__(**data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n        data = super().custom_model_dump(handler, info)\n        exclude_set = set(info.exclude or [])\n        if \"text\" not in exclude_set:\n            data[\"text\"] = self.text\n        return data\n\n    @property\n    def text(self) -> str:\n\"\"\"Provided for backward compatibility, it returns the content of text_resource.\"\"\"\n        return self.get_content()\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n        return ObjectType.DOCUMENT\n\n    @property\n    def doc_id(self) -> str:\n\"\"\"Get document ID.\"\"\"\n        return self.id_\n\n    @doc_id.setter\n    def doc_id(self, id_: str) -> None:\n        self.id_ = id_\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Doc ID: {self.doc_id}\\n{source_text_wrapped}\"\n\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'get_doc_id' is deprecated, access the 'id_' property instead.\",\n    )\n    def get_doc_id(self) -> str:  # pragma: nocover\n        return self.id_\n\n    def to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n        from llama_index.core.bridge.langchain import (\n            Document as LCDocument,  # type: ignore\n        )\n\n        metadata = self.metadata or {}\n        return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n    @classmethod\n    def from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n        if doc.id:\n            return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n        return cls(text=doc.page_content, metadata=doc.metadata)\n\n    def to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n        from haystack import Document as HaystackDocument  # type: ignore\n\n        return HaystackDocument(\n            content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n        )\n\n    @classmethod\n    def from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n        return cls(\n            text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n        )\n\n    def to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n        return {\n            \"doc_id\": self.id_,\n            \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n        }\n\n    @classmethod\n    def from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n        return cls(\n            text=doc[\"data\"][\"content\"],\n            metadata=doc[\"data\"][\"meta_data\"],\n            id_=doc[\"doc_id\"],\n        )\n\n    def to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n        import numpy as np\n        from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n        return MemoryRecord(\n            id=self.id_,\n            text=self.text,\n            additional_metadata=self.get_metadata_str(),\n            embedding=np.array(self.embedding) if self.embedding else None,\n        )\n\n    @classmethod\n    def from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n        return cls(\n            text=doc._text,\n            metadata={\"additional_metadata\": doc._additional_metadata},\n            embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n            id_=doc._id,\n        )\n\n    def to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n        # write document to temp file\n        import tempfile\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(self.text.encode(\"utf-8\"))\n            f.flush()\n            client.embed(f.name)\n\n    @classmethod\n    def example(cls) -> Document:\n        return Document(\n            text=SAMPLE_TEXT,\n            metadata={\"filename\": \"README.md\", \"category\": \"codebase\"},\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Document\"\n\n    def to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n        from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n        return CloudDocument(\n            text=self.text,\n            metadata=self.metadata,\n            excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n            id=self.id_,\n        )\n\n    @classmethod\n    def from_cloud_document(\n        cls,\n        doc: CloudDocument,\n    ) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n        return Document(\n            text=doc.text,\n            metadata=doc.metadata,\n            excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n            id_=doc.id,\n        )\n\n```\n  \n---|---  \n###  text `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.text \"Permanent link\")\n```\ntext: \n\n```\n\nProvided for backward compatibility, it returns the content of text_resource.\n###  doc_id `property` `writable` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.doc_id \"Permanent link\")\n```\ndoc_id: \n\n```\n\nGet document ID.\n###  custom_model_dump [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.custom_model_dump \"Permanent link\")\n```\ncustom_model_dump(handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> [, ]\n\n```\n\nFor full backward compatibility with the text field, we customize the model serializer.\nSource code in `llama_index/core/schema.py`\n```\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n```\n| ```\n@model_serializer(mode=\"wrap\")\ndef custom_model_dump(\n    self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n    data = super().custom_model_dump(handler, info)\n    exclude_set = set(info.exclude or [])\n    if \"text\" not in exclude_set:\n        data[\"text\"] = self.text\n    return data\n\n```\n  \n---|---  \n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Document type.\nSource code in `llama_index/core/schema.py`\n```\n1077\n1078\n1079\n1080\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n    return ObjectType.DOCUMENT\n\n```\n  \n---|---  \n###  to_langchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_langchain_format \"Permanent link\")\n```\nto_langchain_format() -> Document\n\n```\n\nConvert struct to LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n```\n| ```\ndef to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n    from llama_index.core.bridge.langchain import (\n        Document as LCDocument,  # type: ignore\n    )\n\n    metadata = self.metadata or {}\n    return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n```\n  \n---|---  \n###  from_langchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_langchain_format \"Permanent link\")\n```\nfrom_langchain_format(doc: Document) -> \n\n```\n\nConvert struct from LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1116\n1117\n1118\n1119\n1120\n1121\n```\n| ```\n@classmethod\ndef from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n    if doc.id:\n        return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n    return cls(text=doc.page_content, metadata=doc.metadata)\n\n```\n  \n---|---  \n###  to_haystack_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_haystack_format \"Permanent link\")\n```\nto_haystack_format() -> Document\n\n```\n\nConvert struct to Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n```\n| ```\ndef to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n    from haystack import Document as HaystackDocument  # type: ignore\n\n    return HaystackDocument(\n        content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n    )\n\n```\n  \n---|---  \n###  from_haystack_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_haystack_format \"Permanent link\")\n```\nfrom_haystack_format(doc: Document) -> \n\n```\n\nConvert struct from Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1131\n1132\n1133\n1134\n1135\n1136\n```\n| ```\n@classmethod\ndef from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n    return cls(\n        text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n    )\n\n```\n  \n---|---  \n###  to_embedchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_embedchain_format \"Permanent link\")\n```\nto_embedchain_format() -> [, ]\n\n```\n\nConvert struct to EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1138\n1139\n1140\n1141\n1142\n1143\n```\n| ```\ndef to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n    return {\n        \"doc_id\": self.id_,\n        \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n    }\n\n```\n  \n---|---  \n###  from_embedchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_embedchain_format \"Permanent link\")\n```\nfrom_embedchain_format(doc: [, ]) -> \n\n```\n\nConvert struct from EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n```\n| ```\n@classmethod\ndef from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n    return cls(\n        text=doc[\"data\"][\"content\"],\n        metadata=doc[\"data\"][\"meta_data\"],\n        id_=doc[\"doc_id\"],\n    )\n\n```\n  \n---|---  \n###  to_semantic_kernel_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_semantic_kernel_format \"Permanent link\")\n```\nto_semantic_kernel_format() -> MemoryRecord\n\n```\n\nConvert struct to Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n```\n| ```\ndef to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n    import numpy as np\n    from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n    return MemoryRecord(\n        id=self.id_,\n        text=self.text,\n        additional_metadata=self.get_metadata_str(),\n        embedding=np.array(self.embedding) if self.embedding else None,\n    )\n\n```\n  \n---|---  \n###  from_semantic_kernel_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_semantic_kernel_format \"Permanent link\")\n```\nfrom_semantic_kernel_format(doc: MemoryRecord) -> \n\n```\n\nConvert struct from Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n```\n| ```\n@classmethod\ndef from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n    return cls(\n        text=doc._text,\n        metadata={\"additional_metadata\": doc._additional_metadata},\n        embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n        id_=doc._id,\n    )\n\n```\n  \n---|---  \n###  to_vectorflow [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_vectorflow \"Permanent link\")\n```\nto_vectorflow(client: ) -> None\n\n```\n\nSend a document to vectorflow, since they don't have a document object.\nSource code in `llama_index/core/schema.py`\n```\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n```\n| ```\ndef to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n    # write document to temp file\n    import tempfile\n\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(self.text.encode(\"utf-8\"))\n        f.flush()\n        client.embed(f.name)\n\n```\n  \n---|---  \n###  to_cloud_document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_cloud_document \"Permanent link\")\n```\nto_cloud_document() -> CloudDocument\n\n```\n\nConvert to LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n```\n| ```\ndef to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n    from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n    return CloudDocument(\n        text=self.text,\n        metadata=self.metadata,\n        excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n        id=self.id_,\n    )\n\n```\n  \n---|---  \n###  from_cloud_document `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_cloud_document \"Permanent link\")\n```\nfrom_cloud_document(doc: CloudDocument) -> \n\n```\n\nConvert from LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\n@classmethod\ndef from_cloud_document(\n    cls,\n    doc: CloudDocument,\n) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n    return Document(\n        text=doc.text,\n        metadata=doc.metadata,\n        excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n        id_=doc.id,\n    )\n\n```\n  \n---|---  \n##  ImageDocument [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument \"Permanent link\")\nBases: \nBackward compatible wrapper around Document containing an image.\nSource code in `llama_index/core/schema.py`\n```\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\nclass ImageDocument(Document):\n\"\"\"Backward compatible wrapper around Document containing an image.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        image = kwargs.pop(\"image\", None)\n        image_path = kwargs.pop(\"image_path\", None)\n        image_url = kwargs.pop(\"image_url\", None)\n        image_mimetype = kwargs.pop(\"image_mimetype\", None)\n        text_embedding = kwargs.pop(\"text_embedding\", None)\n\n        if image:\n            kwargs[\"image_resource\"] = MediaResource(\n                data=image, mimetype=image_mimetype\n            )\n        elif image_path:\n            if not is_image_pil(image_path):\n                raise ValueError(\"The specified file path is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                path=image_path, mimetype=image_mimetype\n            )\n        elif image_url:\n            if not is_image_url_pil(image_url):\n                raise ValueError(\"The specified URL is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                url=image_url, mimetype=image_mimetype\n            )\n\n        super().__init__(**kwargs)\n\n    @property\n    def image(self) -> str | None:\n        if self.image_resource and self.image_resource.data:\n            return self.image_resource.data.decode(\"utf-8\")\n        return None\n\n    @image.setter\n    def image(self, image: str) -> None:\n        self.image_resource = MediaResource(data=image.encode(\"utf-8\"))\n\n    @property\n    def image_path(self) -> str | None:\n        if self.image_resource and self.image_resource.path:\n            return str(self.image_resource.path)\n        return None\n\n    @image_path.setter\n    def image_path(self, image_path: str) -> None:\n        self.image_resource = MediaResource(path=Path(image_path))\n\n    @property\n    def image_url(self) -> str | None:\n        if self.image_resource and self.image_resource.url:\n            return str(self.image_resource.url)\n        return None\n\n    @image_url.setter\n    def image_url(self, image_url: str) -> None:\n        self.image_resource = MediaResource(url=AnyUrl(url=image_url))\n\n    @property\n    def image_mimetype(self) -> str | None:\n        if self.image_resource:\n            return self.image_resource.mimetype\n        return None\n\n    @image_mimetype.setter\n    def image_mimetype(self, image_mimetype: str) -> None:\n        if self.image_resource:\n            self.image_resource.mimetype = image_mimetype\n\n    @property\n    def text_embedding(self) -> list[float] | None:\n        if self.text_resource and self.text_resource.embeddings:\n            return self.text_resource.embeddings.get(\"dense\")\n        return None\n\n    @text_embedding.setter\n    def text_embedding(self, embeddings: list[float]) -> None:\n        if self.text_resource:\n            if self.text_resource.embeddings is None:\n                self.text_resource.embeddings = {}\n            self.text_resource.embeddings[\"dense\"] = embeddings\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageDocument\"\n\n    def resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n        Resolve an image such that PIL can read it.\n\n        Args:\n            as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n        \"\"\"\n        if self.image_resource is None:\n            return BytesIO()\n\n        if self.image_resource.data is not None:\n            if as_base64:\n                return BytesIO(self.image_resource.data)\n            return BytesIO(base64.b64decode(self.image_resource.data))\n        elif self.image_resource.path is not None:\n            img_bytes = self.image_resource.path.read_bytes()\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        elif self.image_resource.url is not None:\n            # load image from URL\n            response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n            img_bytes = response.content\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        else:\n            raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument.resolve_image \"Permanent link\")\n```\nresolve_image(as_base64:  = False) -> BytesIO\n\n```\n\nResolve an image such that PIL can read it.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`as_base64` |  `bool` |  whether the resolved image should be returned as base64-encoded bytes |  `False`  \nSource code in `llama_index/core/schema.py`\n```\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\ndef resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n    Resolve an image such that PIL can read it.\n\n    Args:\n        as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n    \"\"\"\n    if self.image_resource is None:\n        return BytesIO()\n\n    if self.image_resource.data is not None:\n        if as_base64:\n            return BytesIO(self.image_resource.data)\n        return BytesIO(base64.b64decode(self.image_resource.data))\n    elif self.image_resource.path is not None:\n        img_bytes = self.image_resource.path.read_bytes()\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    elif self.image_resource.url is not None:\n        # load image from URL\n        response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n        img_bytes = response.content\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    else:\n        raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n##  QueryBundle `dataclass` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle \"Permanent link\")\nBases: `DataClassJsonMixin`\nQuery bundle.\nThis dataclass contains the original query string and associated transformations.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`query_str` |  the original user-specified query string. This is currently used by all non embedding-based queries. |  _required_  \n`custom_embedding_strs` |  `list[str]` |  list of strings used for embedding the query. This is currently used by all embedding-based queries. |  `None`  \n`embedding` |  `list[float]` |  the stored embedding for the query. |  `None`  \n`image_path` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n```\n| ```\n@dataclass\nclass QueryBundle(DataClassJsonMixin):\n\"\"\"\n    Query bundle.\n\n    This dataclass contains the original query string and associated transformations.\n\n    Args:\n        query_str (str): the original user-specified query string.\n            This is currently used by all non embedding-based queries.\n        custom_embedding_strs (list[str]): list of strings used for embedding the query.\n            This is currently used by all embedding-based queries.\n        embedding (list[float]): the stored embedding for the query.\n\n    \"\"\"\n\n    query_str: str\n    # using single image path as query input\n    image_path: Optional[str] = None\n    custom_embedding_strs: Optional[List[str]] = None\n    embedding: Optional[List[float]] = None\n\n    @property\n    def embedding_strs(self) -> List[str]:\n\"\"\"Use custom embedding strs if specified, otherwise use query str.\"\"\"\n        if self.custom_embedding_strs is None:\n            if len(self.query_str) == 0:\n                return []\n            return [self.query_str]\n        else:\n            return self.custom_embedding_strs\n\n    @property\n    def embedding_image(self) -> List[ImageType]:\n\"\"\"Use image path for image retrieval.\"\"\"\n        if self.image_path is None:\n            return []\n        return [self.image_path]\n\n    def __str__(self) -> str:\n\"\"\"Convert to string representation.\"\"\"\n        return self.query_str\n\n```\n  \n---|---  \n###  embedding_strs `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_strs \"Permanent link\")\n```\nembedding_strs: []\n\n```\n\nUse custom embedding strs if specified, otherwise use query str.\n###  embedding_image `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_image \"Permanent link\")\n```\nembedding_image: [ImageType]\n\n```\n\nUse image path for image retrieval.\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "# Index\nBase schema for data structures.\n##  BaseComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent \"Permanent link\")\nBases: `BaseModel`\nBase component object to capture class names.\nSource code in `llama_index/core/schema.py`\n```\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n```\n| ```\nclass BaseComponent(BaseModel):\n\"\"\"Base component object to capture class names.\"\"\"\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)\n        json_schema = handler.resolve_ref_schema(json_schema)\n\n        # inject class name to help with serde\n        if \"properties\" in json_schema:\n            json_schema[\"properties\"][\"class_name\"] = {\n                \"title\": \"Class Name\",\n                \"type\": \"string\",\n                \"default\": cls.class_name(),\n            }\n        return json_schema\n\n    @classmethod\n    def class_name(cls) -> str:\n\"\"\"\n        Get the class name, used as a unique ID in serialization.\n\n        This provides a key that makes serialization robust against actual class\n        name changes.\n        \"\"\"\n        return \"base_component\"\n\n    def json(self, **kwargs: Any) -> str:\n        return self.to_json(**kwargs)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n        data = handler(self)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        return self.model_dump(**kwargs)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = super().__getstate__()\n\n        # remove attributes that are not pickleable -- kind of dangerous\n        keys_to_remove = []\n        for key, val in state[\"__dict__\"].items():\n            try:\n                pickle.dumps(val)\n            except Exception:\n                keys_to_remove.append(key)\n\n        for key in keys_to_remove:\n            logging.warning(f\"Removing unpickleable attribute {key}\")\n            del state[\"__dict__\"][key]\n\n        # remove private attributes if they aren't pickleable -- kind of dangerous\n        keys_to_remove = []\n        private_attrs = state.get(\"__pydantic_private__\", None)\n        if private_attrs:\n            for key, val in state[\"__pydantic_private__\"].items():\n                try:\n                    pickle.dumps(val)\n                except Exception:\n                    keys_to_remove.append(key)\n\n            for key in keys_to_remove:\n                logging.warning(f\"Removing unpickleable private attribute {key}\")\n                del state[\"__pydantic_private__\"][key]\n\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        # Use the __dict__ and __init__ method to set state\n        # so that all variables initialize\n        try:\n            self.__init__(**state[\"__dict__\"])  # type: ignore\n        except Exception:\n            # Fall back to the default __setstate__ method\n            # This may not work if the class had unpickleable attributes\n            super().__setstate__(state)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = self.dict(**kwargs)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def to_json(self, **kwargs: Any) -> str:\n        data = self.to_dict(**kwargs)\n        return json.dumps(data)\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        # In SimpleKVStore we rely on shallow coping. Hence, the data will be modified in the store directly.\n        # And it is the same when the user is passing a dictionary to create a component. We can't modify the passed down dictionary.\n        data = dict(data)\n        if isinstance(kwargs, dict):\n            data.update(kwargs)\n        data.pop(\"class_name\", None)\n        return cls(**data)\n\n    @classmethod\n    def from_json(cls, data_str: str, **kwargs: Any) -> Self:  # type: ignore\n        data = json.loads(data_str)\n        return cls.from_dict(data, **kwargs)\n\n```\n  \n---|---  \n###  class_name `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseComponent.class_name \"Permanent link\")\n```\nclass_name() -> \n\n```\n\nGet the class name, used as a unique ID in serialization.\nThis provides a key that makes serialization robust against actual class name changes.\nSource code in `llama_index/core/schema.py`\n```\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n```\n| ```\n@classmethod\ndef class_name(cls) -> str:\n\"\"\"\n    Get the class name, used as a unique ID in serialization.\n\n    This provides a key that makes serialization robust against actual class\n    name changes.\n    \"\"\"\n    return \"base_component\"\n\n```\n  \n---|---  \n##  TransformComponent [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent \"Permanent link\")\nBases: , `DispatcherSpanMixin`\nBase class for transform components.\nSource code in `llama_index/core/schema.py`\n```\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n```\n| ```\nclass TransformComponent(BaseComponent, DispatcherSpanMixin):\n\"\"\"Base class for transform components.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:\n\"\"\"Transform nodes.\"\"\"\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n        return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n###  acall `async` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TransformComponent.acall \"Permanent link\")\n```\nacall(nodes: Sequence[], **kwargs: ) -> Sequence[]\n\n```\n\nAsync transform nodes.\nSource code in `llama_index/core/schema.py`\n```\n199\n200\n201\n202\n203\n```\n| ```\nasync def acall(\n    self, nodes: Sequence[BaseNode], **kwargs: Any\n) -> Sequence[BaseNode]:\n\"\"\"Async transform nodes.\"\"\"\n    return self.__call__(nodes, **kwargs)\n\n```\n  \n---|---  \n##  NodeRelationship [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeRelationship \"Permanent link\")\nBases: `str`, `Enum`\nNode relationships used in `BaseNode` class.\nAttributes:\nName | Type | Description  \n---|---|---  \n`SOURCE` |  The node is the source document.  \n`PREVIOUS` |  The node is the previous node in the document.  \nThe node is the next node in the document.  \n`PARENT` |  The node is the parent node in the document.  \n`CHILD` |  The node is a child node in the document.  \nSource code in `llama_index/core/schema.py`\n```\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n```\n| ```\nclass NodeRelationship(str, Enum):\n\"\"\"\n    Node relationships used in `BaseNode` class.\n\n    Attributes:\n        SOURCE: The node is the source document.\n        PREVIOUS: The node is the previous node in the document.\n        NEXT: The node is the next node in the document.\n        PARENT: The node is the parent node in the document.\n        CHILD: The node is a child node in the document.\n\n    \"\"\"\n\n    SOURCE = auto()\n    PREVIOUS = auto()\n    NEXT = auto()\n    PARENT = auto()\n    CHILD = auto()\n\n```\n  \n---|---  \n##  RelatedNodeInfo [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.RelatedNodeInfo \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node_id` |  _required_  \n`node_type` |  `Annotated[ObjectType, PlainSerializer] | str | None` |  `None`  \n`hash` |  `str | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n248\n249\n250\n251\n252\n253\n254\n255\n256\n```\n| ```\nclass RelatedNodeInfo(BaseComponent):\n    node_id: str\n    node_type: Annotated[ObjectType, EnumNameSerializer] | str | None = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    hash: Optional[str] = None\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"RelatedNodeInfo\"\n\n```\n  \n---|---  \n##  BaseNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode \"Permanent link\")\nBases: \nBase node Object.\nGeneric abstract interface for retrievable nodes\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`id_` |  Unique ID of the node. |  `'f64e9cec-5009-4429-bf4d-fa73983e3f01'`  \n`embedding` |  `List[float] | None` |  Embedding of the node. |  `None`  \n`excluded_embed_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the embed model. |  `<dynamic>`  \n`excluded_llm_metadata_keys` |  `List[str]` |  Metadata keys that are excluded from text for the LLM. |  `<dynamic>`  \n`metadata_template` |  Template for how metadata is formatted, with {key} and {value} placeholders. |  `'{key}: {value}'`  \n`metadata_separator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \nSource code in `llama_index/core/schema.py`\n```\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\nclass BaseNode(BaseComponent):\n\"\"\"\n    Base node Object.\n\n    Generic abstract interface for retrievable nodes\n\n    \"\"\"\n\n    # hash is computed on local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\"\n    )\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"Embedding of the node.\"\n    )\n\n\"\"\"\"\n    metadata fields\n    - injected as part of the text shown to LLMs as context\n    - injected as part of the text for generating embeddings\n    - used by vector DBs for metadata filtering\n\n    \"\"\"\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"A flat dictionary of metadata fields\",\n        alias=\"extra_info\",\n    )\n    excluded_embed_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the embed model.\",\n    )\n    excluded_llm_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the LLM.\",\n    )\n    relationships: Dict[\n        Annotated[NodeRelationship, EnumNameSerializer],\n        RelatedNodeType,\n    ] = Field(\n        default_factory=dict,\n        description=\"A mapping of relationships to other node information.\",\n    )\n    metadata_template: str = Field(\n        default=DEFAULT_METADATA_TMPL,\n        description=(\n            \"Template for how metadata is formatted, with {key} and \"\n            \"{value} placeholders.\"\n        ),\n    )\n    metadata_separator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n        alias=\"metadata_seperator\",\n    )\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n    @abstractmethod\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_separator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    @abstractmethod\n    def set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n    @property\n    @abstractmethod\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n\n    @property\n    def node_id(self) -> str:\n        return self.id_\n\n    @node_id.setter\n    def node_id(self, value: str) -> None:\n        self.id_ = value\n\n    @property\n    def source_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"\n        Source object node.\n\n        Extracted from the relationships field.\n\n        \"\"\"\n        if NodeRelationship.SOURCE not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.SOURCE]\n        if isinstance(relation, list):\n            raise ValueError(\"Source object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def prev_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Prev node.\"\"\"\n        if NodeRelationship.PREVIOUS not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PREVIOUS]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Previous object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def next_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Next node.\"\"\"\n        if NodeRelationship.NEXT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.NEXT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Next object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def parent_node(self) -> Optional[RelatedNodeInfo]:\n\"\"\"Parent node.\"\"\"\n        if NodeRelationship.PARENT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PARENT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Parent object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def child_nodes(self) -> Optional[List[RelatedNodeInfo]]:\n\"\"\"Child nodes.\"\"\"\n        if NodeRelationship.CHILD not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.CHILD]\n        if not isinstance(relation, list):\n            raise ValueError(\"Child objects must be a list of RelatedNodeInfo objects.\")\n        return relation\n\n    @property\n    def ref_doc_id(self) -> Optional[str]:  # pragma: no cover\n\"\"\"Deprecated: Get ref doc id.\"\"\"\n        source_node = self.source_node\n        if source_node is None:\n            return None\n        return source_node.node_id\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self) -> dict[str, Any]:  # pragma: no coverde\n        return self.metadata\n\n    @extra_info.setter\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self, extra_info: dict[str, Any]) -> None:  # pragma: no coverde\n        self.metadata = extra_info\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Node ID: {self.node_id}\\n{source_text_wrapped}\"\n\n    def get_embedding(self) -> List[float]:\n\"\"\"\n        Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    def as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n        return RelatedNodeInfo(\n            node_id=self.node_id,\n            node_type=self.get_type(),\n            metadata=self.metadata,\n            hash=self.hash,\n        )\n\n```\n  \n---|---  \n###  embedding `class-attribute` `instance-attribute` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.embedding \"Permanent link\")\n```\nembedding: Optional[[float]] = (default=None, description='Embedding of the node.')\n\n```\n\n\" metadata fields - injected as part of the text shown to LLMs as context - injected as part of the text for generating embeddings - used by vector DBs for metadata filtering\n###  hash `abstractmethod` `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  source_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.source_node \"Permanent link\")\n```\nsource_node: Optional[]\n\n```\n\nSource object node.\nExtracted from the relationships field.\n###  prev_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.prev_node \"Permanent link\")\n```\nprev_node: Optional[]\n\n```\n\nPrev node.\n###  next_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.next_node \"Permanent link\")\n```\nnext_node: Optional[]\n\n```\n\nNext node.\n###  parent_node `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.parent_node \"Permanent link\")\n```\nparent_node: Optional[]\n\n```\n\nParent node.\n###  child_nodes `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.child_nodes \"Permanent link\")\n```\nchild_nodes: Optional[[]]\n\n```\n\nChild nodes.\n###  ref_doc_id `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.ref_doc_id \"Permanent link\")\n```\nref_doc_id: Optional[]\n\n```\n\nDeprecated: Get ref doc id.\n###  get_type `abstractmethod` `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n321\n322\n323\n324\n```\n| ```\n@classmethod\n@abstractmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n\n```\n  \n---|---  \n###  get_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n326\n327\n328\n```\n| ```\n@abstractmethod\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Get object content.\"\"\"\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_separator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content `abstractmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n353\n354\n355\n```\n| ```\n@abstractmethod\ndef set_content(self, value: Any) -> None:\n\"\"\"Set the content of the node.\"\"\"\n\n```\n  \n---|---  \n###  get_embedding [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.get_embedding \"Permanent link\")\n```\nget_embedding() -> [float]\n\n```\n\nGet embedding.\nErrors if embedding is None.\nSource code in `llama_index/core/schema.py`\n```\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n```\n| ```\ndef get_embedding(self) -> List[float]:\n\"\"\"\n    Get embedding.\n\n    Errors if embedding is None.\n\n    \"\"\"\n    if self.embedding is None:\n        raise ValueError(\"embedding not set.\")\n    return self.embedding\n\n```\n  \n---|---  \n###  as_related_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.BaseNode.as_related_node_info \"Permanent link\")\n```\nas_related_node_info() -> \n\n```\n\nGet node as RelatedNodeInfo.\nSource code in `llama_index/core/schema.py`\n```\n474\n475\n476\n477\n478\n479\n480\n481\n```\n| ```\ndef as_related_node_info(self) -> RelatedNodeInfo:\n\"\"\"Get node as RelatedNodeInfo.\"\"\"\n    return RelatedNodeInfo(\n        node_id=self.node_id,\n        node_type=self.get_type(),\n        metadata=self.metadata,\n        hash=self.hash,\n    )\n\n```\n  \n---|---  \n##  MediaResource [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"Permanent link\")\nBases: `BaseModel`\nA container class for media content.\nThis class represents a generic media resource that can be stored and accessed in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports storing vector embeddings for the media content.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`embeddings` |  `dict[Literal['sparse', 'dense'], list[float]] | None` |  Vector representation of this resource. |  `None`  \n`data` |  `bytes | None` |  base64 binary representation of this resource. |  `None`  \n`text` |  `str | None` |  Text representation of this resource. |  `None`  \n`path` |  `Path | None` |  Filesystem path of this resource. |  `None`  \n`url` |  `AnyUrl | None` |  URL to reach this resource. |  `None`  \n`mimetype` |  `str | None` |  MIME type of this resource. |  `None`  \nAttributes:\nName | Type | Description  \n---|---|---  \n`embeddings` |  Multi-vector dict representation of this resource for embedding-based search/retrieval  \n`text` |  Plain text representation of this resource  \n`data` |  Raw binary data of the media content  \n`mimetype` |  The MIME type indicating the format/type of the media content  \n`path` |  Local filesystem path where the media content can be accessed  \nURL where the media content can be accessed remotely  \nSource code in `llama_index/core/schema.py`\n```\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n```\n| ```\nclass MediaResource(BaseModel):\n\"\"\"\n    A container class for media content.\n\n    This class represents a generic media resource that can be stored and accessed\n    in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports\n    storing vector embeddings for the media content.\n\n    Attributes:\n        embeddings: Multi-vector dict representation of this resource for embedding-based search/retrieval\n        text: Plain text representation of this resource\n        data: Raw binary data of the media content\n        mimetype: The MIME type indicating the format/type of the media content\n        path: Local filesystem path where the media content can be accessed\n        url: URL where the media content can be accessed remotely\n\n    \"\"\"\n\n    embeddings: dict[EmbeddingKind, list[float]] | None = Field(\n        default=None, description=\"Vector representation of this resource.\"\n    )\n    data: bytes | None = Field(\n        default=None,\n        exclude=True,\n        description=\"base64 binary representation of this resource.\",\n    )\n    text: str | None = Field(\n        default=None, description=\"Text representation of this resource.\"\n    )\n    path: Path | None = Field(\n        default=None, description=\"Filesystem path of this resource.\"\n    )\n    url: AnyUrl | None = Field(default=None, description=\"URL to reach this resource.\")\n    mimetype: str | None = Field(\n        default=None, description=\"MIME type of this resource.\"\n    )\n\n    model_config = {\n        # This ensures validation runs even for None values\n        \"validate_default\": True\n    }\n\n    @field_validator(\"data\", mode=\"after\")\n    @classmethod\n    def validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n        If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n        In case the model was built passing binary data but without a mimetype,\n        we try to guess it using the filetype library. To avoid resource-intense\n        operations, we won't load the path or the URL to guess the mimetype.\n        \"\"\"\n        if v is None:\n            return v\n\n        try:\n            # Check if data is already base64 encoded.\n            # b64decode() can succeed on random binary data, so we\n            # pass verify=True to make sure it's not a false positive\n            decoded = base64.b64decode(v, validate=True)\n        except BinasciiError:\n            # b64decode failed, return encoded\n            return base64.b64encode(v)\n\n        # Good as is, return unchanged\n        return v\n\n    @field_validator(\"mimetype\", mode=\"after\")\n    @classmethod\n    def validate_mimetype(cls, v: str | None, info: ValidationInfo) -> str | None:\n        if v is not None:\n            return v\n\n        # Since this field validator runs after the one for `data`\n        # then the contents of `data` should be encoded already\n        b64_data = info.data.get(\"data\")\n        if b64_data:  # encoded bytes\n            decoded_data = base64.b64decode(b64_data)\n            if guess := filetype.guess(decoded_data):\n                return guess.mime\n\n        # guess from path\n        rpath: str | None = info.data[\"path\"]\n        if rpath:\n            extension = Path(rpath).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                return ftype.mime\n\n        return v\n\n    @field_serializer(\"path\")  # type: ignore\n    def serialize_path(\n        self, path: Optional[Path], _info: ValidationInfo\n    ) -> Optional[str]:\n        if path is None:\n            return path\n        return str(path)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash to uniquely identify the media resource.\n\n        The hash is generated based on the available content (data, path, text or url).\n        Returns an empty string if no content is available.\n        \"\"\"\n        bits: list[str] = []\n        if self.text is not None:\n            bits.append(self.text)\n        if self.data is not None:\n            # Hash the binary data if available\n            bits.append(str(sha256(self.data).hexdigest()))\n        if self.path is not None:\n            # Hash the file path if provided\n            bits.append(str(sha256(str(self.path).encode(\"utf-8\")).hexdigest()))\n        if self.url is not None:\n            # Use the URL string as basis for hash\n            bits.append(str(sha256(str(self.url).encode(\"utf-8\")).hexdigest()))\n\n        doc_identity = \"\".join(bits)\n        if not doc_identity:\n            return \"\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash to uniquely identify the media resource.\nThe hash is generated based on the available content (data, path, text or url). Returns an empty string if no content is available.\n###  validate_data `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource.validate_data \"Permanent link\")\n```\nvalidate_data(v: bytes | None, info: ValidationInfo) -> bytes | None\n\n```\n\nIf binary data was passed, store the resource as base64 and guess the mimetype when possible.\nIn case the model was built passing binary data but without a mimetype, we try to guess it using the filetype library. To avoid resource-intense operations, we won't load the path or the URL to guess the mimetype.\nSource code in `llama_index/core/schema.py`\n```\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n```\n| ```\n@field_validator(\"data\", mode=\"after\")\n@classmethod\ndef validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n\"\"\"\n    If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n    In case the model was built passing binary data but without a mimetype,\n    we try to guess it using the filetype library. To avoid resource-intense\n    operations, we won't load the path or the URL to guess the mimetype.\n    \"\"\"\n    if v is None:\n        return v\n\n    try:\n        # Check if data is already base64 encoded.\n        # b64decode() can succeed on random binary data, so we\n        # pass verify=True to make sure it's not a false positive\n        decoded = base64.b64decode(v, validate=True)\n    except BinasciiError:\n        # b64decode failed, return encoded\n        return base64.b64encode(v)\n\n    # Good as is, return unchanged\n    return v\n\n```\n  \n---|---  \n##  Node [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Text content of the node. |  `None`  \n`image_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Image content of the node. |  `None`  \n`audio_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Audio content of the node. |  `None`  \n`video_resource` |  `MediaResource[](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.MediaResource \"llama_index.core.schema.MediaResource\") | None` |  Video content of the node. |  `None`  \n`text_template` |  Template for how text_resource is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n```\n| ```\nclass Node(BaseNode):\n    text_resource: MediaResource | None = Field(\n        default=None, description=\"Text content of the node.\"\n    )\n    image_resource: MediaResource | None = Field(\n        default=None, description=\"Image content of the node.\"\n    )\n    audio_resource: MediaResource | None = Field(\n        default=None, description=\"Audio content of the node.\"\n    )\n    video_resource: MediaResource | None = Field(\n        default=None, description=\"Video content of the node.\"\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text_resource is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Node\"\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.MULTIMODAL\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n        Get the text content for the node if available.\n\n        Provided for backward compatibility, use self.text_resource directly instead.\n        \"\"\"\n        if self.text_resource:\n            metadata_str = self.get_metadata_str(metadata_mode)\n            if metadata_mode == MetadataMode.NONE or not metadata_str:\n                return self.text_resource.text or \"\"\n\n            return self.text_template.format(\n                content=self.text_resource.text or \"\",\n                metadata_str=metadata_str,\n            ).strip()\n        return \"\"\n\n    def set_content(self, value: str) -> None:\n\"\"\"\n        Set the text content of the node.\n\n        Provided for backward compatibility, set self.text_resource instead.\n        \"\"\"\n        self.text_resource = MediaResource(text=value)\n\n    @property\n    def hash(self) -> str:\n\"\"\"\n        Generate a hash representing the state of the node.\n\n        The hash is generated based on the available resources (audio, image, text or video) and its metadata.\n        \"\"\"\n        doc_identities = []\n        metadata_str = self.get_metadata_str(mode=MetadataMode.ALL)\n        if metadata_str:\n            doc_identities.append(metadata_str)\n        if self.audio_resource is not None:\n            doc_identities.append(self.audio_resource.hash)\n        if self.image_resource is not None:\n            doc_identities.append(self.image_resource.hash)\n        if self.text_resource is not None:\n            doc_identities.append(self.text_resource.hash)\n        if self.video_resource is not None:\n            doc_identities.append(self.video_resource.hash)\n\n        doc_identity = \"-\".join(doc_identities)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGenerate a hash representing the state of the node.\nThe hash is generated based on the available resources (audio, image, text or video) and its metadata.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n637\n638\n639\n640\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.MULTIMODAL\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet the text content for the node if available.\nProvided for backward compatibility, use self.text_resource directly instead.\nSource code in `llama_index/core/schema.py`\n```\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"\n    Get the text content for the node if available.\n\n    Provided for backward compatibility, use self.text_resource directly instead.\n    \"\"\"\n    if self.text_resource:\n        metadata_str = self.get_metadata_str(metadata_mode)\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text_resource.text or \"\"\n\n        return self.text_template.format(\n            content=self.text_resource.text or \"\",\n            metadata_str=metadata_str,\n        ).strip()\n    return \"\"\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Node.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the text content of the node.\nProvided for backward compatibility, set self.text_resource instead.\nSource code in `llama_index/core/schema.py`\n```\n659\n660\n661\n662\n663\n664\n665\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"\n    Set the text content of the node.\n\n    Provided for backward compatibility, set self.text_resource instead.\n    \"\"\"\n    self.text_resource = MediaResource(text=value)\n\n```\n  \n---|---  \n##  TextNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode \"Permanent link\")\nBases: \nProvided for backward compatibility.\nNote: we keep the field with the typo \"seperator\" to maintain backward compatibility for serialized objects.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`text` |  Text content of the node.  \n`mimetype` |  MIME type of the node content. |  `'text/plain'`  \n`start_char_idx` |  `int | None` |  Start char index of the node. |  `None`  \n`end_char_idx` |  `int | None` |  End char index of the node. |  `None`  \n`metadata_seperator` |  Separator between metadata fields when converting to string. |  `'\\n'`  \n`text_template` |  Template for how text is formatted, with {content} and {metadata_str} placeholders. |  `'{metadata_str}\\n\\n{content}'`  \nSource code in `llama_index/core/schema.py`\n```\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n```\n| ```\nclass TextNode(BaseNode):\n\"\"\"\n    Provided for backward compatibility.\n\n    Note: we keep the field with the typo \"seperator\" to maintain backward compatibility for\n    serialized objects.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make TextNode forward-compatible with Node by supporting 'text_resource' in the constructor.\"\"\"\n        if \"text_resource\" in kwargs:\n            tr = kwargs.pop(\"text_resource\")\n            if isinstance(tr, MediaResource):\n                kwargs[\"text\"] = tr.text\n            else:\n                kwargs[\"text\"] = tr[\"text\"]\n        super().__init__(*args, **kwargs)\n\n    text: str = Field(default=\"\", description=\"Text content of the node.\")\n    mimetype: str = Field(\n        default=\"text/plain\", description=\"MIME type of the node content.\"\n    )\n    start_char_idx: Optional[int] = Field(\n        default=None, description=\"Start char index of the node.\"\n    )\n    end_char_idx: Optional[int] = Field(\n        default=None, description=\"End char index of the node.\"\n    )\n    metadata_seperator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TextNode\"\n\n    @property\n    def hash(self) -> str:\n        doc_identity = str(self.text) + str(self.metadata)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n        return ObjectType.TEXT\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n        metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n        if metadata_mode == MetadataMode.NONE or not metadata_str:\n            return self.text\n\n        return self.text_template.format(\n            content=self.text, metadata_str=metadata_str\n        ).strip()\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_seperator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    def set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n        self.text = value\n\n    def get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n        return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n    def get_text(self) -> str:\n        return self.get_content(metadata_mode=MetadataMode.NONE)\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'node_info' is deprecated, use 'get_node_info' instead.\",\n    )\n    def node_info(self) -> Dict[str, Any]:\n\"\"\"Deprecated: Get node info.\"\"\"\n        return self.get_node_info()\n\n```\n  \n---|---  \n###  node_info `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.node_info \"Permanent link\")\n```\nnode_info: [, ]\n\n```\n\nDeprecated: Get node info.\n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Object type.\nSource code in `llama_index/core/schema.py`\n```\n740\n741\n742\n743\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Object type.\"\"\"\n    return ObjectType.TEXT\n\n```\n  \n---|---  \n###  get_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_content \"Permanent link\")\n```\nget_content(metadata_mode: MetadataMode = ) -> \n\n```\n\nGet object content.\nSource code in `llama_index/core/schema.py`\n```\n745\n746\n747\n748\n749\n750\n751\n752\n753\n```\n| ```\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n\"\"\"Get object content.\"\"\"\n    metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n    if metadata_mode == MetadataMode.NONE or not metadata_str:\n        return self.text\n\n    return self.text_template.format(\n        content=self.text, metadata_str=metadata_str\n    ).strip()\n\n```\n  \n---|---  \n###  get_metadata_str [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_metadata_str \"Permanent link\")\n```\nget_metadata_str(mode: MetadataMode = ) -> \n\n```\n\nMetadata info string.\nSource code in `llama_index/core/schema.py`\n```\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n```\n| ```\ndef get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n\"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    usable_metadata_keys = set(self.metadata.keys())\n    if mode == MetadataMode.LLM:\n        for key in self.excluded_llm_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n    elif mode == MetadataMode.EMBED:\n        for key in self.excluded_embed_metadata_keys:\n            if key in usable_metadata_keys:\n                usable_metadata_keys.remove(key)\n\n    return self.metadata_seperator.join(\n        [\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in self.metadata.items()\n            if key in usable_metadata_keys\n        ]\n    )\n\n```\n  \n---|---  \n###  set_content [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.set_content \"Permanent link\")\n```\nset_content(value: ) -> None\n\n```\n\nSet the content of the node.\nSource code in `llama_index/core/schema.py`\n```\n778\n779\n780\n```\n| ```\ndef set_content(self, value: str) -> None:\n\"\"\"Set the content of the node.\"\"\"\n    self.text = value\n\n```\n  \n---|---  \n###  get_node_info [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.TextNode.get_node_info \"Permanent link\")\n```\nget_node_info() -> [, ]\n\n```\n\nGet node info.\nSource code in `llama_index/core/schema.py`\n```\n782\n783\n784\n```\n| ```\ndef get_node_info(self) -> Dict[str, Any]:\n\"\"\"Get node info.\"\"\"\n    return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n```\n  \n---|---  \n##  ImageNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode \"Permanent link\")\nBases: \nNode with image.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`image` |  `str | None` |  `None`  \n`image_path` |  `str | None` |  `None`  \n`image_url` |  `str | None` |  `None`  \n`image_mimetype` |  `str | None` |  `None`  \n`text_embedding` |  `List[float] | None` |  Text embedding of image node, if text field is filled out |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n```\n| ```\nclass ImageNode(TextNode):\n\"\"\"Node with image.\"\"\"\n\n    # TODO: store reference instead of actual image\n    # base64 encoded image str\n    image: Optional[str] = None\n    image_path: Optional[str] = None\n    image_url: Optional[str] = None\n    image_mimetype: Optional[str] = None\n    text_embedding: Optional[List[float]] = Field(\n        default=None,\n        description=\"Text embedding of image node, if text field is filled out\",\n    )\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\"\"\"Make ImageNode forward-compatible with Node by supporting 'image_resource' in the constructor.\"\"\"\n        if \"image_resource\" in kwargs:\n            ir = kwargs.pop(\"image_resource\")\n            if isinstance(ir, MediaResource):\n                kwargs[\"image_path\"] = ir.path.as_posix() if ir.path else None\n                kwargs[\"image_url\"] = ir.url\n                kwargs[\"image_mimetype\"] = ir.mimetype\n            else:\n                kwargs[\"image_path\"] = ir.get(\"path\", None)\n                kwargs[\"image_url\"] = ir.get(\"url\", None)\n                kwargs[\"image_mimetype\"] = ir.get(\"mimetype\", None)\n\n        mimetype = kwargs.get(\"image_mimetype\")\n        if not mimetype and kwargs.get(\"image_path\") is not None:\n            # guess mimetype from image_path\n            extension = Path(kwargs[\"image_path\"]).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                kwargs[\"image_mimetype\"] = ftype.mime\n\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.IMAGE\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageNode\"\n\n    def resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n        if self.image is not None:\n            import base64\n\n            return BytesIO(base64.b64decode(self.image))\n        elif self.image_path is not None:\n            return self.image_path\n        elif self.image_url is not None:\n            # load image from URL\n            import requests\n\n            response = requests.get(self.image_url, timeout=(60, 60))\n            return BytesIO(response.content)\n        else:\n            raise ValueError(\"No image found in node.\")\n\n    @property\n    def hash(self) -> str:\n\"\"\"Get hash of node.\"\"\"\n        # doc identity depends on if image, image_path, or image_url is set\n        image_str = self.image or \"None\"\n        image_path_str = self.image_path or \"None\"\n        image_url_str = self.image_url or \"None\"\n        image_text = self.text or \"None\"\n        doc_identity = f\"{image_str}-{image_path_str}-{image_url_str}-{image_text}\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n```\n  \n---|---  \n###  hash `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.hash \"Permanent link\")\n```\nhash: \n\n```\n\nGet hash of node.\n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageNode.resolve_image \"Permanent link\")\n```\nresolve_image() -> ImageType\n\n```\n\nResolve an image such that PIL can read it.\nSource code in `llama_index/core/schema.py`\n```\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n```\n| ```\ndef resolve_image(self) -> ImageType:\n\"\"\"Resolve an image such that PIL can read it.\"\"\"\n    if self.image is not None:\n        import base64\n\n        return BytesIO(base64.b64decode(self.image))\n    elif self.image_path is not None:\n        return self.image_path\n    elif self.image_url is not None:\n        # load image from URL\n        import requests\n\n        response = requests.get(self.image_url, timeout=(60, 60))\n        return BytesIO(response.content)\n    else:\n        raise ValueError(\"No image found in node.\")\n\n```\n  \n---|---  \n##  IndexNode [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode \"Permanent link\")\nBases: \nNode with reference to any object.\nThis can include other indices, query engines, retrievers.\nThis can also include other nodes (though this is overlapping with `relationships` on the Node class).\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`index_id` |  _required_  \n`obj` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n```\n| ```\nclass IndexNode(TextNode):\n\"\"\"\n    Node with reference to any object.\n\n    This can include other indices, query engines, retrievers.\n\n    This can also include other nodes (though this is overlapping with `relationships`\n    on the Node class).\n\n    \"\"\"\n\n    index_id: str\n    obj: Any = None\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        from llama_index.core.storage.docstore.utils import doc_to_json\n\n        data = super().dict(**kwargs)\n\n        try:\n            if self.obj is None:\n                data[\"obj\"] = None\n            elif isinstance(self.obj, BaseNode):\n                data[\"obj\"] = doc_to_json(self.obj)\n            elif isinstance(self.obj, BaseModel):\n                data[\"obj\"] = self.obj.model_dump()\n            else:\n                data[\"obj\"] = json.dumps(self.obj)\n        except Exception:\n            raise ValueError(\"IndexNode obj is not serializable: \" + str(self.obj))\n\n        return data\n\n    @classmethod\n    def from_text_node(\n        cls,\n        node: TextNode,\n        index_id: str,\n    ) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n        # copy all attributes from text node, add index id\n        return cls(\n            **node.dict(),\n            index_id=index_id,\n        )\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        output = super().from_dict(data, **kwargs)\n\n        obj = data.get(\"obj\")\n        parsed_obj = None\n\n        if isinstance(obj, str):\n            parsed_obj = TextNode(text=obj)\n        elif isinstance(obj, dict):\n            from llama_index.core.storage.docstore.utils import json_to_doc\n\n            # check if its a node, else assume stringable\n            try:\n                parsed_obj = json_to_doc(obj)  # type: ignore[assignment]\n            except Exception:\n                parsed_obj = TextNode(text=str(obj))\n\n        output.obj = parsed_obj\n\n        return output\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.INDEX\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"IndexNode\"\n\n```\n  \n---|---  \n###  from_text_node `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.IndexNode.from_text_node \"Permanent link\")\n```\nfrom_text_node(node: , index_id: ) -> \n\n```\n\nCreate index node from text node.\nSource code in `llama_index/core/schema.py`\n```\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n```\n| ```\n@classmethod\ndef from_text_node(\n    cls,\n    node: TextNode,\n    index_id: str,\n) -> IndexNode:\n\"\"\"Create index node from text node.\"\"\"\n    # copy all attributes from text node, add index id\n    return cls(\n        **node.dict(),\n        index_id=index_id,\n    )\n\n```\n  \n---|---  \n##  NodeWithScore [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore \"Permanent link\")\nBases: \nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`node` |  |  _required_  \n`score` |  `float | None` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n```\n| ```\nclass NodeWithScore(BaseComponent):\n    node: SerializeAsAny[BaseNode]\n    score: Optional[float] = None\n\n    def __str__(self) -> str:\n        score_str = \"None\" if self.score is None else f\"{self.score: 0.3f}\"\n        return f\"{self.node}\\nScore: {score_str}\\n\"\n\n    def get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n        if self.score is None:\n            if raise_error:\n                raise ValueError(\"Score not set.\")\n            else:\n                return 0.0\n        else:\n            return self.score\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"NodeWithScore\"\n\n    ##### pass through methods to BaseNode #####\n    @property\n    def node_id(self) -> str:\n        return self.node.node_id\n\n    @property\n    def id_(self) -> str:\n        return self.node.id_\n\n    @property\n    def text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.text\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        return self.node.metadata\n\n    @property\n    def embedding(self) -> Optional[List[float]]:\n        return self.node.embedding\n\n    def get_text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.get_text()\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n        return self.node.get_content(metadata_mode=metadata_mode)\n\n    def get_embedding(self) -> List[float]:\n        return self.node.get_embedding()\n\n```\n  \n---|---  \n###  get_score [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.NodeWithScore.get_score \"Permanent link\")\n```\nget_score(raise_error:  = False) -> float\n\n```\n\nGet score.\nSource code in `llama_index/core/schema.py`\n```\n958\n959\n960\n961\n962\n963\n964\n965\n966\n```\n| ```\ndef get_score(self, raise_error: bool = False) -> float:\n\"\"\"Get score.\"\"\"\n    if self.score is None:\n        if raise_error:\n            raise ValueError(\"Score not set.\")\n        else:\n            return 0.0\n    else:\n        return self.score\n\n```\n  \n---|---  \n##  Document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document \"Permanent link\")\nBases: \nGeneric interface for a data document.\nThis document connects to data sources.\nSource code in `llama_index/core/schema.py`\n```\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\nclass Document(Node):\n\"\"\"\n    Generic interface for a data document.\n\n    This document connects to data sources.\n    \"\"\"\n\n    def __init__(self, **data: Any) -> None:\n\"\"\"\n        Keeps backward compatibility with old 'Document' versions.\n\n        If 'text' was passed, store it in 'text_resource'.\n        If 'doc_id' was passed, store it in 'id_'.\n        If 'extra_info' was passed, store it in 'metadata'.\n        \"\"\"\n        if \"doc_id\" in data:\n            value = data.pop(\"doc_id\")\n            if \"id_\" in data:\n                msg = \"'doc_id' is deprecated and 'id_' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"id_\"] = value\n\n        if \"extra_info\" in data:\n            value = data.pop(\"extra_info\")\n            if \"metadata\" in data:\n                msg = \"'extra_info' is deprecated and 'metadata' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"metadata\"] = value\n\n        if data.get(\"text\"):\n            text = data.pop(\"text\")\n            if \"text_resource\" in data:\n                text_resource = (\n                    data[\"text_resource\"]\n                    if isinstance(data[\"text_resource\"], MediaResource)\n                    else MediaResource.model_validate(data[\"text_resource\"])\n                )\n                if (text_resource.text or \"\").strip() != text.strip():\n                    msg = (\n                        \"'text' is deprecated and 'text_resource' will be used instead\"\n                    )\n                    logging.warning(msg)\n            else:\n                data[\"text_resource\"] = MediaResource(text=text)\n\n        super().__init__(**data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n        data = super().custom_model_dump(handler, info)\n        exclude_set = set(info.exclude or [])\n        if \"text\" not in exclude_set:\n            data[\"text\"] = self.text\n        return data\n\n    @property\n    def text(self) -> str:\n\"\"\"Provided for backward compatibility, it returns the content of text_resource.\"\"\"\n        return self.get_content()\n\n    @classmethod\n    def get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n        return ObjectType.DOCUMENT\n\n    @property\n    def doc_id(self) -> str:\n\"\"\"Get document ID.\"\"\"\n        return self.id_\n\n    @doc_id.setter\n    def doc_id(self, id_: str) -> None:\n        self.id_ = id_\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Doc ID: {self.doc_id}\\n{source_text_wrapped}\"\n\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'get_doc_id' is deprecated, access the 'id_' property instead.\",\n    )\n    def get_doc_id(self) -> str:  # pragma: nocover\n        return self.id_\n\n    def to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n        from llama_index.core.bridge.langchain import (\n            Document as LCDocument,  # type: ignore\n        )\n\n        metadata = self.metadata or {}\n        return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n    @classmethod\n    def from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n        if doc.id:\n            return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n        return cls(text=doc.page_content, metadata=doc.metadata)\n\n    def to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n        from haystack import Document as HaystackDocument  # type: ignore\n\n        return HaystackDocument(\n            content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n        )\n\n    @classmethod\n    def from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n        return cls(\n            text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n        )\n\n    def to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n        return {\n            \"doc_id\": self.id_,\n            \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n        }\n\n    @classmethod\n    def from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n        return cls(\n            text=doc[\"data\"][\"content\"],\n            metadata=doc[\"data\"][\"meta_data\"],\n            id_=doc[\"doc_id\"],\n        )\n\n    def to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n        import numpy as np\n        from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n        return MemoryRecord(\n            id=self.id_,\n            text=self.text,\n            additional_metadata=self.get_metadata_str(),\n            embedding=np.array(self.embedding) if self.embedding else None,\n        )\n\n    @classmethod\n    def from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n        return cls(\n            text=doc._text,\n            metadata={\"additional_metadata\": doc._additional_metadata},\n            embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n            id_=doc._id,\n        )\n\n    def to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n        # write document to temp file\n        import tempfile\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(self.text.encode(\"utf-8\"))\n            f.flush()\n            client.embed(f.name)\n\n    @classmethod\n    def example(cls) -> Document:\n        return Document(\n            text=SAMPLE_TEXT,\n            metadata={\"filename\": \"README.md\", \"category\": \"codebase\"},\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Document\"\n\n    def to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n        from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n        return CloudDocument(\n            text=self.text,\n            metadata=self.metadata,\n            excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n            id=self.id_,\n        )\n\n    @classmethod\n    def from_cloud_document(\n        cls,\n        doc: CloudDocument,\n    ) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n        return Document(\n            text=doc.text,\n            metadata=doc.metadata,\n            excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n            id_=doc.id,\n        )\n\n```\n  \n---|---  \n###  text `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.text \"Permanent link\")\n```\ntext: \n\n```\n\nProvided for backward compatibility, it returns the content of text_resource.\n###  doc_id `property` `writable` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.doc_id \"Permanent link\")\n```\ndoc_id: \n\n```\n\nGet document ID.\n###  custom_model_dump [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.custom_model_dump \"Permanent link\")\n```\ncustom_model_dump(handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> [, ]\n\n```\n\nFor full backward compatibility with the text field, we customize the model serializer.\nSource code in `llama_index/core/schema.py`\n```\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n```\n| ```\n@model_serializer(mode=\"wrap\")\ndef custom_model_dump(\n    self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n) -> Dict[str, Any]:\n\"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n    data = super().custom_model_dump(handler, info)\n    exclude_set = set(info.exclude or [])\n    if \"text\" not in exclude_set:\n        data[\"text\"] = self.text\n    return data\n\n```\n  \n---|---  \n###  get_type `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.get_type \"Permanent link\")\n```\nget_type() -> \n\n```\n\nGet Document type.\nSource code in `llama_index/core/schema.py`\n```\n1077\n1078\n1079\n1080\n```\n| ```\n@classmethod\ndef get_type(cls) -> str:\n\"\"\"Get Document type.\"\"\"\n    return ObjectType.DOCUMENT\n\n```\n  \n---|---  \n###  to_langchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_langchain_format \"Permanent link\")\n```\nto_langchain_format() -> Document\n\n```\n\nConvert struct to LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n```\n| ```\ndef to_langchain_format(self) -> LCDocument:\n\"\"\"Convert struct to LangChain document format.\"\"\"\n    from llama_index.core.bridge.langchain import (\n        Document as LCDocument,  # type: ignore\n    )\n\n    metadata = self.metadata or {}\n    return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n```\n  \n---|---  \n###  from_langchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_langchain_format \"Permanent link\")\n```\nfrom_langchain_format(doc: Document) -> \n\n```\n\nConvert struct from LangChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1116\n1117\n1118\n1119\n1120\n1121\n```\n| ```\n@classmethod\ndef from_langchain_format(cls, doc: LCDocument) -> Document:\n\"\"\"Convert struct from LangChain document format.\"\"\"\n    if doc.id:\n        return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n    return cls(text=doc.page_content, metadata=doc.metadata)\n\n```\n  \n---|---  \n###  to_haystack_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_haystack_format \"Permanent link\")\n```\nto_haystack_format() -> Document\n\n```\n\nConvert struct to Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n```\n| ```\ndef to_haystack_format(self) -> HaystackDocument:\n\"\"\"Convert struct to Haystack document format.\"\"\"\n    from haystack import Document as HaystackDocument  # type: ignore\n\n    return HaystackDocument(\n        content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n    )\n\n```\n  \n---|---  \n###  from_haystack_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_haystack_format \"Permanent link\")\n```\nfrom_haystack_format(doc: Document) -> \n\n```\n\nConvert struct from Haystack document format.\nSource code in `llama_index/core/schema.py`\n```\n1131\n1132\n1133\n1134\n1135\n1136\n```\n| ```\n@classmethod\ndef from_haystack_format(cls, doc: HaystackDocument) -> Document:\n\"\"\"Convert struct from Haystack document format.\"\"\"\n    return cls(\n        text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n    )\n\n```\n  \n---|---  \n###  to_embedchain_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_embedchain_format \"Permanent link\")\n```\nto_embedchain_format() -> [, ]\n\n```\n\nConvert struct to EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1138\n1139\n1140\n1141\n1142\n1143\n```\n| ```\ndef to_embedchain_format(self) -> Dict[str, Any]:\n\"\"\"Convert struct to EmbedChain document format.\"\"\"\n    return {\n        \"doc_id\": self.id_,\n        \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n    }\n\n```\n  \n---|---  \n###  from_embedchain_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_embedchain_format \"Permanent link\")\n```\nfrom_embedchain_format(doc: [, ]) -> \n\n```\n\nConvert struct from EmbedChain document format.\nSource code in `llama_index/core/schema.py`\n```\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n```\n| ```\n@classmethod\ndef from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n\"\"\"Convert struct from EmbedChain document format.\"\"\"\n    return cls(\n        text=doc[\"data\"][\"content\"],\n        metadata=doc[\"data\"][\"meta_data\"],\n        id_=doc[\"doc_id\"],\n    )\n\n```\n  \n---|---  \n###  to_semantic_kernel_format [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_semantic_kernel_format \"Permanent link\")\n```\nto_semantic_kernel_format() -> MemoryRecord\n\n```\n\nConvert struct to Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n```\n| ```\ndef to_semantic_kernel_format(self) -> MemoryRecord:\n\"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n    import numpy as np\n    from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n    return MemoryRecord(\n        id=self.id_,\n        text=self.text,\n        additional_metadata=self.get_metadata_str(),\n        embedding=np.array(self.embedding) if self.embedding else None,\n    )\n\n```\n  \n---|---  \n###  from_semantic_kernel_format `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_semantic_kernel_format \"Permanent link\")\n```\nfrom_semantic_kernel_format(doc: MemoryRecord) -> \n\n```\n\nConvert struct from Semantic Kernel document format.\nSource code in `llama_index/core/schema.py`\n```\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n```\n| ```\n@classmethod\ndef from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n\"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n    return cls(\n        text=doc._text,\n        metadata={\"additional_metadata\": doc._additional_metadata},\n        embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n        id_=doc._id,\n    )\n\n```\n  \n---|---  \n###  to_vectorflow [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_vectorflow \"Permanent link\")\n```\nto_vectorflow(client: ) -> None\n\n```\n\nSend a document to vectorflow, since they don't have a document object.\nSource code in `llama_index/core/schema.py`\n```\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n```\n| ```\ndef to_vectorflow(self, client: Any) -> None:\n\"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n    # write document to temp file\n    import tempfile\n\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(self.text.encode(\"utf-8\"))\n        f.flush()\n        client.embed(f.name)\n\n```\n  \n---|---  \n###  to_cloud_document [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.to_cloud_document \"Permanent link\")\n```\nto_cloud_document() -> CloudDocument\n\n```\n\nConvert to LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n```\n| ```\ndef to_cloud_document(self) -> CloudDocument:\n\"\"\"Convert to LlamaCloud document type.\"\"\"\n    from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n    return CloudDocument(\n        text=self.text,\n        metadata=self.metadata,\n        excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n        id=self.id_,\n    )\n\n```\n  \n---|---  \n###  from_cloud_document `classmethod` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.Document.from_cloud_document \"Permanent link\")\n```\nfrom_cloud_document(doc: CloudDocument) -> \n\n```\n\nConvert from LlamaCloud document type.\nSource code in `llama_index/core/schema.py`\n```\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n```\n| ```\n@classmethod\ndef from_cloud_document(\n    cls,\n    doc: CloudDocument,\n) -> Document:\n\"\"\"Convert from LlamaCloud document type.\"\"\"\n    return Document(\n        text=doc.text,\n        metadata=doc.metadata,\n        excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n        excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n        id_=doc.id,\n    )\n\n```\n  \n---|---  \n##  ImageDocument [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument \"Permanent link\")\nBases: \nBackward compatible wrapper around Document containing an image.\nSource code in `llama_index/core/schema.py`\n```\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\nclass ImageDocument(Document):\n\"\"\"Backward compatible wrapper around Document containing an image.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        image = kwargs.pop(\"image\", None)\n        image_path = kwargs.pop(\"image_path\", None)\n        image_url = kwargs.pop(\"image_url\", None)\n        image_mimetype = kwargs.pop(\"image_mimetype\", None)\n        text_embedding = kwargs.pop(\"text_embedding\", None)\n\n        if image:\n            kwargs[\"image_resource\"] = MediaResource(\n                data=image, mimetype=image_mimetype\n            )\n        elif image_path:\n            if not is_image_pil(image_path):\n                raise ValueError(\"The specified file path is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                path=image_path, mimetype=image_mimetype\n            )\n        elif image_url:\n            if not is_image_url_pil(image_url):\n                raise ValueError(\"The specified URL is not an accessible image\")\n            kwargs[\"image_resource\"] = MediaResource(\n                url=image_url, mimetype=image_mimetype\n            )\n\n        super().__init__(**kwargs)\n\n    @property\n    def image(self) -> str | None:\n        if self.image_resource and self.image_resource.data:\n            return self.image_resource.data.decode(\"utf-8\")\n        return None\n\n    @image.setter\n    def image(self, image: str) -> None:\n        self.image_resource = MediaResource(data=image.encode(\"utf-8\"))\n\n    @property\n    def image_path(self) -> str | None:\n        if self.image_resource and self.image_resource.path:\n            return str(self.image_resource.path)\n        return None\n\n    @image_path.setter\n    def image_path(self, image_path: str) -> None:\n        self.image_resource = MediaResource(path=Path(image_path))\n\n    @property\n    def image_url(self) -> str | None:\n        if self.image_resource and self.image_resource.url:\n            return str(self.image_resource.url)\n        return None\n\n    @image_url.setter\n    def image_url(self, image_url: str) -> None:\n        self.image_resource = MediaResource(url=AnyUrl(url=image_url))\n\n    @property\n    def image_mimetype(self) -> str | None:\n        if self.image_resource:\n            return self.image_resource.mimetype\n        return None\n\n    @image_mimetype.setter\n    def image_mimetype(self, image_mimetype: str) -> None:\n        if self.image_resource:\n            self.image_resource.mimetype = image_mimetype\n\n    @property\n    def text_embedding(self) -> list[float] | None:\n        if self.text_resource and self.text_resource.embeddings:\n            return self.text_resource.embeddings.get(\"dense\")\n        return None\n\n    @text_embedding.setter\n    def text_embedding(self, embeddings: list[float]) -> None:\n        if self.text_resource:\n            if self.text_resource.embeddings is None:\n                self.text_resource.embeddings = {}\n            self.text_resource.embeddings[\"dense\"] = embeddings\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageDocument\"\n\n    def resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n        Resolve an image such that PIL can read it.\n\n        Args:\n            as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n        \"\"\"\n        if self.image_resource is None:\n            return BytesIO()\n\n        if self.image_resource.data is not None:\n            if as_base64:\n                return BytesIO(self.image_resource.data)\n            return BytesIO(base64.b64decode(self.image_resource.data))\n        elif self.image_resource.path is not None:\n            img_bytes = self.image_resource.path.read_bytes()\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        elif self.image_resource.url is not None:\n            # load image from URL\n            response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n            img_bytes = response.content\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        else:\n            raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n###  resolve_image [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.ImageDocument.resolve_image \"Permanent link\")\n```\nresolve_image(as_base64:  = False) -> BytesIO\n\n```\n\nResolve an image such that PIL can read it.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`as_base64` |  `bool` |  whether the resolved image should be returned as base64-encoded bytes |  `False`  \nSource code in `llama_index/core/schema.py`\n```\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n```\n| ```\ndef resolve_image(self, as_base64: bool = False) -> BytesIO:\n\"\"\"\n    Resolve an image such that PIL can read it.\n\n    Args:\n        as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n\n    \"\"\"\n    if self.image_resource is None:\n        return BytesIO()\n\n    if self.image_resource.data is not None:\n        if as_base64:\n            return BytesIO(self.image_resource.data)\n        return BytesIO(base64.b64decode(self.image_resource.data))\n    elif self.image_resource.path is not None:\n        img_bytes = self.image_resource.path.read_bytes()\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    elif self.image_resource.url is not None:\n        # load image from URL\n        response = requests.get(str(self.image_resource.url), timeout=(60, 60))\n        img_bytes = response.content\n        if as_base64:\n            return BytesIO(base64.b64encode(img_bytes))\n        return BytesIO(img_bytes)\n    else:\n        raise ValueError(\"No image found in the chat message!\")\n\n```\n  \n---|---  \n##  QueryBundle `dataclass` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle \"Permanent link\")\nBases: `DataClassJsonMixin`\nQuery bundle.\nThis dataclass contains the original query string and associated transformations.\nParameters:\nName | Type | Description | Default  \n---|---|---|---  \n`query_str` |  the original user-specified query string. This is currently used by all non embedding-based queries. |  _required_  \n`custom_embedding_strs` |  `list[str]` |  list of strings used for embedding the query. This is currently used by all embedding-based queries. |  `None`  \n`embedding` |  `list[float]` |  the stored embedding for the query. |  `None`  \n`image_path` |  `None`  \nSource code in `llama_index/core/schema.py`\n```\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n```\n| ```\n@dataclass\nclass QueryBundle(DataClassJsonMixin):\n\"\"\"\n    Query bundle.\n\n    This dataclass contains the original query string and associated transformations.\n\n    Args:\n        query_str (str): the original user-specified query string.\n            This is currently used by all non embedding-based queries.\n        custom_embedding_strs (list[str]): list of strings used for embedding the query.\n            This is currently used by all embedding-based queries.\n        embedding (list[float]): the stored embedding for the query.\n\n    \"\"\"\n\n    query_str: str\n    # using single image path as query input\n    image_path: Optional[str] = None\n    custom_embedding_strs: Optional[List[str]] = None\n    embedding: Optional[List[float]] = None\n\n    @property\n    def embedding_strs(self) -> List[str]:\n\"\"\"Use custom embedding strs if specified, otherwise use query str.\"\"\"\n        if self.custom_embedding_strs is None:\n            if len(self.query_str) == 0:\n                return []\n            return [self.query_str]\n        else:\n            return self.custom_embedding_strs\n\n    @property\n    def embedding_image(self) -> List[ImageType]:\n\"\"\"Use image path for image retrieval.\"\"\"\n        if self.image_path is None:\n            return []\n        return [self.image_path]\n\n    def __str__(self) -> str:\n\"\"\"Convert to string representation.\"\"\"\n        return self.query_str\n\n```\n  \n---|---  \n###  embedding_strs `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_strs \"Permanent link\")\n```\nembedding_strs: []\n\n```\n\nUse custom embedding strs if specified, otherwise use query str.\n###  embedding_image `property` [#](https://developers.llamaindex.ai/python/framework-api-reference/schema/#llama_index.core.schema.QueryBundle.embedding_image \"Permanent link\")\n```\nembedding_image: [ImageType]\n\n```\n\nUse image path for image retrieval.\n"}, "__type__": "4"}, "f9fdccac-4c2f-4eb1-b8a5-a5f777ccdc0f": {"__data__": {"id_": "f9fdccac-4c2f-4eb1-b8a5-a5f777ccdc0f", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework_getting_started_starter_example_local.md", "file_name": "python_framework_getting_started_starter_example_local.md", "file_type": "text/markdown", "file_size": 8669, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#_top)\n# Starter Tutorial (Using Local LLMs)\nThis tutorial will show you how to get started building agents with LlamaIndex. We\u2019ll start with a basic example and then show how to add RAG (Retrieval-Augmented Generation) capabilities.\nWe will use [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) as our embedding model and `llama3.1 8B` served through `Ollama`.\n## Setup\n[Section titled \u201cSetup\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#setup)\nOllama is a tool to help you get set up with LLMs locally with minimal setup.\nFollow the [README](https://github.com/jmorganca/ollama) to learn how to install it.\nTo download the Llama3 model just do `ollama pull llama3.1`.\n**NOTE** : You will need a machine with at least ~32GB of RAM.\nAs explained in our [installation guide](https://developers.llamaindex.ai/python/framework/getting_started/installation), `llama-index` is actually a collection of packages. To run Ollama and Huggingface, we will need to install those integrations:\nTerminal window```\n\n\npipinstallllama-index-llms-ollamallama-index-embeddings-huggingface\n\n\n```\n\nThe package names spell out the imports, which is very helpful for remembering how to import them or install them!\n```\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n```\n\nMore integrations are all listed on <https://llamahub.ai>.\n## Basic Agent Example\n[Section titled \u201cBasic Agent Example\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#basic-agent-example)\nLet\u2019s start with a simple example using an agent that can perform basic multiplication by calling a tool. Create a file called `starter.py`:\n```\n\n\nimport asyncio\n\n\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\n\n# Define a simple calculator tool\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n# Create an agent workflow with our calculator tool\n\n\n\nagent =FunctionAgent(\n\n\n\n\ntools=[multiply],\n\n\n\n\nllm=Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\nsystem_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n\n\n\n\n\n\n\nasyncdefmain():\n\n\n\n\n# Run the agent\n\n\n\n\nresponse =await agent.run(\"What is 1234 * 4567?\")\n\n\n\n\nprint(str(response))\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThis will output something like: `The answer to 1234 * 4567 is 5635678.`\nWhat happened is:\n  * The agent was given a question: `What is 1234 * 4567?`\n  * Under the hood, this question, plus the schema of the tools (name, docstring, and arguments) were passed to the LLM\n  * The agent selected the `multiply` tool and wrote the arguments to the tool\n  * The agent received the result from the tool and interpolated it into the final response\n\n\n## Adding Chat History\n[Section titled \u201cAdding Chat History\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-chat-history)\nThe `AgentWorkflow` is also able to remember previous messages. This is contained inside the `Context` of the `AgentWorkflow`.\nIf the `Context` is passed in, the agent will use it to continue the conversation.\n```\n\n\nfrom llama_index.core.workflow import Context\n\n\n\n\n# create context\n\n\n\nctx =Context(agent)\n\n\n\n\n# run agent with context\n\n\n\nresponse =await agent.run(\"My name is Logan\",ctx=ctx)\n\n\n\n\nresponse =await agent.run(\"What is my name?\",ctx=ctx)\n\n\n```\n\n## Adding RAG Capabilities\n[Section titled \u201cAdding RAG Capabilities\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-rag-capabilities)\nNow let\u2019s enhance our agent by adding the ability to search through documents. First, let\u2019s get some example data using our terminal:\nTerminal window```\n\n\nmkdirdata\n\n\n\n\nwgethttps://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt-Odata/paul_graham_essay.txt\n\n\n```\n\nYour directory structure should look like this now:\n```\n\u251c\u2500\u2500 starter.py \u2514\u2500\u2500 data \u00a0\u00a0 \u2514\u2500\u2500 paul_graham_essay.txt\n```\n\nNow we can create a tool for searching through documents using LlamaIndex. By default, our `VectorStoreIndex` will use a `text-embedding-ada-002` embeddings from OpenAI to embed and retrieve the text.\nOur modified `starter.py` should look like this:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n\n\n\n\nfrom llama_index.core.agent.workflow import AgentWorkflow\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\nimport asyncio\n\n\n\n\nimport os\n\n\n\n\n# Settings control global defaults\n\n\n\nSettings.embed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n\n\n\nSettings.llm =Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\n# Create a RAG tool using LlamaIndex\n\n\n\ndocuments =SimpleDirectoryReader(\"data\").load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n\nasyncdefsearch_documents(query: str) -> str:\n\n\n\n\n\"\"\"Useful for answering natural language questions about an personal essay written by Paul Graham.\"\"\"\n\n\n\n\nresponse =await query_engine.aquery(query)\n\n\n\n\nreturnstr(response)\n\n\n\n\n\n# Create an enhanced workflow with both tools\n\n\n\nagent = AgentWorkflow.from_tools_or_functions(\n\n\n\n\n[multiply, search_documents],\n\n\n\n\nllm=Settings.llm,\n\n\n\n\nsystem_prompt=\"\"\"You are a helpful assistant that can perform calculations\n\n\n\n\nand search through documents to answer questions.\"\"\",\n\n\n\n\n\n\n# Now we can ask questions about the documents or do calculations\n\n\n\nasyncdefmain():\n\n\n\n\nresponse =await agent.run(\n\n\n\n\n\"What did the author do in college? Also, what's 7 * 8?\"\n\n\n\n\n\nprint(response)\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThe agent can now seamlessly switch between using the calculator and searching through documents to answer questions.\n## Storing the RAG Index\n[Section titled \u201cStoring the RAG Index\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#storing-the-rag-index)\nTo avoid reprocessing documents every time, you can persist the index to disk:\n```\n\n# Save the index\n\n\n\nindex.storage_context.persist(\"storage\")\n\n\n\n\n# Later, load the index\n\n\n\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n\n\n\n\nindex =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n```\n\n```\n\n\nindex = VectorStoreIndex.from_vector_store(\n\n\n\n\nvector_store,\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n```\n\n## What\u2019s Next?\n[Section titled \u201cWhat\u2019s Next?\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#whats-next)\nThis is just the beginning of what you can do with LlamaIndex agents! You can:\n  * Add more tools to your agent\n  * Use different LLMs\n  * Customize the agent\u2019s behavior using system prompts\n  * Add streaming capabilities\n  * Implement human-in-the-loop workflows\n  * Use multiple agents to collaborate on tasks\n\n\nSome helpful next links:\n  * See more advanced agent examples in our [Agent documentation](https://developers.llamaindex.ai/python/framework/understanding/agent)\n  * Learn more about [high-level concepts](https://developers.llamaindex.ai/python/framework/getting_started/concepts)\n  * Explore how to [customize things](https://developers.llamaindex.ai/python/framework/getting_started/faq)\n  * Check out the [component guides](https://developers.llamaindex.ai/python/framework/module_guides)\n\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#_top)\n# Starter Tutorial (Using Local LLMs)\nThis tutorial will show you how to get started building agents with LlamaIndex. We\u2019ll start with a basic example and then show how to add RAG (Retrieval-Augmented Generation) capabilities.\nWe will use [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) as our embedding model and `llama3.1 8B` served through `Ollama`.\n## Setup\n[Section titled \u201cSetup\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#setup)\nOllama is a tool to help you get set up with LLMs locally with minimal setup.\nFollow the [README](https://github.com/jmorganca/ollama) to learn how to install it.\nTo download the Llama3 model just do `ollama pull llama3.1`.\n**NOTE** : You will need a machine with at least ~32GB of RAM.\nAs explained in our [installation guide](https://developers.llamaindex.ai/python/framework/getting_started/installation), `llama-index` is actually a collection of packages. To run Ollama and Huggingface, we will need to install those integrations:\nTerminal window```\n\n\npipinstallllama-index-llms-ollamallama-index-embeddings-huggingface\n\n\n```\n\nThe package names spell out the imports, which is very helpful for remembering how to import them or install them!\n```\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n```\n\nMore integrations are all listed on <https://llamahub.ai>.\n## Basic Agent Example\n[Section titled \u201cBasic Agent Example\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#basic-agent-example)\nLet\u2019s start with a simple example using an agent that can perform basic multiplication by calling a tool. Create a file called `starter.py`:\n```\n\n\nimport asyncio\n\n\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\n\n# Define a simple calculator tool\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n# Create an agent workflow with our calculator tool\n\n\n\nagent =FunctionAgent(\n\n\n\n\ntools=[multiply],\n\n\n\n\nllm=Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\nsystem_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n\n\n\n\n\n\n\nasyncdefmain():\n\n\n\n\n# Run the agent\n\n\n\n\nresponse =await agent.run(\"What is 1234 * 4567?\")\n\n\n\n\nprint(str(response))\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThis will output something like: `The answer to 1234 * 4567 is 5635678.`\nWhat happened is:\n  * The agent was given a question: `What is 1234 * 4567?`\n  * Under the hood, this question, plus the schema of the tools (name, docstring, and arguments) were passed to the LLM\n  * The agent selected the `multiply` tool and wrote the arguments to the tool\n  * The agent received the result from the tool and interpolated it into the final response\n\n\n## Adding Chat History\n[Section titled \u201cAdding Chat History\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-chat-history)\nThe `AgentWorkflow` is also able to remember previous messages. This is contained inside the `Context` of the `AgentWorkflow`.\nIf the `Context` is passed in, the agent will use it to continue the conversation.\n```\n\n\nfrom llama_index.core.workflow import Context\n\n\n\n\n# create context\n\n\n\nctx =Context(agent)\n\n\n\n\n# run agent with context\n\n\n\nresponse =await agent.run(\"My name is Logan\",ctx=ctx)\n\n\n\n\nresponse =await agent.run(\"What is my name?\",ctx=ctx)\n\n\n```\n\n## Adding RAG Capabilities\n[Section titled \u201cAdding RAG Capabilities\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#adding-rag-capabilities)\nNow let\u2019s enhance our agent by adding the ability to search through documents. First, let\u2019s get some example data using our terminal:\nTerminal window```\n\n\nmkdirdata\n\n\n\n\nwgethttps://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt-Odata/paul_graham_essay.txt\n\n\n```\n\nYour directory structure should look like this now:\n```\n\u251c\u2500\u2500 starter.py \u2514\u2500\u2500 data \u00a0\u00a0 \u2514\u2500\u2500 paul_graham_essay.txt\n```\n\nNow we can create a tool for searching through documents using LlamaIndex. By default, our `VectorStoreIndex` will use a `text-embedding-ada-002` embeddings from OpenAI to embed and retrieve the text.\nOur modified `starter.py` should look like this:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n\n\n\n\nfrom llama_index.core.agent.workflow import AgentWorkflow\n\n\n\n\nfrom llama_index.llms.ollama import Ollama\n\n\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\n\n\nimport asyncio\n\n\n\n\nimport os\n\n\n\n\n# Settings control global defaults\n\n\n\nSettings.embed_model =HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n\n\n\nSettings.llm =Ollama(\n\n\n\n\nmodel=\"llama3.1\",\n\n\n\n\nrequest_timeout=360.0,\n\n\n\n\n# Manually set the context window to limit memory usage\n\n\n\n\ncontext_window=8000,\n\n\n\n\n\n# Create a RAG tool using LlamaIndex\n\n\n\ndocuments =SimpleDirectoryReader(\"data\").load_data()\n\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\n\n\ndocuments,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n\n\n\n\ndefmultiply(a: float, b: float) -> float:\n\n\n\n\n\"\"\"Useful for multiplying two numbers.\"\"\"\n\n\n\n\nreturn* b\n\n\n\n\n\n\nasyncdefsearch_documents(query: str) -> str:\n\n\n\n\n\"\"\"Useful for answering natural language questions about an personal essay written by Paul Graham.\"\"\"\n\n\n\n\nresponse =await query_engine.aquery(query)\n\n\n\n\nreturnstr(response)\n\n\n\n\n\n# Create an enhanced workflow with both tools\n\n\n\nagent = AgentWorkflow.from_tools_or_functions(\n\n\n\n\n[multiply, search_documents],\n\n\n\n\nllm=Settings.llm,\n\n\n\n\nsystem_prompt=\"\"\"You are a helpful assistant that can perform calculations\n\n\n\n\nand search through documents to answer questions.\"\"\",\n\n\n\n\n\n\n# Now we can ask questions about the documents or do calculations\n\n\n\nasyncdefmain():\n\n\n\n\nresponse =await agent.run(\n\n\n\n\n\"What did the author do in college? Also, what's 7 * 8?\"\n\n\n\n\n\nprint(response)\n\n\n\n\n\n# Run the agent\n\n\n\nif __name__ ==\"__main__\":\n\n\n\n\nasyncio.run(main())\n\n\n```\n\nThe agent can now seamlessly switch between using the calculator and searching through documents to answer questions.\n## Storing the RAG Index\n[Section titled \u201cStoring the RAG Index\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#storing-the-rag-index)\nTo avoid reprocessing documents every time, you can persist the index to disk:\n```\n\n# Save the index\n\n\n\nindex.storage_context.persist(\"storage\")\n\n\n\n\n# Later, load the index\n\n\n\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n\n\n\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n\n\n\n\nindex =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n# we can optionally override the embed_model here\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n\n\nquery_engine = index.as_query_engine(\n\n\n\n\n# we can optionally override the llm here\n\n\n\n\n# llm=Settings.llm,\n\n\n\n```\n\n```\n\n\nindex = VectorStoreIndex.from_vector_store(\n\n\n\n\nvector_store,\n\n\n\n\n# it's important to use the same embed_model as the one used to build the index\n\n\n\n\n# embed_model=Settings.embed_model,\n\n\n\n```\n\n## What\u2019s Next?\n[Section titled \u201cWhat\u2019s Next?\u201d](https://developers.llamaindex.ai/python/framework/getting_started/starter_example_local/#whats-next)\nThis is just the beginning of what you can do with LlamaIndex agents! You can:\n  * Add more tools to your agent\n  * Use different LLMs\n  * Customize the agent\u2019s behavior using system prompts\n  * Add streaming capabilities\n  * Implement human-in-the-loop workflows\n  * Use multiple agents to collaborate on tasks\n\n\nSome helpful next links:\n  * See more advanced agent examples in our [Agent documentation](https://developers.llamaindex.ai/python/framework/understanding/agent)\n  * Learn more about [high-level concepts](https://developers.llamaindex.ai/python/framework/getting_started/concepts)\n  * Explore how to [customize things](https://developers.llamaindex.ai/python/framework/getting_started/faq)\n  * Check out the [component guides](https://developers.llamaindex.ai/python/framework/module_guides)\n\n\n"}, "__type__": "4"}, "9761d9a9-b545-4f33-81ed-79a3ff9b5997": {"__data__": {"id_": "9761d9a9-b545-4f33-81ed-79a3ff9b5997", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework_module_guides_loading_connector_usage_pattern.md", "file_name": "python_framework_module_guides_loading_connector_usage_pattern.md", "file_type": "text/markdown", "file_size": 998, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#_top)\n# Usage Pattern\n## Get Started\n[Section titled \u201cGet Started\u201d](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#get-started)\nEach data loader contains a \u201cUsage\u201d section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which downloads the loader file into a module that you can use within your application.\nExample usage:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, download_loader\n\n\n\n\n\nfrom llama_index.readers.google import GoogleDocsReader\n\n\n\n\n\ngdoc_ids =[\"1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec\"]\n\n\n\n\nloader =GoogleDocsReader()\n\n\n\n\ndocuments = loader.load_data(document_ids=gdoc_ids)\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nquery_engine.query(\"Where did the author go to school?\")\n\n\n```\n\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#_top)\n# Usage Pattern\n## Get Started\n[Section titled \u201cGet Started\u201d](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/usage_pattern/#get-started)\nEach data loader contains a \u201cUsage\u201d section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which downloads the loader file into a module that you can use within your application.\nExample usage:\n```\n\n\nfrom llama_index.core import VectorStoreIndex, download_loader\n\n\n\n\n\nfrom llama_index.readers.google import GoogleDocsReader\n\n\n\n\n\ngdoc_ids =[\"1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec\"]\n\n\n\n\nloader =GoogleDocsReader()\n\n\n\n\ndocuments = loader.load_data(document_ids=gdoc_ids)\n\n\n\n\nindex = VectorStoreIndex.from_documents(documents)\n\n\n\n\nquery_engine = index.as_query_engine()\n\n\n\n\nquery_engine.query(\"Where did the author go to school?\")\n\n\n```\n\n"}, "__type__": "4"}, "e9633413-2f6b-4dc5-b7c3-e4cfd74e55fe": {"__data__": {"id_": "e9633413-2f6b-4dc5-b7c3-e4cfd74e55fe", "embedding": null, "metadata": {"file_path": "/Users/Amol.Nikam/Documents/GenAI/Llamaindex/documentation-helper/llamaindex-docs/python_framework_understanding_putting_it_all_together_chatbots_building_a_chatbot.md", "file_name": "python_framework_understanding_putting_it_all_together_chatbots_building_a_chatbot.md", "file_type": "text/markdown", "file_size": 15079, "creation_date": "2026-01-13", "last_modified_date": "2026-01-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text_resource": {"embeddings": null, "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#_top)\n# How to Build a Chatbot\nLlamaIndex serves as a bridge between your data and Large Language Models (LLMs), providing a toolkit that enables you to establish a query interface around your data for a variety of tasks, such as question-answering and summarization.\nIn this tutorial, we\u2019ll walk you through building a context-augmented chatbot using a [Data Agent](https://gpt-index.readthedocs.io/en/stable/core_modules/agent_modules/agents/root.html). This agent, powered by LLMs, is capable of intelligently executing tasks over your data. The end result is a chatbot agent equipped with a robust set of data interface tools provided by LlamaIndex to answer queries about your data.\n**Note** : This tutorial builds upon initial work on creating a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).\n### Context\n[Section titled \u201cContext\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#context)\nIn this guide, we\u2019ll build a \u201c10-K Chatbot\u201d that uses raw UBER 10-K HTML filings from Dropbox. Users can interact with the chatbot to ask questions related to the 10-K filings.\n### Preparation\n[Section titled \u201cPreparation\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#preparation)\n```\n\n\nimport os\n\n\n\n\nimport openai\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] =\"sk-...\"\n\n\n\n\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n\n\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n### Ingest Data\n[Section titled \u201cIngest Data\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#ingest-data)\nLet\u2019s first download the raw 10-k files, from 2019-2022.\n```\n\n# NOTE: the code examples assume you're operating within a Jupyter notebook.\n\n\n# download files\n\n\n!mkdir data\n\n\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n\n\n!unzip data/UBER.zip -d data\n\n```\n\nTo parse the HTML files into formatted text, we use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library. Thanks to [LlamaHub](https://llamahub.ai/), we can directly integrate with Unstructured, allowing conversion of any text into a Document format that LlamaIndex can ingest.\nFirst we install the necessary packages:\n```\n\n!pip install llama-hub unstructured\n\n```\n\nThen we can use the `UnstructuredReader` to parse the HTML files into a list of `Document` objects.\n```\n\n\nfrom llama_index.readers.file import UnstructuredReader\n\n\n\n\nfrom pathlib import Path\n\n\n\n\n\nyears =[2022, 2021, 2020, 2019]\n\n\n\n\n\nloader =UnstructuredReader()\n\n\n\n\ndoc_set = {}\n\n\n\n\nall_docs =[]\n\n\n\n\nfor year in years:\n\n\n\n\nyear_docs = loader.load_data(\n\n\n\n\nfile=Path(f\"./data/UBER/UBER_{year}.html\"),split_documents=False\n\n\n\n\n\n# insert year metadata into each year\n\n\n\n\nforin year_docs:\n\n\n\n\nd.metadata = {\"year\": year}\n\n\n\n\ndoc_set[year] = year_docs\n\n\n\n\nall_docs.extend(year_docs)\n\n\n```\n\n### Setting up Vector Indices for each year\n[Section titled \u201cSetting up Vector Indices for each year\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-vector-indices-for-each-year)\nWe first setup a vector index for each year. Each vector index allows us to ask questions about the 10-K filing of a given year.\nWe build each index and save it to disk.\n```\n\n# initialize simple vector indices\n\n\n\nfrom llama_index.core import VectorStoreIndex, StorageContext\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\nSettings.chunk_size =512\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults()\n\n\n\n\ncur_index = VectorStoreIndex.from_documents(\n\n\n\n\ndoc_set[year],\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n\n\nstorage_context.persist(persist_dir=f\"./storage/{year}\")\n\n\n```\n\nTo load an index from disk, do the following\n```\n\n# Load indices from disk\n\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=f\"./storage/{year}\"\n\n\n\n\n\ncur_index =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n```\n\n### Setting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\n[Section titled \u201cSetting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-a-sub-question-query-engine-to-synthesize-answers-across-10-k-filings)\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.\nTo address this, we can use a [Sub Question Query Engine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/sub_question_query_engine.html). It decomposes a query into subqueries, each answered by an individual vector index, and synthesizes the results to answer the overall query.\nLlamaIndex provides some wrappers around indices (and query engines) so that they can be used by query engines and agents. First we define a `QueryEngineTool` for each vector index. Each tool has a name and a description; these are what the LLM agent sees to decide which tool to choose.\n```\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\n\nindividual_query_engine_tools =[\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=index_set[year].as_query_engine(),\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"vector_index_{year}\",\n\n\n\n\ndescription=f\"useful for when you want to answer queries about the {year} SEC 10-K for Uber\",\n\n\n\n\n\n\nfor year in years\n\n\n\n```\n\nNow we can create the Sub Question Query Engine, which will allow us to synthesize answers across the 10-K filings. We pass in the `individual_query_engine_tools` we defined above, as well as an `llm` that will be used to run the subqueries.\n```\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\n\n\n\n\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n\n\n\n\nquery_engine_tools=individual_query_engine_tools,\n\n\n\n\nllm=OpenAI(model=\"gpt-3.5-turbo\"),\n\n\n\n```\n\n### Setting up the Chatbot Agent\n[Section titled \u201cSetting up the Chatbot Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-agent)\nWe use a LlamaIndex Data Agent to setup the outer chatbot agent, which has access to a set of Tools. Specifically, we will use a `FunctionAgent`, that takes advantage of OpenAI API function calling. We want to use the separate Tools we defined previously for each index (corresponding to a given year), as well as a tool for the sub question query engine we defined above.\nFirst we define a `QueryEngineTool` for the sub question query engine:\n```\n\n\nquery_engine_tool =QueryEngineTool(\n\n\n\n\nquery_engine=query_engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=\"sub_question_query_engine\",\n\n\n\n\ndescription=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber\",\n\n\n\n\n```\n\nThen, we combine the Tools we defined above into a single list of tools for the agent:\n```\n\n\ntools = individual_query_engine_tools +[query_engine_tool]\n\n\n```\n\nFinally, we call `FunctionAgent()` to create the agent, passing in the list of tools we defined above.\n```\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n```\n\n### Testing the Agent\n[Section titled \u201cTesting the Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#testing-the-agent)\nWe can now test the agent with various queries.\nIf we test it with a simple \u201chello\u201d query, the agent does not use any Tools.\n```\n\n\nresponse =await agent.run(\"hi, i am bob\")\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHello Bob! How can I assist you today?\n\n```\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use the relevant vector index Tool.\n```\n\n\nresponse =await agent.run(\n\n\n\n\n\"What were some of the biggest risk factors in 2020 for Uber?\"\n\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nSome of the biggest risk factors for Uber in 2020 were:\n\n\n\n1. The adverse impact of the COVID-19 pandemic and actions taken to mitigate it on the business.\n\n\n2. The potential reclassification of drivers as employees, workers, or quasi-employees instead of independent contractors.\n\n\n3. Intense competition in the mobility, delivery, and logistics industries, with low-cost alternatives and well-capitalized competitors.\n\n\n4. The need to lower fares or service fees and offer driver incentives and consumer discounts to remain competitive.\n\n\n5. Significant losses incurred and the uncertainty of achieving profitability.\n\n\n6. The risk of not attracting or maintaining a critical mass of platform users.\n\n\n7. Operational, compliance, and cultural challenges related to the workplace culture and forward-leaning approach.\n\n\n8. The potential negative impact of international investments and the challenges of conducting business in foreign countries.\n\n\n9. Risks associated with operational and compliance challenges, localization, laws and regulations, competition, social acceptance, technological compatibility, improper business practices, liability uncertainty, managing international operations, currency fluctuations, cash transactions, tax consequences, and payment fraud.\n\n\n\nThese risk factors highlight the challenges and uncertainties that Uber faced in 2020.\n\n```\n\nFinally, if we test it with a query to compare/contrast risk factors across years, the agent will use the Sub Question Query Engine Tool.\n```\n\n\ncross_query_str =\"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n\n\n\n\n\nresponse =await agent.run(cross_query_str)\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHere is a comparison of the risk factors described in the Uber 10-K reports across years:\n\n\n\n2022 Risk Factors:\n\n\n- Potential adverse effect if drivers were classified as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and expectation of increased operating expenses.\n\n\n- Impact of future pandemics or disease outbreaks on the business and financial results.\n\n\n- Potential harm to the business due to economic conditions and their effect on discretionary consumer spending.\n\n\n\n2021 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic and actions to mitigate it on the business.\n\n\n- Potential reclassification of drivers as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees and offer incentives to remain competitive.\n\n\n- History of significant losses and uncertainty of achieving profitability.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n\n2020 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic on the business.\n\n\n- Potential reclassification of drivers as employees.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and potential future expenses.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n- Operational and cultural challenges faced by the company.\n\n\n\n2019 Risk Factors:\n\n\n- Competition with local companies.\n\n\n- Differing levels of social acceptance.\n\n\n- Technological compatibility issues.\n\n\n- Exposure to improper business practices.\n\n\n- Legal uncertainty.\n\n\n- Difficulties in managing international operations.\n\n\n- Fluctuations in currency exchange rates.\n\n\n- Regulations governing local currencies.\n\n\n- Tax consequences.\n\n\n- Financial accounting burdens.\n\n\n- Difficulties in implementing financial systems.\n\n\n- Import and export restrictions.\n\n\n- Political and economic instability.\n\n\n- Public health concerns.\n\n\n- Reduced protection for intellectual property rights.\n\n\n- Limited influence over minority-owned affiliates.\n\n\n- Regulatory complexities.\n\n\n\nThese comparisons highlight both common and unique risk factors that Uber faced in different years.\n\n```\n\n### Setting up the Chatbot Loop\n[Section titled \u201cSetting up the Chatbot Loop\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-loop)\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to chat with our SEC-augmented chatbot!\n```\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n\n\n\nwhileTrue:\n\n\n\n\ntext_input =input(\"User: \")\n\n\n\n\nif text_input ==\"exit\":\n\n\n\n\nbreak\n\n\n\n\nresponse =await agent.run(text_input)\n\n\n\n\nprint(f\"Agent: {response}\")\n\n\n```\n\nHere\u2019s an example of the loop in action:\n```\n\nUser:  What were some of the legal proceedings against Uber in 2022?\n\n\nAgent: In 2022, Uber faced several legal proceedings. Some of the notable ones include:\n\n\n\n1. Petition against Proposition 22: A petition was filed in California alleging that Proposition 22, which classifies app-based drivers as independent contractors, is unconstitutional.\n\n\n\n2. Lawsuit by Massachusetts Attorney General: The Massachusetts Attorney General filed a lawsuit against Uber, claiming that drivers should be classified as employees and entitled to protections under wage and labor laws.\n\n\n\n3. Allegations by New York Attorney General: The New York Attorney General made allegations against Uber regarding the misclassification of drivers and related employment violations.\n\n\n\n4. Swiss social security rulings: Swiss social security rulings classified Uber drivers as employees, which could have implications for Uber's operations in Switzerland.\n\n\n\n5. Class action lawsuits in Australia: Uber faced class action lawsuits in Australia, with allegations that the company conspired to harm participants in the taxi, hire-car, and limousine industries.\n\n\n\nIt's important to note that the outcomes of these legal proceedings are uncertain and may vary.\n\n\n\nUser:\n\n```\n\n### Notebook\n[Section titled \u201cNotebook\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#notebook)\nTake a look at our [corresponding notebook](https://developers.llamaindex.ai/python/examples/agent/chatbot_sec).\n", "path": null, "url": null, "mimetype": null}, "image_resource": null, "audio_resource": null, "video_resource": null, "text_template": "{metadata_str}\n\n{content}", "class_name": "Document", "text": "[Skip to content](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#_top)\n# How to Build a Chatbot\nLlamaIndex serves as a bridge between your data and Large Language Models (LLMs), providing a toolkit that enables you to establish a query interface around your data for a variety of tasks, such as question-answering and summarization.\nIn this tutorial, we\u2019ll walk you through building a context-augmented chatbot using a [Data Agent](https://gpt-index.readthedocs.io/en/stable/core_modules/agent_modules/agents/root.html). This agent, powered by LLMs, is capable of intelligently executing tasks over your data. The end result is a chatbot agent equipped with a robust set of data interface tools provided by LlamaIndex to answer queries about your data.\n**Note** : This tutorial builds upon initial work on creating a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).\n### Context\n[Section titled \u201cContext\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#context)\nIn this guide, we\u2019ll build a \u201c10-K Chatbot\u201d that uses raw UBER 10-K HTML filings from Dropbox. Users can interact with the chatbot to ask questions related to the 10-K filings.\n### Preparation\n[Section titled \u201cPreparation\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#preparation)\n```\n\n\nimport os\n\n\n\n\nimport openai\n\n\n\n\n\nos.environ[\"OPENAI_API_KEY\"] =\"sk-...\"\n\n\n\n\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n\n\n\n\nimport nest_asyncio\n\n\n\n\n\nnest_asyncio.apply()\n\n\n```\n\n### Ingest Data\n[Section titled \u201cIngest Data\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#ingest-data)\nLet\u2019s first download the raw 10-k files, from 2019-2022.\n```\n\n# NOTE: the code examples assume you're operating within a Jupyter notebook.\n\n\n# download files\n\n\n!mkdir data\n\n\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n\n\n!unzip data/UBER.zip -d data\n\n```\n\nTo parse the HTML files into formatted text, we use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library. Thanks to [LlamaHub](https://llamahub.ai/), we can directly integrate with Unstructured, allowing conversion of any text into a Document format that LlamaIndex can ingest.\nFirst we install the necessary packages:\n```\n\n!pip install llama-hub unstructured\n\n```\n\nThen we can use the `UnstructuredReader` to parse the HTML files into a list of `Document` objects.\n```\n\n\nfrom llama_index.readers.file import UnstructuredReader\n\n\n\n\nfrom pathlib import Path\n\n\n\n\n\nyears =[2022, 2021, 2020, 2019]\n\n\n\n\n\nloader =UnstructuredReader()\n\n\n\n\ndoc_set = {}\n\n\n\n\nall_docs =[]\n\n\n\n\nfor year in years:\n\n\n\n\nyear_docs = loader.load_data(\n\n\n\n\nfile=Path(f\"./data/UBER/UBER_{year}.html\"),split_documents=False\n\n\n\n\n\n# insert year metadata into each year\n\n\n\n\nforin year_docs:\n\n\n\n\nd.metadata = {\"year\": year}\n\n\n\n\ndoc_set[year] = year_docs\n\n\n\n\nall_docs.extend(year_docs)\n\n\n```\n\n### Setting up Vector Indices for each year\n[Section titled \u201cSetting up Vector Indices for each year\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-vector-indices-for-each-year)\nWe first setup a vector index for each year. Each vector index allows us to ask questions about the 10-K filing of a given year.\nWe build each index and save it to disk.\n```\n\n# initialize simple vector indices\n\n\n\nfrom llama_index.core import VectorStoreIndex, StorageContext\n\n\n\n\nfrom llama_index.core import Settings\n\n\n\n\n\nSettings.chunk_size =512\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults()\n\n\n\n\ncur_index = VectorStoreIndex.from_documents(\n\n\n\n\ndoc_set[year],\n\n\n\n\nstorage_context=storage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n\n\nstorage_context.persist(persist_dir=f\"./storage/{year}\")\n\n\n```\n\nTo load an index from disk, do the following\n```\n\n# Load indices from disk\n\n\n\nfrom llama_index.core import load_index_from_storage\n\n\n\n\n\nindex_set = {}\n\n\n\n\nfor year in years:\n\n\n\n\nstorage_context = StorageContext.from_defaults(\n\n\n\n\npersist_dir=f\"./storage/{year}\"\n\n\n\n\n\ncur_index =load_index_from_storage(\n\n\n\n\nstorage_context,\n\n\n\n\n\nindex_set[year] = cur_index\n\n\n```\n\n### Setting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\n[Section titled \u201cSetting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-a-sub-question-query-engine-to-synthesize-answers-across-10-k-filings)\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.\nTo address this, we can use a [Sub Question Query Engine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/sub_question_query_engine.html). It decomposes a query into subqueries, each answered by an individual vector index, and synthesizes the results to answer the overall query.\nLlamaIndex provides some wrappers around indices (and query engines) so that they can be used by query engines and agents. First we define a `QueryEngineTool` for each vector index. Each tool has a name and a description; these are what the LLM agent sees to decide which tool to choose.\n```\n\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\n\n\n\n\nindividual_query_engine_tools =[\n\n\n\n\nQueryEngineTool(\n\n\n\n\nquery_engine=index_set[year].as_query_engine(),\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=f\"vector_index_{year}\",\n\n\n\n\ndescription=f\"useful for when you want to answer queries about the {year} SEC 10-K for Uber\",\n\n\n\n\n\n\nfor year in years\n\n\n\n```\n\nNow we can create the Sub Question Query Engine, which will allow us to synthesize answers across the 10-K filings. We pass in the `individual_query_engine_tools` we defined above, as well as an `llm` that will be used to run the subqueries.\n```\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\n\n\n\n\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n\n\n\n\nquery_engine_tools=individual_query_engine_tools,\n\n\n\n\nllm=OpenAI(model=\"gpt-3.5-turbo\"),\n\n\n\n```\n\n### Setting up the Chatbot Agent\n[Section titled \u201cSetting up the Chatbot Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-agent)\nWe use a LlamaIndex Data Agent to setup the outer chatbot agent, which has access to a set of Tools. Specifically, we will use a `FunctionAgent`, that takes advantage of OpenAI API function calling. We want to use the separate Tools we defined previously for each index (corresponding to a given year), as well as a tool for the sub question query engine we defined above.\nFirst we define a `QueryEngineTool` for the sub question query engine:\n```\n\n\nquery_engine_tool =QueryEngineTool(\n\n\n\n\nquery_engine=query_engine,\n\n\n\n\nmetadata=ToolMetadata(\n\n\n\n\nname=\"sub_question_query_engine\",\n\n\n\n\ndescription=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber\",\n\n\n\n\n```\n\nThen, we combine the Tools we defined above into a single list of tools for the agent:\n```\n\n\ntools = individual_query_engine_tools +[query_engine_tool]\n\n\n```\n\nFinally, we call `FunctionAgent()` to create the agent, passing in the list of tools we defined above.\n```\n\n\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n\n\n\nfrom llama_index.llms.openai import OpenAI\n\n\n\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n```\n\n### Testing the Agent\n[Section titled \u201cTesting the Agent\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#testing-the-agent)\nWe can now test the agent with various queries.\nIf we test it with a simple \u201chello\u201d query, the agent does not use any Tools.\n```\n\n\nresponse =await agent.run(\"hi, i am bob\")\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHello Bob! How can I assist you today?\n\n```\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use the relevant vector index Tool.\n```\n\n\nresponse =await agent.run(\n\n\n\n\n\"What were some of the biggest risk factors in 2020 for Uber?\"\n\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nSome of the biggest risk factors for Uber in 2020 were:\n\n\n\n1. The adverse impact of the COVID-19 pandemic and actions taken to mitigate it on the business.\n\n\n2. The potential reclassification of drivers as employees, workers, or quasi-employees instead of independent contractors.\n\n\n3. Intense competition in the mobility, delivery, and logistics industries, with low-cost alternatives and well-capitalized competitors.\n\n\n4. The need to lower fares or service fees and offer driver incentives and consumer discounts to remain competitive.\n\n\n5. Significant losses incurred and the uncertainty of achieving profitability.\n\n\n6. The risk of not attracting or maintaining a critical mass of platform users.\n\n\n7. Operational, compliance, and cultural challenges related to the workplace culture and forward-leaning approach.\n\n\n8. The potential negative impact of international investments and the challenges of conducting business in foreign countries.\n\n\n9. Risks associated with operational and compliance challenges, localization, laws and regulations, competition, social acceptance, technological compatibility, improper business practices, liability uncertainty, managing international operations, currency fluctuations, cash transactions, tax consequences, and payment fraud.\n\n\n\nThese risk factors highlight the challenges and uncertainties that Uber faced in 2020.\n\n```\n\nFinally, if we test it with a query to compare/contrast risk factors across years, the agent will use the Sub Question Query Engine Tool.\n```\n\n\ncross_query_str =\"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n\n\n\n\n\nresponse =await agent.run(cross_query_str)\n\n\n\n\nprint(str(response))\n\n\n```\n\n```\n\nHere is a comparison of the risk factors described in the Uber 10-K reports across years:\n\n\n\n2022 Risk Factors:\n\n\n- Potential adverse effect if drivers were classified as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and expectation of increased operating expenses.\n\n\n- Impact of future pandemics or disease outbreaks on the business and financial results.\n\n\n- Potential harm to the business due to economic conditions and their effect on discretionary consumer spending.\n\n\n\n2021 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic and actions to mitigate it on the business.\n\n\n- Potential reclassification of drivers as employees instead of independent contractors.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees and offer incentives to remain competitive.\n\n\n- History of significant losses and uncertainty of achieving profitability.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n\n2020 Risk Factors:\n\n\n- Adverse impact of the COVID-19 pandemic on the business.\n\n\n- Potential reclassification of drivers as employees.\n\n\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n\n\n- Need to lower fares or service fees to remain competitive.\n\n\n- History of significant losses and potential future expenses.\n\n\n- Importance of attracting and maintaining a critical mass of platform users.\n\n\n- Operational and cultural challenges faced by the company.\n\n\n\n2019 Risk Factors:\n\n\n- Competition with local companies.\n\n\n- Differing levels of social acceptance.\n\n\n- Technological compatibility issues.\n\n\n- Exposure to improper business practices.\n\n\n- Legal uncertainty.\n\n\n- Difficulties in managing international operations.\n\n\n- Fluctuations in currency exchange rates.\n\n\n- Regulations governing local currencies.\n\n\n- Tax consequences.\n\n\n- Financial accounting burdens.\n\n\n- Difficulties in implementing financial systems.\n\n\n- Import and export restrictions.\n\n\n- Political and economic instability.\n\n\n- Public health concerns.\n\n\n- Reduced protection for intellectual property rights.\n\n\n- Limited influence over minority-owned affiliates.\n\n\n- Regulatory complexities.\n\n\n\nThese comparisons highlight both common and unique risk factors that Uber faced in different years.\n\n```\n\n### Setting up the Chatbot Loop\n[Section titled \u201cSetting up the Chatbot Loop\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#setting-up-the-chatbot-loop)\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to chat with our SEC-augmented chatbot!\n```\n\n\nagent =FunctionAgent(tools=tools,llm=OpenAI(model=\"gpt-4.1\"))\n\n\n\n\n\nwhileTrue:\n\n\n\n\ntext_input =input(\"User: \")\n\n\n\n\nif text_input ==\"exit\":\n\n\n\n\nbreak\n\n\n\n\nresponse =await agent.run(text_input)\n\n\n\n\nprint(f\"Agent: {response}\")\n\n\n```\n\nHere\u2019s an example of the loop in action:\n```\n\nUser:  What were some of the legal proceedings against Uber in 2022?\n\n\nAgent: In 2022, Uber faced several legal proceedings. Some of the notable ones include:\n\n\n\n1. Petition against Proposition 22: A petition was filed in California alleging that Proposition 22, which classifies app-based drivers as independent contractors, is unconstitutional.\n\n\n\n2. Lawsuit by Massachusetts Attorney General: The Massachusetts Attorney General filed a lawsuit against Uber, claiming that drivers should be classified as employees and entitled to protections under wage and labor laws.\n\n\n\n3. Allegations by New York Attorney General: The New York Attorney General made allegations against Uber regarding the misclassification of drivers and related employment violations.\n\n\n\n4. Swiss social security rulings: Swiss social security rulings classified Uber drivers as employees, which could have implications for Uber's operations in Switzerland.\n\n\n\n5. Class action lawsuits in Australia: Uber faced class action lawsuits in Australia, with allegations that the company conspired to harm participants in the taxi, hire-car, and limousine industries.\n\n\n\nIt's important to note that the outcomes of these legal proceedings are uncertain and may vary.\n\n\n\nUser:\n\n```\n\n### Notebook\n[Section titled \u201cNotebook\u201d](https://developers.llamaindex.ai/python/framework/understanding/putting_it_all_together/chatbots/building_a_chatbot/#notebook)\nTake a look at our [corresponding notebook](https://developers.llamaindex.ai/python/examples/agent/chatbot_sec).\n"}, "__type__": "4"}}}