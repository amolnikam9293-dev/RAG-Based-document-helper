[Skip to content](https://developers.llamaindex.ai/python/examples/llm/qianfan/#_top)
# Client of Baidu Intelligent Cloud's Qianfan LLM Platform 
Baidu Intelligent Cloud’s Qianfan LLM Platform offers API services for all Baidu LLMs, such as ERNIE-3.5-8K and ERNIE-4.0-8K. It also provides a small number of open-source LLMs like Llama-2-70b-chat.
Before using the chat client, you need to activate the LLM service on the Qianfan LLM Platform console’s [online service](https://console.bce.baidu.com/qianfan/ais/console/onlineService) page. Then, Generate an Access Key and a Secret Key in the [Security Authentication](https://console.bce.baidu.com/iam/#/iam/accesslist) page of the console.
## Installation
[Section titled “Installation”](https://developers.llamaindex.ai/python/examples/llm/qianfan/#installation)
Install the necessary package:
```


%pip install llama-index-llms-qianfan


```

## Initialization
[Section titled “Initialization”](https://developers.llamaindex.ai/python/examples/llm/qianfan/#initialization)
```


from llama_index.llms.qianfan import Qianfan




import asyncio





access_key ="XXX"




secret_key ="XXX"




model_name ="ERNIE-Speed-8K"




endpoint_url ="https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/ernie_speed"




context_window =8192




llm =Qianfan(access_key, secret_key, model_name, endpoint_url, context_window)


```

## Synchronous Chat
[Section titled “Synchronous Chat”](https://developers.llamaindex.ai/python/examples/llm/qianfan/#synchronous-chat)
Generate a chat response synchronously using the `chat` method:
```


from llama_index.core.base.llms.types import ChatMessage





messages =[




ChatMessage(role="user",content="Tell me a joke."),





chat_response = llm.chat(messages)




print(chat_response.message.content)


```

## Synchronous Stream Chat
[Section titled “Synchronous Stream Chat”](https://developers.llamaindex.ai/python/examples/llm/qianfan/#synchronous-stream-chat)
Generate a streaming chat response synchronously using the `stream_chat` method:
```


messages =[




ChatMessage(role="system",content="You are a helpful assistant."),




ChatMessage(role="user",content="Tell me a story."),





content =""




for chat_response in llm.stream_chat(messages):




content += chat_response.delta




print(chat_response.delta,end="")


```

## Asynchronous Chat
[Section titled “Asynchronous Chat”](https://developers.llamaindex.ai/python/examples/llm/qianfan/#asynchronous-chat)
Generate a chat response asynchronously using the `achat` method:
```


asyncdefasync_chat():




messages =[




ChatMessage(role="user",content="Tell me an async joke."),





chat_response =await llm.achat(messages)




print(chat_response.message.content)






asyncio.run(async_chat())


```

## Asynchronous Stream Chat
[Section titled “Asynchronous Stream Chat”](https://developers.llamaindex.ai/python/examples/llm/qianfan/#asynchronous-stream-chat)
Generate a streaming chat response asynchronously using the `astream_chat` method:
```


asyncdefasync_stream_chat():




messages =[




ChatMessage(role="system",content="You are a helpful assistant."),




ChatMessage(role="user",content="Tell me an async story."),





content =""




response =await llm.astream_chat(messages)




asyncfor chat_response in response:




content += chat_response.delta




print(chat_response.delta,end="")






asyncio.run(async_stream_chat())


```

