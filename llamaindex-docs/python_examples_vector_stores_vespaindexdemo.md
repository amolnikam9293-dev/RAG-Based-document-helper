[Skip to content](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#_top)
# Vespa Vector Store demo 
If you‚Äôre opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô.
```


%pip install llama-index-vector-stores-vespa llama-index pyvespa


```

#### Setting up API key
[Section titled ‚ÄúSetting up API key‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#setting-up-api-key)
```


import os




import openai





os.environ["OPENAI_API_KEY"] ="sk-..."




openai.api_key = os.environ["OPENAI_API_KEY"]


```

#### Load documents, build the VectorStoreIndex
[Section titled ‚ÄúLoad documents, build the VectorStoreIndex‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#load-documents-build-the-vectorstoreindex)
```


from llama_index.core import VectorStoreIndex




from llama_index.vector_stores.vespa import VespaVectorStore




from IPython.display import Markdown, display


```

## Defining some sample data
[Section titled ‚ÄúDefining some sample data‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#defining-some-sample-data)
Let‚Äôs insert some documents.
```


from llama_index.core.schema import TextNode





nodes =[




TextNode(




text="The Shawshank Redemption",




metadata={




"author": "Stephen King",




"theme": "Friendship",




"year": 1994,






TextNode(




text="The Godfather",




metadata={




"director": "Francis Ford Coppola",




"theme": "Mafia",




"year": 1972,






TextNode(




text="Inception",




metadata={




"director": "Christopher Nolan",




"theme": "Fiction",




"year": 2010,






TextNode(




text="To Kill a Mockingbird",




metadata={




"author": "Harper Lee",




"theme": "Mafia",




"year": 1960,






TextNode(




text="1984",




metadata={




"author": "George Orwell",




"theme": "Totalitarianism",




"year": 1949,






TextNode(




text="The Great Gatsby",




metadata={




"author": "F. Scott Fitzgerald",




"theme": "The American Dream",




"year": 1925,






TextNode(




text="Harry Potter and the Sorcerer's Stone",




metadata={




"author": "J.K. Rowling",




"theme": "Fiction",




"year": 1997,





```

### Initilizing the VespaVectorStore
[Section titled ‚ÄúInitilizing the VespaVectorStore‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#initilizing-the-vespavectorstore)
To make it really simple to get started, we provide a template Vespa application that will be deployed upon initializing the vector store.
This is a huge abstraction and there are endless opportunities to tailor and customize the Vespa application to your needs. But for now, let‚Äôs keep it simple and initialize with the default template.
```


from llama_index.core import StorageContext





vector_store =VespaVectorStore()




storage_context = StorageContext.from_defaults(vector_store=vector_store)




index =VectorStoreIndex(nodes,storage_context=storage_context)


```

### Deleting documents
[Section titled ‚ÄúDeleting documents‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#deleting-documents)
```


node_to_delete = nodes[0].node_id



node_to_delete

```

```


vector_store.delete(ref_doc_id=node_to_delete)


```

## Querying
[Section titled ‚ÄúQuerying‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#querying)
```


from llama_index.core.vector_stores.types import (




VectorStoreQuery,




VectorStoreQueryMode,



```

```


query =VectorStoreQuery(




query_str="Great Gatsby",




mode=VectorStoreQueryMode.TEXT_SEARCH,




similarity_top_k=1,





result = vector_store.query(query)


```

```

result

```

## As retriever
[Section titled ‚ÄúAs retriever‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#as-retriever)
### Default query mode (text search)
[Section titled ‚ÄúDefault query mode (text search)‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#default-query-mode-text-search)
```


retriever = index.as_retriever(vector_store_query_mode="default")




results = retriever.retrieve("Who directed inception?")




display(Markdown(f"**Retrieved nodes:**\n{results}"))


```

```


retriever = index.as_retriever(vector_store_query_mode="semantic_hybrid")




results = retriever.retrieve("Who wrote Harry Potter?")




display(Markdown(f"**Retrieved nodes:**\n{results}"))


```

### As query engine
[Section titled ‚ÄúAs query engine‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#as-query-engine)
```


query_engine = index.as_query_engine()




response = query_engine.query("Who directed inception?")




display(Markdown(f"**Response:** {response}"))


```

```


query_engine = index.as_query_engine(




vector_store_query_mode="semantic_hybrid",verbose=True





response = query_engine.query(




"When was the book about the wizard boy published and what was it called?"





display(Markdown(f"**Response:** {response}"))




display(Markdown(f"**Sources:** {response.source_nodes}"))


```

## Using metadata filters
[Section titled ‚ÄúUsing metadata filters‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#using-metadata-filters)
**NOTE** : This metadata filtering is done by llama-index, outside of vespa. For native and much more performant filtering, you should use Vespa‚Äôs own filtering capabilities.
See [Vespa‚Äôs documentation](https://docs.vespa.ai/en/reference/query-language-reference.html) for more information.
```


from llama_index.core.vector_stores import (




FilterOperator,




FilterCondition,




MetadataFilter,




MetadataFilters,





# Let's define a filter that will only allow nodes that has the theme "Fiction" OR is published after 1997




filters =MetadataFilters(




filters=[




MetadataFilter="theme",value="Fiction"),




MetadataFilter="year",value=1997,operator=FilterOperator.GT),





condition=FilterCondition.OR,






retriever = index.as_retriever(filters=filters)




result = retriever.retrieve("Harry Potter")




display(Markdown(f"**Result:** {result}"))


```

## Abstraction level of this integration
[Section titled ‚ÄúAbstraction level of this integration‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#abstraction-level-of-this-integration)
To make it really simple to get started, we provide a template Vespa application that will be deployed upon initializing the vector store. This removes some of the complexity of setting up Vespa for the first time, but for serious use cases, we strongly recommend that you read the [Vespa documentation](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/docs.vespa.ai) and tailor the application to your needs.
### The template
[Section titled ‚ÄúThe template‚Äù](https://developers.llamaindex.ai/python/examples/vector_stores/vespaindexdemo/#the-template)
The provided template Vespa application can be seen below:
```


from vespa.package import (




ApplicationPackage,




Field,




Schema,




Document,




HNSW,




RankProfile,




Component,




Parameter,




FieldSet,




GlobalPhaseRanking,




Function,






hybrid_template =ApplicationPackage(




name="hybridsearch",




schema=[




Schema(




name="doc",




document=Document(




fields=[




Field(name="id",type="string",indexing=["summary"]),




Field(name="metadata",type="string",indexing=["summary"]),




Field(




name="text",




type="string",




indexing=["index", "summary"],




index="enable-bm25",




bolding=True,





Field(




name="embedding",




type="tensor<float>(x[384])",




indexing=[




"input text",




"embed",




"index",




"attribute",





ann=HNSW(distance_metric="angular"),




is_document_field=False,







fieldsets=[FieldSet(name="default",fields=["text", "metadata"])],




rank_profiles=[




RankProfile(




name="bm25",




inputs=[("query(q)", "tensor<float>(x[384])")],




functions=[Function(name="bm25sum",expression="bm25(text)")],




first_phase="bm25sum",





RankProfile(




name="semantic",




inputs=[("query(q)", "tensor<float>(x[384])")],




first_phase="closeness(field, embedding)",





RankProfile(




name="fusion",




inherits="bm25",




inputs=[("query(q)", "tensor<float>(x[384])")],




first_phase="closeness(field, embedding)",




global_phase=GlobalPhaseRanking(




expression="reciprocal_rank_fusion(bm25sum, closeness(field, embedding))",




rerank_count=1000,









components=[




Component(




id="e5",




type="hugging-face-embedder",




parameters=[




Parameter(




"transformer-model",





"url": "https://github.com/vespa-engine/sample-apps/raw/master/simple-semantic-search/model/e5-small-v2-int8.onnx"






Parameter(




"tokenizer-model",





"url": "https://raw.githubusercontent.com/vespa-engine/sample-apps/master/simple-semantic-search/model/tokenizer.json"








```

Note that the fields `id`, `metadata`, `text`, and `embedding` are required for the integration to work. The schema name must also be `doc`, and the rank profiles must be named `bm25`, `semantic`, and `fusion`.
Other than that you are free to modify as you see fit by switching out embedding models, adding more fields, or changing the ranking expressions.
For more details, check out this Pyvespa example notebook on [hybrid search](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa.html).
