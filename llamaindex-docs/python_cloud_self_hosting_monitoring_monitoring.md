[Skip to content](https://developers.llamaindex.ai/python/cloud/self_hosting/monitoring/monitoring/#_top)
# Monitoring
##  Self-Hosting Documentation Access 
This section requires a password to access. Interested in self-hosting? [Contact sales](https://www.llamaindex.ai/contact) to learn more. 
Self-Hosting Documentation Access Granted  Logout 
LlamaCloud services expose metrics, which are collected by [Prometheus](https://prometheus.io) and visualized in [Grafana](https://grafana.com).
## Prerequisites
[Section titled â€œPrerequisitesâ€](https://developers.llamaindex.ai/python/cloud/self_hosting/monitoring/monitoring/#prerequisites)
To monitor your LlamaCloud deployment, youâ€™ll need:
  * [Prometheus](https://prometheus.io) - For metrics collection and storage
  * [Grafana](https://grafana.com) - For metrics visualization
  * [AlertManager](https://prometheus.io/docs/alerting/latest/alertmanager/) - For alert management


These services can be deployed using the [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) Helm chart, which provides a complete monitoring solution that includes:
  * Prometheus server for metrics collection
  * Grafana for visualization with pre-configured dashboards
  * AlertManager for handling alerts
  * Node exporter for hardware and OS metrics
  * kube-state-metrics for Kubernetes object metrics
  * Prometheus Operator for managing Prometheus instances


Use this file as a starting point for a basic installation:
kube-prometheus-stack.yaml```


prometheus:




enabled: true




grafana:




enabled: true




alertmanager:




enabled: true


```

Terminal window```


helminstallkube-prometheus-stackprometheus-community/kube-prometheus-stack\




-fkube-prometheus-stack.yaml


```

For more information about the kube-prometheus-stack Helm chart, please refer to the [kube-prometheus-stack README](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack).
## Metrics
[Section titled â€œMetricsâ€](https://developers.llamaindex.ai/python/cloud/self_hosting/monitoring/monitoring/#metrics)
The following LlamaCloud services expose metrics which can be scraped at the `/metrics` endpoint.
  * `backend`
  * `jobsService`
  * `jobsWorker`
  * `llamaParse`
  * `llamaParseOcr`


To enable metrics for a service, you need to set the `metrics.enabled` parameter to `true` in the LlamaCloud values.yaml file.
llamacloud-values.yaml```


backend:




metrics:




enabled: true




jobsService:




metrics:




enabled: true




jobsWorker:




metrics:




enabled: true




llamaParse:




metrics:




enabled: true




llamaParseOcr:




metrics:




enabled: true


```

## Service Monitoring
[Section titled â€œService Monitoringâ€](https://developers.llamaindex.ai/python/cloud/self_hosting/monitoring/monitoring/#service-monitoring)
After enabling metrics and installing the [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) Helm chart, you should create `ServiceMonitor` objects. These objects tell Prometheus to scrape the metric endpoints exposed by the LlamaCloud services.


```


apiVersion: monitoring.coreos.com/v1




kind: ServiceMonitor




metadata:




# ğŸ‘‡ The name of the service you want to monitor.




name: llamacloud




# ğŸ‘‡ The namespace where the ServiceMonitor object lives; typically the namespace where you installed the chart.




namespace: <kube-prometheus-stack-namespace>




labels:




# ğŸ‘‡ Typically this is the same namespace as above.




release: kube-prometheus-stack




spec:




namespaceSelector:




matchNames:




# ğŸ‘‡ The namespace you installed Llamacloud into.




- <llamacloud-namespace>




selector:




matchLabels:




app.kubernetes.io/instance: llamacloud




app.kubernetes.io/name: llamacloud




app.kubernetes.io/managed-by: Helm




endpoints:




- port: http




path: /metrics




interval: 30s




scrapeTimeout: 10s


```

```


apiVersion: monitoring.coreos.com/v1




kind: ServiceMonitor




metadata:




# ğŸ‘‡ The name of the service you want to monitor.




name: llamacloud-operator




# ğŸ‘‡ The namespace where the ServiceMonitor object lives; typically the namespace where you installed the chart.




namespace: <kube-prometheus-stack-namespace>




labels:




# ğŸ‘‡ Typically this is the same namespace as above.




release: kube-prometheus-stack




spec:




namespaceSelector:




matchNames:




# ğŸ‘‡ The namespace you installed Llamacloud into.




- <llamacloud-namespace>




selector:




matchLabels:




app.kubernetes.io/instance: llamacloud




app.kubernetes.io/name: llamacloud-operator




app.kubernetes.io/managed-by: Helm




endpoints:




- port: http




path: /metrics




interval: 30s




scrapeTimeout: 10s


```

```


apiVersion: monitoring.coreos.com/v1




kind: ServiceMonitor




metadata:




# ğŸ‘‡ The name of the service you want to monitor.




name: llamacloud-worker




# ğŸ‘‡ The namespace where the ServiceMonitor object lives; typically the namespace where you installed the chart.




namespace: <kube-prometheus-stack-namespace>




labels:




# ğŸ‘‡ Typically this is the same namespace as above.




release: kube-prometheus-stack




spec:




namespaceSelector:




matchNames:




# ğŸ‘‡ The namespace you installed Llamacloud into.




- <llamacloud-namespace>




selector:




matchLabels:




app.kubernetes.io/instance: llamacloud




app.kubernetes.io/name: llamacloud-worker




app.kubernetes.io/managed-by: Helm




endpoints:




- port: http




path: /metrics




interval: 30s




scrapeTimeout: 10s


```

```


apiVersion: monitoring.coreos.com/v1




kind: ServiceMonitor




metadata:




# ğŸ‘‡ The name of the service you want to monitor.




name: llamacloud-parse




# ğŸ‘‡ The namespace where the ServiceMonitor object lives; typically the namespace where you installed the chart.




namespace: <kube-prometheus-stack-namespace>




labels:




# ğŸ‘‡ Typically this is the same namespace as above.




release: kube-prometheus-stack




spec:




namespaceSelector:




matchNames:




# ğŸ‘‡ The namespace you installed Llamacloud into.




- <llamacloud-namespace>




selector:




matchLabels:




app.kubernetes.io/instance: llamacloud




app.kubernetes.io/name: llamacloud-parse




app.kubernetes.io/managed-by: Helm




endpoints:




- port: http




path: /metrics




interval: 30s




scrapeTimeout: 10s


```

```


apiVersion: monitoring.coreos.com/v1




kind: ServiceMonitor




metadata:




# ğŸ‘‡ The name of the service you want to monitor.




name: llamacloud-ocr




# ğŸ‘‡ The namespace where the ServiceMonitor object lives; typically the namespace where you installed the chart.




namespace: <kube-prometheus-stack-namespace>




labels:




# ğŸ‘‡ Typically this is the same namespace as above.




release: kube-prometheus-stack




spec:




namespaceSelector:




matchNames:




# ğŸ‘‡ The namespace you installed Llamacloud into.




- <llamacloud-namespace>




selector:




matchLabels:




app.kubernetes.io/instance: llamacloud




app.kubernetes.io/name: llamacloud-ocr




app.kubernetes.io/managed-by: Helm




endpoints:




- port: http




path: /metrics




interval: 30s




scrapeTimeout: 10s


```

## Grafana
[Section titled â€œGrafanaâ€](https://developers.llamaindex.ai/python/cloud/self_hosting/monitoring/monitoring/#grafana)
Once you have installed the [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) and installed the `ServiceMonitor` objects into your Kubernetes cluster, you will be able to view the metrics in Grafana.
First, get the Grafana `admin` userâ€™s password:
Terminal window```


kubectl--namespace<llamacloud-namespace>getsecretskube-prometheus-stack-grafana-ojsonpath="{.data.admin-password}"|base64-decho


```

Next, create a `port-forward` to Grafana:
Terminal window```


exportGRAFANA_POD_NAME=$(kubectl--namespace<llamacloud-namespace>\




getpod-l"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=kube-prometheus-stack"-oname)




kubectl--namespace<llamacloud-namespace>port-forward$GRAFANA_POD_NAME3000


```

Now open your browswer to `localhost:3000` and log into the Grafana console. You should be able to explore all the LlamaCloud metrics in the **Explore** tab.
