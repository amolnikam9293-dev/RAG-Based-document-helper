[Skip to content](https://developers.llamaindex.ai/python/examples/llm/databricks/#_top)
# Databricks 
Integrate with Databricks LLMs APIs.
## Pre-requisites
[Section titled ‚ÄúPre-requisites‚Äù](https://developers.llamaindex.ai/python/examples/llm/databricks/#pre-requisites)
  * [Databricks personal access token](https://docs.databricks.com/en/dev-tools/auth/pat.html) to query and access Databricks model serving endpoints.
  * [Databricks workspace](https://docs.databricks.com/en/workspace/index.html) in a [supported region](https://docs.databricks.com/en/machine-learning/model-serving/model-serving-limits.html#regions) for Foundation Model APIs pay-per-token.


## Setup
[Section titled ‚ÄúSetup‚Äù](https://developers.llamaindex.ai/python/examples/llm/databricks/#setup)
If you‚Äôre opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô.
```


% pip install llama-index-llms-databricks


```

```


!pip install llama-index


```

```


from llama_index.llms.databricks import Databricks


```

```

None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.

```

Terminal window```


exportDATABRICKS_TOKEN=<yourapikey>




exportDATABRICKS_SERVING_ENDPOINT=<yourapiservingendpoint>


```

Alternatively, you can pass your API key and serving endpoint to the LLM when you init it:
```


llm =Databricks(




model="databricks-dbrx-instruct",




api_key="your_api_key",




api_base="https://[your-work-space].cloud.databricks.com/serving-endpoints/",



```

A list of available LLM models can be found [here](https://console.groq.com/docs/models).
```


response = llm.complete("Explain the importance of open source LLMs")


```

```


print(response)


```

#### Call `chat` with a list of messages
[Section titled ‚ÄúCall chat with a list of messages‚Äù](https://developers.llamaindex.ai/python/examples/llm/databricks/#call-chat-with-a-list-of-messages)
```


from llama_index.core.llms import ChatMessage





messages =[




ChatMessage(




role="system",content="You are a pirate with a colorful personality"





ChatMessage(role="user",content="What is your name"),





resp = llm.chat(messages)


```

```


print(resp)


```

### Streaming
[Section titled ‚ÄúStreaming‚Äù](https://developers.llamaindex.ai/python/examples/llm/databricks/#streaming)
Using `stream_complete` endpoint
```


response = llm.stream_complete("Explain the importance of open source LLMs")


```

```


forin response:




print(r.delta,end="")


```

Using `stream_chat` endpoint
```


from llama_index.core.llms import ChatMessage





messages =[




ChatMessage(




role="system",content="You are a pirate with a colorful personality"





ChatMessage(role="user",content="What is your name"),





resp = llm.stream_chat(messages)


```

```


forin resp:




print(r.delta,end="")


```

