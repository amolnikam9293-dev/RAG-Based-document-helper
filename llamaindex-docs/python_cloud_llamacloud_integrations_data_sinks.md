[Skip to content](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/#_top)
# Data Sinks
# Data Sinks
[Section titled “Data Sinks”](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/data_sinks/#data-sinks)
Once your input documents have been processed, they’re ready to be sent to their final destination: a vector database.
If you don’t want to set up and host a vector database, we offer a full-managed option in which we host the vector database for you. Alternatively, you can host your own vector database and connect it to Index:
Once the vector database is setup, they will be store using a [Embedding Model](https://developers.llamaindex.ai/python/cloud/llamacloud/integrations/embedding_models/) of choice and will be ready to be used in your RAG use case ➡️
For the time being, the term “Data Sink” means a vector database. However, this definition of a Data Sink may expand in the future.
